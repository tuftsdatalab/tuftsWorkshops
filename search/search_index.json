{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Tufts Research Technology Bioinformatics","text":"<p>Research Technology Bioinformatics provides consultations to Tufts students, faculty and researchers. In addition we maintain bioinformatics tools on the Tufts HPC Cluster and the Tufts Galaxy Platform. We also lead in-class sessions, partner on grants, and develop workshops. If you have suggestions for future workshops, please reach out to bioinformatics-workshop-questions@elist.tufts.edu. Additionally, if you'd like to be kept up to date on current workshops consider subscribing to our e-list: best@elist.tufts.edu</p> <p>Check out our calendar to stay up to date on upcoming events:</p>"},{"location":"01_array_jobs/","title":"Slurm Job Arrays","text":"<p>Job arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily, saving both time and computational resources.</p>"},{"location":"01_array_jobs/#use-cases","title":"Use cases","text":"<ul> <li>I have 1000 samples and they all need to run the same workflow.</li> <li>I need to run a simulation 1000 times with a different set of parameters.</li> </ul> <p>Why not use serial jobs? </p> <p>A common approach is to use bash loops to submit jobs one by one, but this is not efficient for large numbers of tasks. For example:</p> <p><pre><code>for fq in *.fastq.gz; do \n  fastqc -t 4 $fq\ndone\n</code></pre> </p> <p>Using bash loops works but often results in jobs taking much longer. Instead, using SLURM job arrays can streamline this process.</p>"},{"location":"01_array_jobs/#slurm-arrays","title":"Slurm arrays","text":""},{"location":"01_array_jobs/#basic-syntax","title":"Basic Syntax","text":"<p>Job arrays are only supported for batch jobs, and the array index values are specified using the <code>--array</code> or <code>-a</code> option of the <code>sbatch</code> command or <code>#SBATCH</code> inisde job script. </p> <pre><code>--array=&lt;indices&gt;\n</code></pre> <ul> <li> <p>You can specify the array indices in different ways:</p> </li> <li> <p><code>--array=0-100</code>: Runs jobs with indices from 0 to 100.</p> </li> <li> <p><code>--array=2,4,6,8,10</code>: Runs jobs with specific indices (2, 4, 6, 8, and 10).</p> </li> <li> <p><code>--array=2-1000:2</code>: Runs jobs with a step size, in this case, every 2nd job from 2 to 1000.</p> </li> <li> <p>You can limit the number of array jobs which are allowed to run at once by using the <code>%</code> character when specifying indices.</p> </li> <li> <p><code>1-16%2</code> Create 16 jobs, but only allow two to run at a time</p> </li> </ul>"},{"location":"01_array_jobs/#job-id-and-environment-variables","title":"Job ID and Environment Variables","text":""},{"location":"01_array_jobs/#slurm_array_job_id","title":"SLURM_ARRAY_JOB_ID","text":"<ul> <li> <p>This environment variable represents the job ID of the entire job array.</p> </li> <li> <p>It is the same for all tasks within that job array.</p> </li> <li> <p>If you submit a job array with 10 tasks, each of those tasks will have the same <code>SLURM_ARRAY_JOB_ID</code>.</p> </li> </ul> <p>Example:</p> <p>If you submit a job array with <code>sbatch --array=1-10 script.sh</code>, and the job array is assigned the job ID 12345, then</p> <ul> <li><code>SLURM_ARRAY_JOB_ID</code> for all tasks will be 12345.</li> </ul>"},{"location":"01_array_jobs/#slurm_array_task_id","title":"SLURM_ARRAY_TASK_ID","text":"<ul> <li> <p>This environment variable represents the unique identifier of each task within the job array.</p> </li> <li> <p>It differentiates each task in the array and usually corresponds to the index you specified when submitting the job array.</p> </li> <li> <p>This is the variable you use to handle task-specific operations within the script.</p> </li> </ul> <p>Example:</p> <p>If you submit a job array with <code>sbatch --array=1-10 script.sh</code>, and the job array is assigned the job ID 12345, then:</p> <ul> <li> <p>Task 1 will have SLURM_ARRAY_TASK_ID=1.</p> </li> <li> <p>Task 2 will have SLURM_ARRAY_TASK_ID=2.</p> </li> <li> <p>And so on, up to SLURM_ARRAY_TASK_ID=10 for the last task.</p> </li> </ul> <p>In a simple case, you can directly use the <code>$SLURM_ARRAY_TASK_ID</code> variable in your script to set up your job array. </p> <p>For instance, if you have a fasta file for each sample like: sample1.fa, sample2.fa, sample3.fa ... sample10.fa, and you want each of the 10 Slurm array tasks to handle a separate sample file, you can replace the line specifying the sample filename with <code>sample${SLURM_ARRAY_TASK_ID}.fa</code>. </p> <p>This means that for array task 1, the script will run sample1.fa, for array task 2 it will run sample2.fa, and so on.</p>"},{"location":"01_array_jobs/#monitor-and-cancel-jobs","title":"Monitor and cancel jobs","text":"<p>You can cancel a particular array task using the respective JOBID in the first column, e.g. <code>scancel 7456478_2</code>, or you can cancel all array tasks in the array job by just specifying the main job ID, e.g. <code>scancel 7456478</code>.</p> <pre><code>[yzhang85@login-prod-01 array]$ sbatch fastqc_array.sh \nSubmitted batch job 7456347\n[yzhang85@login-prod-01 array]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n         7456478_1   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_2   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_3   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_4   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_5   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_6   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n</code></pre>"},{"location":"01_array_jobs/#limiting-the-number-of-tasks-to-run-simultaneously","title":"Limiting the number of tasks to run simultaneously","text":"<p>By default, if sufficient resources are available, all tasks in a job array will run simultaneously. However, if you wish to limit the number of tasks running at once, you can use the <code>%N</code> parameter with the <code>--array</code> option (where N specifies the maximum number of tasks to execute concurrently). </p> <p>In the following example, I used <code>--array=0-999%10</code>, which creates a total of 1000 tasks. By appending <code>%10</code>, I limited the number of tasks that can run concurrently to 10, meaning that instead of all 1000 tasks running simultaneously, only 10 tasks will be executed at any given time. This helps manage resource usage on the cluster.</p> <pre><code>         JOBID       PARTITION  NAME     USER  ST       TIME  NODES NODELIST(REASON)\n7689847_[10-999%10   preempt array_te yzhang85 PD       0:00      1 (JobArrayTaskLimit)\n         7689847_0   preempt array_te yzhang85  R       0:03      1 p1cmp029\n         7689847_1   preempt array_te yzhang85  R       0:03      1 p1cmp029\n         7689847_2   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_3   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_4   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_5   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_6   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_7   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_8   preempt array_te yzhang85  R       0:03      1 p1cmp045\n         7689847_9   preempt array_te yzhang85  R       0:03      1 p1cmp045\n</code></pre>"},{"location":"01_array_jobs/#example-job-scripts","title":"Example job scripts","text":"<p>In the following example, I have many <code>fastq.gz</code> files in the folder <code>fastq</code>. I need to run <code>fastqc</code>  to check the quality of each of these <code>fastq.gz</code> files. </p> <pre><code>$ ls -1 fastq/*.gz\nfastq/SRX1693951_1.fastq.gz\nfastq/SRX1693951_2.fastq.gz\nfastq/SRX1693952_1.fastq.gz\nfastq/SRX1693952_2.fastq.gz\nfastq/SRX1693953_1.fastq.gz\nfastq/SRX1693953_2.fastq.gz\nfastq/SRX1693954_1.fastq.gz\nfastq/SRX1693954_2.fastq.gz\nfastq/SRX1693955_1.fastq.gz\nfastq/SRX1693955_2.fastq.gz\nfastq/SRX1693956_1.fastq.gz\nfastq/SRX1693956_2.fastq.gz\n</code></pre> <p>For each <code>fastq.gz</code> file, I want to submit a separate slurm job to our cluster. This can be achieved with slurm job array. </p> <p>We can use <code>fastq/SRX169395${SLURM_ARRAY_TASK_ID}_1.fastq.gz</code> and <code>fastq/SRX169395${SLURM_ARRAY_TASK_ID}_2.fastq.gz</code> to represent pairs of <code>fastq.gz</code> files.</p> <pre><code>#!/bin/bash\n#SBATCH -p preempt  # batch, gpu, preempt, mpi or your group's own partition\n#SBATCH -t 1:00:00  # Runtime limit (D-HH:MM:SS)\n#SBATCH -N 1   # Number of nodes\n#SBATCH -n 1   # Number of tasks per node\n#SBATCH -c 4   # Number of CPU cores per task\n#SBATCH --mem=8G       # Memory required per node\n#SBATCH --array=1-6     # An array of 6 jobs\n#SBATCH --job-name=fastqc      # Job name\n#SBATCH --mail-type=FAIL,BEGIN,END     # Send an email when job fails, begins, and finishes\n#SBATCH --mail-user=yzhang85@tufts.edu       # Email address for notifications\n#SBATCH --error=%x-%A_%a.err   # Standard error file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.err\n#SBATCH --output=%x-%A_%a.out  # Standard output file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.out\n\necho \"SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n\nmodule load fastqc/0.12.1\nfastqc -t 4 fastq/SRX169395${SLURM_ARRAY_TASK_ID}_1.fastq.gz fastq/SRX169395${SLURM_ARRAY_TASK_ID}_2.fastq.gz -o fastqcOut\n</code></pre> <p>Output logs</p> <pre><code>[yzhang85@login-prod-01 array]$ ls -hl\ntotal 13K\ndrwxrws--- 2 yzhang85 workshop 4.0K Aug 30 11:51 fastq/\ndrwxrws--- 2 yzhang85 workshop 4.0K Aug 30 11:39 fastqcOut/\n-rw-rw---- 1 yzhang85 workshop 1.2K Aug 30 11:54 fastqc-7456478_1.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_1.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_2.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_2.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_3.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_3.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_4.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_4.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_5.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_5.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_6.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_6.out\n-rw-rw---- 1 yzhang85 workshop  918 Aug 30 11:48 fastqc_array.sh\n</code></pre>"},{"location":"01_array_jobs/#limits","title":"Limits","text":"<p>HPC is a valuable shared resource that allows many users to perform complex calculations simultaneously. To ensure a productive and fair environment for everyone, we have implemented policies and practices that promote equitable access to our computational resources.</p> <p>There are several limits for array jobs.  If you submit too many array jobs and exceed the limits, you will get the below error message: <pre><code>$ sbatch array.sub \nsbatch: error: AssocMaxSubmitJobLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre></p>"},{"location":"01_array_jobs/#maxarraysize","title":"MaxArraySize","text":"<p>The array index should be smaller than <code>MaxArraySize</code>. </p> <pre><code>scontrol show conf | grep MaxArraySize\n$ scontrol show config | grep -i array\nMaxArraySize            = 2000\n</code></pre> <p>Since <code>MaxArraySize</code> is set as 2000, the maximum array index you can use is 1999. So \"1000-1999\" is valid, but \"1001-2000\" is invalid. </p>"},{"location":"01_array_jobs/#maxsubmit","title":"MaxSubmit","text":"<p>Our cluster does not allow users to submit &gt; 1000 jobs. As a result, the maximum array size is 1000. So \"0-999\" and \"1-1000\" is valid, but \"1-1001\" or \"0-1000\" is invalid. </p>"},{"location":"01_array_jobs/#cpus-ram-and-gpus","title":"CPUs, RAM and GPUs","text":"<pre><code>Public Partitions (batch+mpi+largemem+gpu)\nCPU: 1000 cores\nRAM: 4000 GB\nGPU: 10\n\nPreempt Partition (preempt)\nCPU: 2000 cores\nRAM: 8000 GB\nGPU: 20\n</code></pre> <p>Please note that the above limits are subject to change in the future. To ensure optimal resource allocation, the limit value is dynamic and may change as we evaluate system demands.</p>"},{"location":"01_array_jobs/#create-a-contig-file-for-your-array-tasks-change-the-title","title":"Create a contig file for your array tasks (Change the title)","text":"<p>Use R script as an example.  In most cases, your script will loop through different input parameters, which are usually not number 1-10, 1-100. In this situation, we would like to a config file with input parameters for each job.  (Will revise later)</p>"},{"location":"01_array_jobs/#required-files","title":"Required files","text":"<ol> <li>Parameter File: A file containing the parameters that your array job will iterate through. This file could include different variables or data that each array task will process individually.</li> <li>Script (R, Shell, Python, etc.): The main script that will perform the analysis or visualization tasks. While the example here is in R, the same structure applies to other languages like shell, Python, or Perl. Adapt the script according to the specific tool or language you are using for the job.</li> <li>Wrapper Shell Script: A simple shell script that sends your jobs to the SLURM scheduler. This script makes it easy to run multiple tasks automatically, with each task using different parameters from the parameter file.</li> </ol>"},{"location":"01_array_jobs/#r-script-example","title":"R Script Example","text":"<p>Here is n example of an R script that generates scatter plots of gene expression based on raw RNA-seq count data:</p> <pre><code># Load libraries\nlibrary(tidyverse)\nlibrary(ggrepel)\n\n# Read in parameters\nargs &lt;- commandArgs(trailingOnly = TRUE)\ngene &lt;- as.character(args[1])\npadj &lt;- as.numeric(args[2])\n\n# Subset the gene of interest\ndt &lt;- read.table(\"salmon.merged.gene_counts.tsv\", header=T)\nd &lt;- dt[match(gene, dt$gene_name),]\nd &lt;- gather(d, key = \"condition\", value = \"expression\", GFPkd_1:PRMT5kd_3)\n\n# Reformat for ggplot\nd_long &lt;- separate(d, col = \"condition\", into = c(\"treatment\", \"replicate\"), sep = \"_\")\n\n# Ggplot to visualize\np &lt;- ggplot(d_long, aes(treatment, expression)) +\n        geom_point(size=5, color=\"steelblue\", alpha=0.5) +\n        geom_label_repel(aes(label=replicate)) +\n        theme_classic() +\n        xlab(\"Treatment\") +\n        ylab(\"Gene expression\") +\n        ggtitle(paste0(gene,\": padj \", padj))\n\n# Save plot to a pdf file\nggsave(plot=p, file=paste0(gene, \".pdf\"), width=4, height=4)\n</code></pre>"},{"location":"01_array_jobs/#script-purpose","title":"Script Purpose","text":"<p>This R script creates scatter plots for gene expression levels between control and treated groups from an RNA-seq analysis. It reads in two parameters from the command line: the gene name (<code>genename</code>) and the adjusted p-value (<code>padj</code>). The input data file is <code>salmon.merged.gene_counts.tsv</code>.</p>"},{"location":"01_array_jobs/#example-parameter-file","title":"Example Parameter File","text":"<p>Here\u2019s an example of the parameter file (<code>table.tsv</code>) used in the job array. Each row contains gene expression information, and the R script will extract specific columns for each job.</p> <p><pre><code>gene_id baseMean        log2FoldChange  lfcSE   pvalue  padj    genename\nENSG00000078018 1126.709        -2.161184       0.05810824      1.578201e-304   2.054292e-301   MAP2\nENSG00000004799 1224.003        -2.199776       0.06003955      1.17799e-295    1.4154e-292     PDK4\nENSG00000272398 2064.696        1.615232        0.04513554      2.024618e-282   2.258895e-279   CD24\nENSG00000135046 12905.46        -0.8779134      0.02467955      1.349814e-278   1.405606e-275   ANXA1\n</code></pre> Delete unused columns</p> <p>The R script reads the <code>genename</code> from column 7 and <code>padj</code> from column 6 for each gene.</p>"},{"location":"01_array_jobs/#shell-wrapper-script","title":"Shell Wrapper Script","text":"<p>The following shell script submits the jobs to the SLURM scheduler as an array of tasks. Each task processes a different gene from the parameter file.</p> <pre><code>#!/bin/bash\n#SBATCH -p preempt  # batch, gpu, preempt, mpi or your group's partition\n#SBATCH -t 1:00:00  # Runtime limit (D-HH:MM:SS)\n#SBATCH -N 1        # Number of nodes\n#SBATCH -n 1        # Number of tasks per node\n#SBATCH -c 4        # Number of CPU cores per task\n#SBATCH --mem=2G    # Memory required per node\n#SBATCH --array=2-11 # An array of 10 jobs\n#SBATCH --job-name=Rplot\n#SBATCH --mail-type=FAIL,BEGIN,END\n#SBATCH --mail-user=xue.li37@tufts.edu\n#SBATCH --error=%x-%A_%a.err   # Standard error file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.err\n#SBATCH --output=%x-%A_%a.out  # Standard output file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.out\n\necho \"SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n\nmodule load R/4.4.0\nGENE=$(awk \"NR==${SLURM_ARRAY_TASK_ID} {print \\$7}\" table.tsv) \nPadj=$(awk \"NR==${SLURM_ARRAY_TASK_ID} {print \\$6}\" table.tsv)\n\necho $GENE $Padj\nRscript R_scatter_vis.r $GENE $Padj  \n</code></pre>"},{"location":"01_array_jobs/#script-details","title":"Script Details","text":"<ul> <li><code>SBATCH --array=2-11</code> tells SLURM to run jobs for rows 2 to 11 of the parameter file.</li> <li>The <code>awk</code> commands extract the <code>GENE</code> and <code>Padj</code> values from the specified row and columns (7th and 6th).</li> <li>The script submits 10 jobs, each running the R script with different <code>GENE</code> and <code>Padj</code> values.</li> </ul>"},{"location":"01_array_jobs/#output-files","title":"Output files","text":"<pre><code>A list of files\n</code></pre> <p>  1,2,3,4</p>"},{"location":"01_array_jobs/#customizing-the-array","title":"Customizing the Array","text":"<p>You can adjust the <code>--array</code> option to change the range of jobs. For example, to run jobs for every other line from 2 to 1000, you can specify:</p> <pre><code>#SBATCH --array=2-1000:2\n</code></pre> <p>This would submit jobs for rows 2, 4, 6, ..., up to 1000.</p> <p>Useful links: https://blog.ronin.cloud/slurm-job-arrays/</p>"},{"location":"2022_workshops/2022_workshops/","title":"Schedule","text":""},{"location":"2022_workshops/2022_workshops/#2022-workshops","title":"2022 Workshops","text":"<p>The TTS Research Technology Bioinformatics Team has prepared the following workshops:</p> <ul> <li>Intro to HPC/Slurm For Life Sciences</li> <li>Intro to RStudio For Life Sciences</li> <li>Intro To 16S Metabarcoding</li> <li>Intro To Metagenomics</li> <li>Intro To Genome Technology</li> <li>Intro To AlphaFold2</li> </ul> <p>If you have suggestions for future workshops, please reach out to bioinformatics-workshop-questions@elist.tufts.edu. Additionally, if you'd like to be kept up to date on current workshops consider subscribing to our e-list: best@elist.tufts.edu</p>"},{"location":"2022_workshops/intro16S/01_metabarcoding-intro/","title":"Introduction","text":""},{"location":"2022_workshops/intro16S/01_metabarcoding-intro/#intro-to-16s-metabarcoding","title":"Intro To 16S Metabarcoding","text":"<p>November 2,2022</p>"},{"location":"2022_workshops/intro16S/01_metabarcoding-intro/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Jason Laird, M.S., Bioinformatics Scientist</li> <li>Naisi Zhao, M.S., Dr.PH, Research Assistant Professor</li> <li>Adelaide Rhodes, Ph.D,  Senior Bioinformatics Scientist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"2022_workshops/intro16S/01_metabarcoding-intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"2022_workshops/intro16S/01_metabarcoding-intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Delilah Maloney, Kyle Monahan, Christina Divoll, Kayla Sansevere, and Uku Uustalu for their review of this content</p>"},{"location":"2022_workshops/intro16S/02_background/","title":"Introduction to the Microbiome","text":"<ul> <li>The microbiome refers to the collective set of genes belonging to the microbiota in a specimen. The term microbiota represents the community of microbes themselves.</li> <li>Disturbances in the microbiome have been linked to multiple chronic conditions, including obesity, inflammatory bowel disease, alcoholic and nonalcoholic fatty liver disease, and hepatocellular carcinoma.</li> </ul>"},{"location":"2022_workshops/intro16S/02_background/#microbiome-variability","title":"Microbiome Variability","text":"<ul> <li>Assessing a microbiome disturbance is not a trivial task as it is highly variable from person to person.</li> <li>Large sample sizes, hundreds of patients, are needed to overcome interindividual variability.</li> </ul>"},{"location":"2022_workshops/intro16S/02_background/#sample-collection","title":"Sample Collection","text":"<ul> <li>Sample collection is also a difficult challenge and highly dependent on the study question.</li> <li>The microbiome can change in an individual over time, especially in diseases marked by flare ups like IBD.</li> <li>Samples might not be representative of the site in question. For example, a stool sample sits in the rectum \u2013 an environment that is undergoing dehydration and fermentation which might select for different bacteria than in the small intestine.</li> </ul>"},{"location":"2022_workshops/intro16S/02_background/#confounding-factors","title":"Confounding Factors","text":"<ul> <li>When conducting a clinical experiment, it is pertinent to stratify accounting for age, gender, diet, etc.</li> <li>Sampling over time is incredibly valuable as you can better capture intrapatient variability.</li> <li>Additionally, the way the sample is processed can also confound your results</li> </ul>"},{"location":"2022_workshops/intro16S/02_background/#what-is-an-amplicon","title":"What is an Amplicon?","text":"<ul> <li>Microbiome Amplicon sequencing involves sequencing a specific gene from microbial community</li> </ul>"},{"location":"2022_workshops/intro16S/02_background/#why-sequence-one-gene","title":"Why Sequence One Gene?","text":"<ul> <li> <p>Genes can vary per organism and may not be well conserved across species. To assess the microbial community composition, we need to sequence a conserved gene across organisms of interest:</p> <ul> <li>16S ribosome DNA (rDNA) for prokaryotes</li> <li>18S rDNA and internal transcribed spacers (ITS) for eukaryotes </li> </ul> </li> <li> <p>In the selected gene there are different levels of conservation across organisms. To circumvent this parts of the gene with high conservation (like the V4 region of 16S rRNA) are selected for</p> </li> </ul> <p></p>"},{"location":"2022_workshops/intro16S/02_background/#our-data","title":"Our Data","text":"<p>Today we will be analyzing the microbiome of wild type mice and the C57BL/6NTac laboratory mouse strain, from Rosshart et al. (2107), using amplicon data analysis:</p> <p></p>"},{"location":"2022_workshops/intro16S/02_background/#amplicon-data-analysis","title":"Amplicon Data Analysis","text":"<p>The goal of amplicon data analysis is to generate amplicon sequence variant table (also called feature table).  Researchers can use this table to conduct further downstream analysis including:</p> <ul> <li>alpha/beta-diversity</li> <li>taxonomic composition</li> <li>difference comparison </li> <li>correlation anlysis</li> <li>network analysis</li> </ul> <p> </p> Modified from Liu et al. Protein &amp; Cell (2021) <p>Today we will be using the DADA2 method to perform our amplicon data analysis!</p> Other 16S Analysis Methods <ul> <li>USEARCH</li> <li>Mothur</li> <li>QIIME</li> </ul>"},{"location":"2022_workshops/intro16S/03_setup/","title":"Setup","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"2022_workshops/intro16S/03_setup/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>8GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshop</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p>"},{"location":"2022_workshops/intro16S/03_setup/#project-setup","title":"Project Setup","text":"<p>We are going to create a new project to begin:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>intro-to-16S</code>)</li> <li><code>Create Project</code></li> </ol>"},{"location":"2022_workshops/intro16S/03_setup/#file-organization","title":"File Organization","text":"<p>In our project we will need some folders to contain our scripts, data and results:</p> <ul> <li>Click the New Folder icon</li> <li>Create a folder called data and click ok</li> <li>Following the same process, create a scripts folder and a results folder</li> </ul>"},{"location":"2022_workshops/intro16S/03_setup/#data-scripts","title":"Data &amp; Scripts","text":"<p>Today we will be working with data from Rosshart et al. (2107) where wild-type and laboratory strain mouse microbiomes were assessed. To copy over  this data we will enter the following command into the console:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/raw_fastq/\",to=\"./data/\", recursive = TRUE)\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/meta/metaData.txt\",to=\"./data/\", recursive = TRUE)\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/silva/silva_nr99_v138.1_train_set.fa.gz\",to=\"./data/\")\nfile.copy(from=\"/cluster/tufts/bio/tools/training/microbiome16S/scripts/dada2pipeline.Rmd\",to=\"./scripts/\")\n</code></pre> <p>Now that we have our data and scripts copied, let's navigate to our scripts folder and open up \"dada2pipeline.Rmd\".</p>"},{"location":"2022_workshops/intro16S/03_setup/#libraries","title":"Libraries","text":"<p>To run a code chunk in this R markdown file, click the play button at the top right hand side of the code chunk. We will practice by running the code chunk that loads the R libraries we will need for this workshop:</p> <pre><code># load our libraries\n.libPaths(c('/cluster/tufts/hpc/tools/R/4.0.0',.libPaths()))\nlibrary(dada2)\nlibrary(phyloseq)\nlibrary(ggplot2)\nlibrary(DESeq2)\nlibrary(tidyverse)\nlibrary(phangorn)\nlibrary(msa)\n</code></pre>"},{"location":"2022_workshops/intro16S/04_quality-control/","title":"Quality Control","text":""},{"location":"2022_workshops/intro16S/04_quality-control/#sequencing-overview","title":"Sequencing Overview","text":"<ul> <li>Marker gene (16S, 18S, or ITS) is selected</li> <li>Primers target areas of high conservation in gene </li> <li>DNA is fragmented </li> <li>Adapters are added to help the DNA attach to a flow cell</li> <li>Barcodes may also be added to identify which DNA came from which sample</li> <li>The fragments are sequenced to produce reads</li> <li>Reads can be single-end (one strand sequenced) or paired-end (both strands sequenced) </li> </ul>"},{"location":"2022_workshops/intro16S/04_quality-control/#sequencing-read-data","title":"Sequencing Read Data","text":"<ul> <li>After sequencing we end up with a FASTQ file which contains:<ul> <li>A sequence label</li> <li>The nucleic acid sequence</li> <li>A separator</li> <li>The quality score for each base pair</li> </ul> </li> </ul>"},{"location":"2022_workshops/intro16S/04_quality-control/#demultiplexing","title":"Demultiplexing","text":"<ul> <li>Sometimes samples are mixed to save on sequencing cost </li> <li>To identify which DNA is from which sample Barcodes are added</li> <li>Before moving forward samples need to separated and those DNA barcodes need to be removed </li> <li>Tools like sabre can demultiplex pooled FASTQ data</li> </ul>"},{"location":"2022_workshops/intro16S/04_quality-control/#quality-scores","title":"Quality Scores","text":"<ul> <li>Quality Scores are the probability that a base was called in error</li> <li>Higher scores indicate that the base is less likely to be incorrect</li> <li>Lower scores indicate that the base is more likely to be incorrect</li> </ul>"},{"location":"2022_workshops/intro16S/04_quality-control/#dada2-quality-control","title":"DADA2 Quality Control","text":"<p>Warning</p> <p>DADA2 assumes that your read data has had any adapters removed and that your data is demultiplexed!  Check out sabre to demultiplex your samples and Cutadapt to remove adapters</p> <p>We begin by specifying the path to our data, sorting by forward and reverse strands, and grabbing our sample names:</p> <pre><code># path to files\npath &lt;- \"../data/raw_fastq\"\n\n# sort our files by forward and reverse strands \n# so that the sample names for each strand matches\n# our data has the pattern \"_pass_1.fastq.gz\" \n# and \"_pass_2.fastq.gz\"\npath2Forward &lt;- sort(\n  list.files(\n    path,\n    pattern=\"_pass_1.fastq.gz\",\n    full.names = TRUE)\n  )\npath2Reverse &lt;- sort(\n  list.files(\n    path,\n    pattern=\"_pass_2.fastq.gz\",\n    full.names = TRUE)\n  )\n\n# now let's grab our sample names\nsampleNames &lt;- sapply(\n  strsplit(\n    basename(path2Forward), \"_\"), `[`, 1)\n</code></pre> <p>DADA2 has built in plotting features that allow you to inspect your fastq files:</p> <pre><code># plot the forward strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Forward[1:2])+\n  guides(scale = \"none\")\n</code></pre> <p></p> <pre><code># plot the reverse strand quality plot of our first two sample\ndada2::plotQualityProfile(path2Reverse[1:2])+\n  guides(scale = \"none\")\n</code></pre> <p></p> <p>What does the graph tell us?</p> <ul> <li>Here we see that the quality scores drop off around the 200th base position for the forward reads and the 150th base position for the reverse reads</li> <li>The error rate is considered when determining true biological sequences but is more sensitive to rare biological senquences when reads are trimmed.</li> </ul> <p>Trimming Considerations</p> <ul> <li>The data we are using are 2x250 V4 sequence data. For data that do not overlap as much (i.e. data from the V1-V2 or V3-V4 regions), be wary that this may affect how the reads are merged later on. </li> </ul>"},{"location":"2022_workshops/intro16S/04_quality-control/#trimming","title":"Trimming","text":"<p>Here we notice a dip in quality scores and will trim using the base DADA2 filters:</p> <pre><code># create new file names for filtered forward/reverse fastq files\n# name each file name in the vector with the sample name\n# this way we can compare the forward and reverse files \n# when we filter and trim\nfiltForward &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltReverse &lt;- file.path(path, \"filtered\", paste0(sampleNames, \"_R_filt.fastq.gz\"))\nnames(filtForward) &lt;- sampleNames\nnames(filtReverse) &lt;- sampleNames\n\n# Now we will filter and trim our sequences\nout &lt;- filterAndTrim(\n  path2Forward,\n  filtForward,\n  path2Reverse, \n  filtReverse,\n  truncLen = c(200,150),\n  maxN=0, \n  maxEE=c(2,2), \n  truncQ=2, \n  rm.phix=TRUE,\n  compress=TRUE)\n</code></pre> <p>What do these options mean?</p> <ul> <li><code>truncLen</code>: truncate reads after this base <ul> <li>Here we truncate after base 200 for the forward reads and after basae 150 for the reverse reads</li> </ul> </li> <li><code>maxN</code>: After truncation, sequences with more than maxN Ns will be discarded. Note that dada does not allow Ns.</li> <li><code>maxEE</code>: After truncation, reads with higher than maxEE \"expected errors\" will be discarded.</li> <li><code>truncQ</code>: Truncate reads at the first instance of a quality score less than or equal to <code>truncQ</code></li> <li><code>rm.phix</code>: If TRUE, discard reads that match against the phiX genome<ul> <li>Illumina control libraries derived from a PhiX genome, are used to reduce ambiguity in base calls when highly repetitive sequences are generated. It is pertinent to remove reads mapping to the PhiX genome to ensure you are assessing your microbial community and not the Illumina control run.</li> </ul> </li> <li><code>compress</code>:  If TRUE, the output fastq file(s) are gzipped</li> </ul>"},{"location":"2022_workshops/intro16S/05_error-asv/","title":"Error Model & ASVs","text":""},{"location":"2022_workshops/intro16S/05_error-asv/#dada2-error-model","title":"DADA2 Error Model","text":"<ul> <li>The DADA2 error model attempts to assess whether a sequence is too abundant to be explained by errors in amplicon sequencing. </li> </ul> <p>Here we will leverage this model to learn error rates and then plot them:</p> <pre><code># Learn Error Rates\n\n# dada2 uses a parametric model to learn the error rates\n# for each sequence\nerrForward &lt;- learnErrors(filtForward)\nerrReverse &lt;- learnErrors(filtReverse)\n\n# plot the error rate against theoretical error rates\nplotErrors(errForward,nominalQ=TRUE)\n</code></pre> <p></p> <p>Info</p> <p>So the red line indicates our expected error rate. Essentially, as the quality score gets better so does our error rate.  The black points/line our are actual error rates and we are looking for the trend of the black line to match the trend of the red line.  Here we expect a little deviation since our sample has been subsampled.</p>"},{"location":"2022_workshops/intro16S/05_error-asv/#inferring-sequence-variants","title":"Inferring Sequence Variants","text":"<ul> <li>So far, we have assigned p-values for each sequence in each sample</li> <li>DADA2 then tries to determine which sequences are of biological origin and which aren\u2019t by assessing which sequences are present in other samples</li> <li>If a sequence is present in another sample, it is more likely that it is a real biological sequence</li> </ul> <pre><code># Infer Sequnce Variants\n\n# we will now run the dada2 algorithm \n# this algorithm delivers \"true\" sequence variants\n# with information gathered from the error model \n# generated above\ndadaForward &lt;- dada(filtForward, err=errForward)\ndadaReverse &lt;- dada(filtReverse, err=errReverse)\n\n# let's get a summary of our first sample\ndadaForward[[1]]\n</code></pre> <pre><code>dada-class: object describing DADA2 denoising results\n35 sequence variants were inferred from 430 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n</code></pre> <p>Info</p> <p>Here we note that even though we have 430 unique sequences in our data, only 35 of them have been deemed true sequence variants.</p>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/","title":"Merging, Chimeras & Taxonomy","text":""},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#merging-reads","title":"Merging Reads","text":"<ul> <li>For paired-end data there is a good deal of overlap between the forward and reverse read</li> <li>To resolve this redundancy, these reads are collapsed into contigs</li> </ul> <p>Let's do this with code now!</p> <pre><code># Merge Read Pairs\n\n# so far we have \"denoised\", so to speak, \n# these sequence variants. We now need to merge the\n# forward and reverse strands\nmergers &lt;- mergePairs(\n  dadaForward,\n  filtForward,\n  dadaReverse, \n  filtReverse, \n  verbose=TRUE)\n</code></pre> <pre><code>619 paired-reads (in 18 unique pairings) successfully merged out of 807 (in 82 pairings) input.\n570 paired-reads (in 29 unique pairings) successfully merged out of 815 (in 136 pairings) input.\n619 paired-reads (in 28 unique pairings) successfully merged out of 868 (in 128 pairings) input.\n713 paired-reads (in 18 unique pairings) successfully merged out of 860 (in 76 pairings) input.\n609 paired-reads (in 29 unique pairings) successfully merged out of 851 (in 133 pairings) input.\n620 paired-reads (in 30 unique pairings) successfully merged out of 810 (in 115 pairings) input.\n679 paired-reads (in 28 unique pairings) successfully merged out of 845 (in 104 pairings) input.\n616 paired-reads (in 28 unique pairings) successfully merged out of 830 (in 106 pairings) input.\n</code></pre> <p>Info</p> <p>Here we see that for each sample we get the number of reads that were able to be successfully merged out of the total number of reads that could be merged.</p>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#asvs-vs-otus","title":"ASVs vs. OTUs","text":"<ul> <li>Now that we have finally merged our sequence variants we are left with an Amplicon Sequence Variant. </li> <li>Traditional 16S metagenomic approaches use OTUs or operational taxonomic units instead of ASVs. </li> <li>Operational taxonomic units (OTUs): clusters of reads that differ by less than a fixed sequence dissimilarity threshold, most commonly 3%  Instead of clustering sequences, methods that resolve amplicon sequence variants (ASVs) distinguish sequence variants differing by as little as one nucleotide. ASVs represent a biological reality and provide nucleotide-level resolution. Callahan, MucMurdie, &amp; Holmes (2017) have demonstrated that \u201cASVs capture all biological variation present in the data, and ASVs inferred from a given data set can be reproduced in future data sets and validly compared between data sets.\u201d</li> </ul>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#asv-table","title":"ASV Table","text":"<p>Now that our sequences are merged we can create an ASV counts table, basically telling us how which samples contain which ASV's:</p> <pre><code># Making a Sequence Table\n\n# now that we have merged sequences we can construct\n# an Amplicon Sequence Variant (ASV) table\nseqtab &lt;- makeSequenceTable(mergers)\n</code></pre>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#chimera-removal","title":"Chimera Removal","text":"<ul> <li>During Sequencing microbial DNA is subjected to PCR to amplify DNA</li> <li>During PCR it is possible for two unrelated templates to form a non-biological hybrid sequence</li> <li>DADA2 finds these chimeras by:<ul> <li>aligning each sequence to more abundant sequences </li> <li>now check low abundant sequences and determine:<ul> <li>can this sequence be created if we mix the left and right sides of the abundant sequences</li> </ul> </li> </ul> </li> </ul> <p>Now in code:</p> <pre><code># Removing Chimeras\n\n# Chimeric sequences occur as errors during PCR \n# when two unrelated templates for a hybrid sequence\n# we will need to remove them before going forward\n\nseqtab.nochim &lt;- removeBimeraDenovo(seqtab, method=\"consensus\", verbose=TRUE)\n</code></pre> <p>Now let's check if any chimeric sequences are removed:</p> <pre><code>## check to see if the dimensions are different\n## between the chimera filtered and unfiltered\n## ASV tables\n\ndim(seqtab)\ndim(seqtab.nochim)\n</code></pre> <pre><code>[1]   8 119\n[1]   8 117\n</code></pre> <p>Info</p> <p>We can see here that 2 chimeric sequences were removed because our before and after sequence count matrices differ by two columns.</p>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#pipeline-quality-control","title":"Pipeline Quality Control","text":"<p>We will also take a moment to do some final QC:</p> <pre><code># Final QC\n\n## we have performed quite a few steps \n## and it would be nice to get a final qc check \n## before assigning taxonomy\ngetN &lt;- function(x) sum(getUniques(x))\nfinalQC &lt;- cbind(\n  out, \n  sapply(dadaForward, getN),\n  sapply(dadaReverse, getN),\n  sapply(mergers, getN),\n  rowSums(seqtab.nochim))\ncolnames(finalQC) &lt;- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\nrownames(finalQC) &lt;- sampleNames\nfinalQC\n</code></pre> <pre><code>           input filtered denoisedF denoisedR merged nonchim\nSRR5690809  1000      905       840       855    619     611\nSRR5690810  1000      937       853       885    570     549\nSRR5690811  1000      937       880       910    619     594\nSRR5690812  1000      924       886       888    713     700\nSRR5690819  1000      938       872       906    609     609\nSRR5690820  1000      916       870       844    620     620\nSRR5690821  1000      921       883       879    679     679\nSRR5690822  1000      940       865       891    616     616\n</code></pre> <p>Info</p> <p>Here we see that we start with 1000 sequences per sample, end up with around 900 after filtering, around 800 after denoising to  find unique sequences, and around 600-700 sequences after merging sequences and removing chimeric sequences.</p>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#assigning-taxonomy","title":"Assigning Taxonomy","text":"<ul> <li>To determine which taxon each  ASV belongs to DADA2 uses a na\u00efve bayes classifier </li> <li>This classifier uses a set of reference sequences with known taxonomy, here we use the SILVA database, as the training set and and outputs taxonomic assignments with bootstrapped confidence</li> </ul> <pre><code># Assigning Taxonomy\n\n# dada2 uses a naive Bayes classifier when\n# assigning taxonomy. This means we need a training\n# set of sequences with known taxonomy information.\n# here we use the silva database\n\ntaxa &lt;- assignTaxonomy(seqtab.nochim, \"../data/silva_nr99_v138.1_train_set.fa.gz\")\n</code></pre>"},{"location":"2022_workshops/intro16S/06_merging-chimeras-taxonomy/#databases","title":"Databases","text":"<p>While we use the SILVA database here, there are other options databases:</p> <ul> <li>NCBI 16S RefSeq Database</li> <li>Greengenes</li> </ul> <p>Time for a break!</p>"},{"location":"2022_workshops/intro16S/07_diversity-analysis/","title":"Diversity Analysis","text":""},{"location":"2022_workshops/intro16S/07_diversity-analysis/#constructing-the-phylogenetic-tree","title":"Constructing the Phylogenetic Tree","text":"<p>We will now construct a phylogenic tree based on our sequence data. To construct our tree we will be first aligning our ASV's using ClustalW and then constructing a phylogenetic tree via the neighborhood joining method. </p> <p>To learn more about ClustalW and the neighborhood joining method visit:</p> <ul> <li>Clustal W and Clustal X version 2.0</li> <li>The neighbor-joining method: a new method for reconstructing phylogenetic trees.</li> </ul> <p>Let's work this in R!</p> <pre><code># extract sequences\n# name the sequences with their sequence so \n# that the ends of the phylogenetic tree are labeled\n# align these sequences\nseqs &lt;- getSequences(seqtab)\nnames(seqs) &lt;- seqs \nmult &lt;- msa(seqs, method=\"ClustalW\", type=\"dna\", order=\"input\")\n\n# convert multiple sequence alignment to a phyDat object\n# calculate the nucleotide distances between ASVs\n# use a neighbor joining algorithm to generate the tree\n# finally calculate the likelihood of the tree given the sequence alignment\nphang.align &lt;- as.phyDat(mult, type=\"DNA\", names=getSequence(seqtab))\ndm &lt;- dist.ml(phang.align)\ntreeNJ &lt;- NJ(dm)\nfit = pml(treeNJ, data=phang.align)\n</code></pre>"},{"location":"2022_workshops/intro16S/07_diversity-analysis/#making-a-phyloseq-object","title":"Making a PhyloSeq Object","text":"<p>Once we have quantified our community, we can analyze its composition. Two main methods of doing so are exploring the alpha and beta diversity of the community. First we will need to take our taxonomic data and pass it to the <code>phyloseq</code> package for easier manipulation:</p> <pre><code># Create phyloseq object\n\n# upload meta data for study\n# ensure the rownames of our meta data are our sample name\nmeta &lt;- read.csv(\"../data/metaData.txt\")\nrownames(meta) &lt;- meta$Run\n\n# combine the ASV table, the meta data, and taxonomic data\n# to create the phyloseq object\nps &lt;- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), \n               sample_data(meta), \n               tax_table(taxa),\n               phy_tree(fit$tree)\n               )\n\n# Update ASV names to be shorter\n\n# The full ASV DNA sequence can be hard to look at\n# for this reason we move the sequence information to \n# the refseq slot of the phyloseq object\ndna &lt;- Biostrings::DNAStringSet(taxa_names(ps))\nnames(dna) &lt;- taxa_names(ps)\nps &lt;- merge_phyloseq(ps, dna)\ntaxa_names(ps) &lt;- paste0(\"ASV\", seq(ntaxa(ps)))\n</code></pre>"},{"location":"2022_workshops/intro16S/07_diversity-analysis/#to-rarefy-or-not-to-rarefy","title":"To Rarefy Or Not To Rarefy?","text":"<ul> <li>Rarefaction curves are used to estimate the fraction of species that have been sequenced and usually result in a plot looking something like the following:</li> </ul> <p>What does this mean?</p> <ul> <li>Green curve: a plateau is present and it appears that most species have been sequenced</li> <li>Blue curve: this appears to be a species rich environment and we have not hit our plateau yet</li> <li>Brown curve: only a small fraction of the species appear to have been sequenced as the curve is rapidly rising</li> </ul> <ul> <li>There has been recent debate about whether or not to rarefy amplicon sequencing data:<ul> <li>Pros: Weiss et al. 2017 have noted that sequencing depth has an effect on ordination space and how species richness is displayed </li> <li>Cons: McMurdie and Holmes 2014 have noted that this depends on the species richness metric. </li> </ul> </li> <li>In this tutorial we won't be applying rarefaction to our data.</li> </ul>"},{"location":"2022_workshops/intro16S/07_diversity-analysis/#alpha-diversity","title":"Alpha Diversity","text":"<ul> <li> <p>Alpha Diversity: diversity of organisms sharing the same community or habitat. Alpha diversity metrics can look at richness, evenness, or both within a sample. </p> <ul> <li>Alpha Diversity Metrics:<ul> <li>Faith\u2019s PD: Phylogenetic diversity</li> <li>Observed OTUs: Richness of community</li> <li>Shannon: Balances richness and evenness</li> <li>Pielou\u2019s Evenness: Evenness of community</li> </ul> </li> </ul> </li> <li> <p>We will use the Shannon or Simpson Diversity indices to measure this complexity per sample.</p> </li> </ul> Optional: How to calculate these diversity metrics <p></p> <ul> <li> <p>Here we note:</p> <ul> <li>Shannon Diversity Index: higher values = higher diversity</li> <li>Simpson Diversity Index: higher values = higher diversity</li> </ul> </li> </ul> <p>In R we can visualize this with:</p> <pre><code># Plotting Alpha Diversity Metrics\nplot_richness(ps, x=\"Host\", measures=c(\"Shannon\", \"Simpson\"), color=\"Host\")+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle=65,hjust=1))\n</code></pre> <p></p> <p>Note</p> <p>When running alpha and beta diversity plots you will notice some errors. This is due to the subsampling we needed to do on this data to ensure multiple users could run this workshop at the same time.</p>"},{"location":"2022_workshops/intro16S/07_diversity-analysis/#beta-diversity","title":"Beta Diversity","text":"<ul> <li> <p>Beta Diversity: diversity between communities. Beta diversity calculates how similar two total ecosystems are.</p> <ul> <li>Beta Diversity<ul> <li>Unweighted Unifrac: Presence / absence phylogenetic distance between samples</li> <li>Weighted Unifrac: Abundance weighted phylogenetic distance between samples</li> <li>Jaccard: Presence / absence distance between samples</li> <li>Bray Curtis: Abundance weighted distance between samples</li> </ul> </li> </ul> </li> <li> <p>Here we will use the weighted UniFrac distance since it aware of phylogenetic distances</p> </li> </ul> Optional: How to calculate UniFrac Distance <p></p> <ul> <li>\\(N\\) is the number of nodes in the tree</li> <li>\\(S\\) is the number of sequences represented by the tree</li> <li>\\(L_i\\) is the branch length between node \\(i\\) and its parent </li> <li>\\(L_j\\) is the total branch length from the root to the tip of the tree for sequence \\(j\\)</li> <li>\\(A_i\\) and \\(B_i\\) are the number of sequences from communities \\(A\\) and \\(B\\) that descend from the node, </li> <li>\\(A_T\\) and \\(B_T\\) are the total number of sequences from communities \\(A\\) and \\(B\\).</li> </ul> <p>Mothur UniFrac Alogrith</p> <p>We can plot this in R code:</p> <pre><code># calculate the unifrac distance between samples \n# plot unifrac distances\nordu = ordinate(ps, \"PCoA\", \"unifrac\", weighted=TRUE)\nplot_ordination(ps, ordu, color=\"Host\")+\n  theme_bw()+\n  labs(title = \"Unifrac Distances\")\n</code></pre> <p></p> <p>Here we note that the wild type and C57BL/6NTac cluster together.</p> Which mouse line do you expect to be more spread on the Bray-Curtis Distance plot? <ul> <li>Laboratory Mouse Line (C57BL/6NTac)</li> <li>Wild Type (Mus musculus domesticus)</li> </ul>"},{"location":"2022_workshops/intro16S/08_differential-abundance/","title":"Differential Abundance","text":"<p>When assessing a microbial community, you might be interested to determine which taxa are differentially abundant between conditions. Given that we have a counts matrix we can use DESeq2!</p>"},{"location":"2022_workshops/intro16S/08_differential-abundance/#phylum-present","title":"Phylum Present","text":"<p>Before we assess which phylum are differentially abundant, a bar plot can be a quick first pass at determining this:</p> <pre><code># transform the sample counts to proportions\n# separate out our proportions\n# separate our our tax info\nps.prop &lt;- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))\notu = data.frame(t(data.frame(ps.prop@otu_table)))\ntax = data.frame(ps.prop@tax_table) \n\n# merge the otu table and phylum column\n# reshape our data to be accepted by ggplot\n# merge taxa data with sample meta data\nmerged &lt;- merge(otu,\n                 tax %&gt;% select(Phylum),\n                 by=\"row.names\") %&gt;%\n  select(-Row.names) %&gt;%\n  reshape2::melt() %&gt;%\n  merge(.,\n        data.frame(ps.prop@sam_data) %&gt;%\n          select(Run,Host),\n        by.x=\"variable\",\n        by.y=\"Run\")\n\n# plot our taxa \ntaxa_plot &lt;- ggplot(merged,aes(x=variable,y=value,fill=Phylum)) +\n  geom_bar(stat='identity') +\n  theme_bw()+\n  theme(axis.text.x = element_text(angle=45,hjust=1))+\n  labs(\n    x=\"\",\n    y=\"Abundance\",\n    title = \"Barplot of Phylum Abundance\"\n  )+\n  facet_wrap(Host ~ ., scales = \"free_x\")\ntaxa_plot\n</code></pre> <p></p> <p>Here we note that the wild type seem to have an abundance of Campylobacteria and the C57BL/6NTac have an abundance of Bacteriodota. Let's see if our DESeq2 results confirm this.</p>"},{"location":"2022_workshops/intro16S/08_differential-abundance/#differential-abundance","title":"Differential Abundance","text":"<p>Differential Abundance measures which taxa are differentially abundant between conditions. So how does it work:</p>"},{"location":"2022_workshops/intro16S/08_differential-abundance/#deseq2-normalization","title":"DESeq2 Normalization:","text":"<ol> <li>Geometric mean per ASV</li> <li>Divide rows by geometric mean</li> <li>Take the median of each sample</li> <li>Divide all ASV counts by that median</li> </ol>"},{"location":"2022_workshops/intro16S/08_differential-abundance/#deseq2-model","title":"DESeq2 Model","text":"<ol> <li>The normalized abundances of an ASV are plotted against two conditions</li> <li>The regression line that connects these data is used to determine the p-value for differential abundance</li> </ol>"},{"location":"2022_workshops/intro16S/08_differential-abundance/#deseq2-p-value","title":"DESeq2 P-Value","text":"<ol> <li>The Slope or \ud835\udefd1 is used to calculate a Wald Test Statistic \ud835\udc4d</li> <li>This statistic is compared to a normal distribution to determine the probability of getting that statistic </li> </ol> <p>Now how do we do this in R?</p> <pre><code># Differential Abundance\n\n## convert phyloseq object to DESeq object this dataset was downsampled and \n## as such contains zeros for each ASV, we will need to\n## add a pseudocount of 1 to continue and ensure the data are still integers\n## run DESeq2 against Host status, and ensure wild type is control,\n## filter for significant changes and add in phylogenetic info\ndds = phyloseq_to_deseq2(ps, ~ Host)\ndds@assays@data@listData$counts = apply((dds@assays@data@listData$counts +1),2,as.integer)\ndds = DESeq(dds, test=\"Wald\", fitType=\"parametric\")\nres = data.frame(\n  results(dds,\n          cooksCutoff = FALSE, \n          contrast = c(\"Host\",\"C57BL/6NTac\",\"Mus musculus domesticus\")))\nsigtab = res %&gt;%\n  cbind(tax_table(ps)[rownames(res), ]) %&gt;%\n  dplyr::filter(padj &lt; 0.05) \n\n## order sigtab in direction of fold change\nsigtab &lt;- sigtab %&gt;%\n  mutate(Phylum = factor(as.character(Phylum), \n                        levels=names(sort(tapply(\n                          sigtab$log2FoldChange, \n                          sigtab$Phylum, \n                          function(x) max(x)))))\n  )\n\n# as a reminder let's plot our abundance data again\ntaxa_plot\n\n## plot differential abundance\nggplot(sigtab , aes(x=Phylum, y=log2FoldChange, color=padj)) + \n  geom_point(size=6) + \n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  ggtitle(\"Mus musculus domesticus v. C57BL/6NTac\")\n</code></pre> <p></p> <p></p> <p>Explanation of Results</p> <ul> <li>Wild type seem to have an abundance of Campylobacteria and the C57BL/6NTac have an abundance of Bacteriodota</li> <li>Proteobacteria are severely downregulated in our C57BL/6NTac mice. However, they only show up in one sample!</li> <li>Be sure that your data are not influenced by outliers!</li> <li>Additionally, we collapsed our ASV's to the Phylum level since all ASV's had an identified phylum</li> </ul> Optional: How do I turn this R markdown into an R script? <ul> <li>run the following code (being sure to change the path to where your script is): </li> <li><code>knitr::purl(\"dada2pipeline.Rmd\")</code></li> <li>You should now find an R script called <code>dada2pipeline.R</code>!</li> </ul> <p>References</p> <ol> <li>Galaxy Project - Metagenomics</li> <li>Microbiome 101</li> <li>Current understanding of the human microbiome</li> <li>Amplicon and metagenomics overview</li> <li>Variable regions of the 16S ribosomal RNA</li> <li>A primer on microbial bioinformatics for nonbioinformaticians</li> <li>usearch</li> <li>Sample Multiplexing Overview</li> <li>DADA2: High resolution sample inference from Illumina amplicon data</li> <li>Chimeric 16S rRNA sequence formation and detection in Sanger and 454-pyrosequenced PCR amplicons</li> <li>DADA2 Pipeline Tutorial (1.16)</li> <li>Statistics How To</li> <li>Hierarchical Clustering in Data Mining</li> <li>Abundance-based dissimilarity metrics</li> <li>Differential expression analysis with DESeq2</li> <li>Introduction to RNA-Seq with Galaxy</li> <li>Evaluation of 16S rRNA Databases for Taxonomic Assignments Using a Mock Community</li> <li>Wild Mouse Gut Microbiota Promotes Host Fitness and Improves Disease Resistance</li> <li>Normalization and microbial differential abundance strategies depend upon data characteristics</li> <li>Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible</li> <li>A Primer on Metagenomics</li> <li>Clustal W and Clustal X version 2.0</li> <li>The neighbor-joining method: a new method for reconstructing phylogenetic trees.</li> <li>Large-scale contamination of microbial isolate genomes by Illumina PhiX control</li> <li>Dadasnake, a Snakemake implementation of DADA2 to process amplicon sequencing data for microbial ecology</li> </ol>"},{"location":"2022_workshops/introAlphaFold/00_introduction/","title":"Introduction","text":""},{"location":"2022_workshops/introAlphaFold/00_introduction/#genome-technology-status-and-future-directions","title":"Genome Technology, Status and Future Directions","text":"<p>November 29, 2022</p>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#guest-speaker-bioe291","title":"Guest Speaker, BIOE291","text":""},{"location":"2022_workshops/introAlphaFold/00_introduction/#adelaide-rhodes-phd","title":"Adelaide Rhodes, Ph.D.,","text":"<p>Senior Bioinformatics Scientist</p> <p>Copy of Slides </p>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#assignment-before-next-week","title":"Assignment before Next Week","text":"<p>Read brief blog post about Alphafold </p>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#computer-set-up-for-hands-on-practice","title":"Computer Set Up for Hands On Practice","text":"<ul> <li>Be prepared  to code (i.e.,  work off a laptop/desktop)</li> <li>Account on Tufts HPC</li> <li>Confirm you can log in to Open OnDemand</li> <li>VPN if accessing the HPC from off campus</li> <li>PyMOL</li> </ul>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#introduction-to-alphafold2","title":"Introduction to AlphaFold2","text":"<p>December 6, 2022</p>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Jason Laird, M.S., Bioinformatics Scientist</li> </ul>"},{"location":"2022_workshops/introAlphaFold/00_introduction/#goals","title":"Goals","text":"<ul> <li>Understand the background of Protein Structure Prediction</li> <li>Understand how AlphaFold2 takes a protein sequence and creates a predicted protein structure</li> <li>Analyze AlphaFold2 output and determine how well it compares to available structures</li> <li>Visualize protein structures in PyMOL</li> <li>Optional: Write an AlphaFold2 batch script</li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/","title":"Background","text":""},{"location":"2022_workshops/introAlphaFold/01_background/#protein-organization","title":"Protein Organization","text":"<ul> <li> <p>We are composed mostly of proteins, and proteins are composed of amino acids (e.g. Tryptophan). Proteins can have four levels of organization:</p> </li> <li> <p>Primary Structure: amino acid sequence</p> </li> <li>Secondary Structure: amino acid sequences linked by hydrogen bonds</li> <li>Tertiary Structure: organization of secondary structures</li> <li>Quaternary Structure: organization of multiple amino acid chains</li> </ul> <p></p>"},{"location":"2022_workshops/introAlphaFold/01_background/#the-importance-of-protein-structure","title":"The Importance of Protein Structure","text":"<ul> <li>Can help determine what a protein does</li> <li>Often more conserved than the amino acid sequences that form them</li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/#protein-structure-problem","title":"Protein Structure Problem","text":"<p>As of now there are a few different ways to predict protein structure in the lab:</p> <ul> <li>X-ray Crystallography</li> <li>Nuclear Magnetic Resonance (NMR) Spectroscopy</li> <li>3D Electron Microscopy</li> </ul> <p>However there are 100,000,000 known distinct proteins, each with a unique structure that determines function. Determining protein structure is time consuming and as such only a small fraction of exact 3D structures are known.</p> Where can I find protein structure information? <p>You can use the Protein Data Bank (PDB) to find information on a protein's structure and the study that created that structure.</p>"},{"location":"2022_workshops/introAlphaFold/01_background/#levinthals-paradox","title":"Levinthal's Paradox","text":"<ul> <li>Finding the native folded state of a protein by random searching of all possible configurations would take an enormous amount of time</li> <li>However, proteins can often fold within seconds</li> <li>Meaning some process must be guiding this folding</li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/#using-sequence-to-predict-structure","title":"Using Sequence To Predict Structure","text":"<ul> <li>Instead of laboratory experimentation, there have been massive efforts to use a protein\u2019s sequence to determine structure</li> <li>In 1994, the Critical Assessment of Structure Protein (CASP) was established as a biennial assessment of methods to predict structure from sequence</li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/#how-could-sequence-predict-structure","title":"How Could Sequence Predict Structure?","text":"<ul> <li>Sequences can be aligned to one another to find overlaps</li> <li>These sequences are aligned with one another as to best match similar regions, this is called a multiple sequence alignment or MSA</li> <li>These sequences don\u2019t always line up perfectly and as such we see:<ul> <li>Conserved positions: where the letter does not change</li> <li>Coevolved positions: where the letter will change with another letter</li> <li>Specificity Determining positions: where the letter is consistently different </li> </ul> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/#why-is-an-msa-useful-in-structure-prediction","title":"Why is an MSA Useful In Structure Prediction?","text":"<ul> <li>The theory is that residues that coevolve are generally close to each other in the protein\u2019s folded state</li> <li>So, by assessing what residues change together we get an idea of where they might be spatially!</li> </ul>"},{"location":"2022_workshops/introAlphaFold/01_background/#our-data","title":"Our Data","text":"<p>Today we will be comparing AlphaFold2's structure prediction of Proliferating Cell Nuclear Antigen (PCNA) and DNA ligase 1 (LIG1)!</p>"},{"location":"2022_workshops/introAlphaFold/02_setup/","title":"Setup","text":""},{"location":"2022_workshops/introAlphaFold/02_setup/#log-into-the-hpc-clusters-on-demand-interface","title":"Log into the HPC cluster\u2019s On Demand Interface","text":"<ul> <li>Open a Chrome browser and go to On Demand</li> <li>Log in with your Tufts Credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <pre><code>[tutln01@login001 ~]$\n</code></pre> <ul> <li>This indicates you are logged in to the login node of the cluster. Please do not run any program from the login node.</li> </ul> Are you logged into OnDemand? <ul> <li>Yes</li> <li>No</li> </ul>"},{"location":"2022_workshops/introAlphaFold/02_setup/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<ul> <li>To run our analyses we will need to move from the login node to a compute node. We can do this by entering:</li> </ul> <pre><code>srun -p batch --time=3:00:00 -n 2 --mem=4g --reservation=bioworkshop --pty bash\n</code></pre> <p>Where:</p> <p>Explanation of Commands</p> <ul> <li><code>srun</code>: SLURM command to run a parallel job</li> <li><code>-p</code>: asking for a partition, here we are requesting the batch partition</li> <li><code>--time</code>: time we need here we request 3 hours</li> <li><code>-n</code>:  number of CPUs needed here we requested 2</li> <li><code>--mem</code>:  memory we need here we request 4 Gigabytes</li> <li><code>--reservation</code>: the reservation of compute resources to use here we use the <code>bioworkshop</code> reservation</li> <li><code>--pty</code>: get a pseudo bash terminal</li> </ul> <p>Warning</p> <p>The <code>bioworkshop</code> reservation will be unavailable after December 7th. This reservation is a temporary reservation for this class. </p> <ul> <li>When you get a compute node you'll note that your prompt will no longer say login and instead say the name of the node:</li> </ul> <pre><code>[tutln01@c1cmp048 ~]$\n</code></pre>"},{"location":"2022_workshops/introAlphaFold/02_setup/#set-up-for-analysis","title":"Set Up For Analysis","text":"<ul> <li>To get our AlphaFold data we will enter:</li> </ul> <pre><code>cp -r /cluster/tufts/bio/tools/training/af2Workshop ./\n</code></pre> <ul> <li>You will note that we are copying an existing directory with AlphaFold output rather than generating it. This is because depending on the protein and compute resource availability, running AlphaFold can take a few hours to over a day. At the end of this workshop will be instructions for creating a batch script to run AlphaFold. </li> <li>Today we will examine how well AlphaFold predicted the structures of Proliferating Cell Nuclear Antigen and DNA ligase 1.</li> </ul>"},{"location":"2022_workshops/introAlphaFold/02_setup/#proliferating-cell-nuclear-antigen-pcna","title":"Proliferating Cell Nuclear Antigen (PCNA)","text":"<p>PCNA is a very well conserved protein across eukaryotes and even Archea. It acts as a processivity factor of DNA Polymerase delta, necessary for DNA replication:</p> <p></p> <p>Aside from DNA replication PCNA is involved in:</p> <ul> <li> <p>chromatin remodelling </p> </li> <li> <p>DNA repair</p> </li> <li> <p>sister-chromatid cohesion</p> </li> <li> <p>cell cycle control</p> </li> </ul> <p>It should also be noted that PCNA is a multimeric protein consisting of three monomers.</p>"},{"location":"2022_workshops/introAlphaFold/02_setup/#dna-ligase-1-lig1","title":"DNA ligase 1 (LIG1)","text":"<p>As evidenced by the picture above, DNA Ligase 1 is also involved in DNA replication but also DNA repair. As a part of the DNA replication machinery, DNA Ligase 1 joins Okazaki fragments during lagging strand DNA sythesis. This ligase also interacts with PCNA:</p> <p></p> <p>Here we note that DNA ligase is a monomer consisting of the following domains:</p> <ul> <li>PCNA interacting motif</li> <li>Oligemer Binding Fold Domain</li> <li>Adenylation Domain</li> <li>DNA Binding Domain</li> </ul> <p>The contact between the PCNA interacting motif and PCNA induce a conformational change to create the DNA ligase catalytic region. </p>"},{"location":"2022_workshops/introAlphaFold/02_setup/#fasta-format","title":"FASTA Format","text":"<p>So we have copied over our:</p> <ul> <li>initial data</li> <li>the scripts used to run AlphaFold2</li> <li>AlphaFold2 output</li> </ul> <p>We will start by taking a look at the input data for PCNA, it's protein sequence. We can check it out by running the following command:</p> <pre><code>cat af2Workshop/data/1AXC.fasta \n</code></pre> <pre><code>&gt;1AXC_1|Chain A|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain B|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n&gt;1AXC_1|Chain C|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain D|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n&gt;1AXC_1|Chain E|PCNA|Homo sapiens (9606)\nMFEARLVQGSILKKVLEALKDLINEACWDISSSGVNLQSMDSSHVSLVQLTLRSEGFDTYRCDRNLAMGVNLTSMSKILKCAGNEDIITLRAEDNADTLALVFEAPNQEKVSDYEMKLMDLDVEQLGIPEQEYSCVVKMPSGEFARICRDLSHIGDAVVISCAKDGVKFSASGELGNGNIKLSQTSNVDKEEEAVTIEMNEPVQLTFALRYLNFFTKATPLSSTVTLSMSADVPLVVEYKIADMGHLKYYLAPKIEDEEGS\n&gt;1AXC_2|Chain F|P21/WAF1|Homo sapiens (9606)\nGRKRRQTSMTDFYHSKRRLIFS\n</code></pre> <p>What does this mean?</p> <ul> <li>Here we have 6 sequences (indicating this protein is a multimer) and each sequence has two lines:<ul> <li>A line starting with <code>&gt;</code> which is the sequence header and contains information about the sequence</li> <li>A second line with the amino acid sequence</li> <li>Note here that the fasta file is called <code>1AXC.fasta</code> and does not mention PCNA. This is because 1AXC is the PDB code for the PCNA structure we plan to use. </li> </ul> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/03_af2/","title":"AlphaFold2 Algorithm","text":""},{"location":"2022_workshops/introAlphaFold/03_af2/#alphafold2-algorithm","title":"AlphaFold2 Algorithm","text":""},{"location":"2022_workshops/introAlphaFold/03_af2/#enter-alphafold2","title":"Enter AlphaFold2","text":"<ul> <li>Google\u2019s DeepMind team Entered AlphaFold 2 in CASP14 </li> <li>Achieved a median Global Distance Test Score of 92.4 <ul> <li>This score essentially says: How close is my predicited structure to the known structure?</li> </ul> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/03_af2/#alphafold2-algorithm_1","title":"AlphaFold2 Algorithm","text":"<ul> <li>AlphaFold 2 works by:<ul> <li>starts with a user's query protein sequence</li> <li>finding similar sequences to that query</li> <li>aligns these sequences to create a mutliple sequence alignment</li> <li>uses available structure data based on query sequence to create initial distances between residues</li> <li>uses a neural network to iteratively update the distances between residues by using information from the sequence alignment</li> <li>passes this to another neural network to determine how these residues are oriented in 3D space</li> </ul> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/03_af2/#_1","title":"AlphaFold2 Algorithm","text":""},{"location":"2022_workshops/introAlphaFold/05_af2_output/","title":"AlphaFold2 Output","text":""},{"location":"2022_workshops/introAlphaFold/05_af2_output/#alphafold-output","title":"AlphaFold Output","text":"<ul> <li>Here we predicted the structures of PCNA and LIG1. Let's examine the output by navigating to the PCNA output folder:</li> </ul> <p><pre><code>cd af2Workshop/pcna/1AXC/\nls \n</code></pre>  - You should see output that looks like the following:</p> <pre><code>features.pkl  pcna_ranked_1.pdb  pcna_ranked_4.pdb                  relaxed_model_2_multimer.pdb  relaxed_model_5_multimer.pdb  result_model_3_multimer.pkl  timings.json                    unrelaxed_model_3_multimer.pdb\nmsas          pcna_ranked_2.pdb  ranking_debug.json            relaxed_model_3_multimer.pdb  result_model_1_multimer.pkl   result_model_4_multimer.pkl  unrelaxed_model_1_multimer.pdb  unrelaxed_model_4_multimer.pdb\npcna_ranked_0.pdb  pcna_ranked_3.pdb  relaxed_model_1_multimer.pdb  relaxed_model_4_multimer.pdb  result_model_2_multimer.pkl   result_model_5_multimer.pkl  unrelaxed_model_2_multimer.pdb  unrelaxed_model_5_multimer.pdb\n</code></pre> <ul> <li>Where:</li> </ul> File/Directory Description features.pkl A pickle file w/ input feature NumPy arrays msas A directory containing the files describing the various genetic tool hits that were used to construct the input MSA. unrelaxed_model_*.pdb A PDB file w/ predicted structure, exactly as outputted by the model relaxed_model_*.pdb A PDB file w/ predicted structure, after performing an Amber relaxation procedure on the unrelaxed structure prediction ranked_*.pdb A PDB file w/ relaxed predicted structures, after reordering by model confidence (using predicted LDDT (pLDDT) scores). ranked_1.pdb = highest confidence ranked_5.pdb = lowest confidence ranking_debug.json A JSON file w/pLDDT values used to perform the model ranking, and a mapping back to the original model names. timings.json A JSON file w/times taken to run each section of the AlphaFold pipeline. result_model_*.pkl A pickle file w/ a nested dictionary of the various NumPy arrays directly produced by the model: StructureModule Output, Distograms, Per-residue pLDDT scores, predicted TMscore, predicted pairwise aligned errors"},{"location":"2022_workshops/introAlphaFold/05_af2_output/#assessing-alphafold2-accuracy","title":"Assessing AlphaFold2 Accuracy","text":"<ul> <li>We can assess the accuracy of the AlphaFold prediction using:<ul> <li>Predicted Local Distance Difference Test (pLDDT)</li> <li>Predicted Alignment Error</li> </ul> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/05_af2_output/#predicted-local-distance-difference-test-plddt","title":"Predicted Local Distance Difference Test (pLDDT)","text":"<ul> <li>per-residue confidence metric\u00a0 ranging from 0-100 (100 being the highest confidence)</li> <li>Regions below 50 could indicate disordered regions</li> </ul>"},{"location":"2022_workshops/introAlphaFold/05_af2_output/#predicted-alignment-error-pae","title":"Predicted Alignment Error (PAE)","text":"<ul> <li>The color at (x, y) corresponds to the expected distance error in residue x\u2019s position, when the prediction and true structure are aligned on residue y.</li> <li>So, in the example below:<ul> <li>The darker color indicates a lower error</li> <li>When we are aligning on residue 300, we are more confident in the position of residue 200 and less confident in the position of residue 600</li> </ul> </li> </ul> <ul> <li>The example in the above came from a multimer prediction</li> <li>Here we see that the error is higher when assessing the position between the two chains:</li> </ul>"},{"location":"2022_workshops/introAlphaFold/05_af2_output/#plotting-structure-prediction-information","title":"Plotting Structure Prediction Information","text":"<ul> <li>We can leverage the <code>pkl</code> files to gain insight into our structure predictions. To do so we use a python script provided by the VIB Bioinformatics Core which we call <code>vizaf2.py</code>. First we will need to move back up one directory and load the AlphaFold module so that we have the packages needed to run our script.</li> </ul> <pre><code>cd ../../\n</code></pre> <pre><code>ls\n</code></pre> <p>Do you see the following output?</p> <pre><code>data  lig1  lig1af2.sh  pcna  pcnaaf2.sh  vizaf2.py\n</code></pre> <p>To run our script we will need to load some tools first. We can load tools on the cluster by loading what are called modules. To load the module we will need, go ahead and enter the following command:</p> <pre><code>module load alphafold/2.2.0\n</code></pre> <ul> <li>Now we will need to feed our script three arguments:</li> <li><code>--input_dir</code> input directory with model files mentioned above</li> <li><code>--output_dir</code> output directory to put our plots of model information</li> <li><code>--name</code> optional prefix to add to our file names</li> </ul> <pre><code>python vizaf2.py --input_dir pcna/1AXC/ --output_dir pcna/visuals/ --name pcna\n</code></pre> <ul> <li>Running this will generate two images in your output directory:</li> <li><code>pcna_coverage_LDDT.png</code> - plots of your msa coverage and pLDDT scores per residue per model</li> <li><code>pcna_PAE.png</code> - plots of your predicted alignment error for each of your models</li> </ul> <p>pcna_coverage_LDDT.png</p> <p></p> <p>pcna_PAE.png</p> <p></p> <ul> <li>Now that we have these plots for the PCNA structure prediction, let's run this on the LIG1 prediction as well!</li> </ul> <pre><code>python vizaf2.py --input_dir lig1/1X9N/ --output_dir lig1/visuals/ --name lig1\n</code></pre> <p>lig1_coverage_LDDT.png</p> <p></p> <p>lig1_PAE.png</p> <p></p>"},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/","title":"PyMOL Visualization","text":""},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/#visualizing-with-pymol","title":"Visualizing With Pymol","text":"<ul> <li>In the previous slide we plotted our MSA alignment, the pLDDT scores, and the predicted alignement error. However, it is also useful to visualize the actual predicted protein structure and compare it to the known structure if there is one. Here we use a software called PyMOL to do just that:</li> </ul> <ul> <li>Here we see that PyMOL takes either the PDB ID or a PDB file and creates a vizualization for us to examine. If you have not done so already please download PyMOL and open the app. You should see a window like the follwing:</li> </ul> <ul> <li>Here we have a:</li> <li>History Window with log of previous commands</li> <li>Command Interface to enter PyMOL commands</li> <li>List of Objects Loaded which list of objects/proteins that have been loaded into PyMOL</li> <li> <p>Visualization Window to visualize protiens loaded into PyMOL</p> </li> <li> <p>Let's try on our data!</p> </li> </ul>"},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/#download-alphafold-output","title":"Download AlphaFold Output","text":"<ul> <li>First we will need to download our predicted structure pdb files. To this go to Files &gt; Home Directory:</li> </ul> <ul> <li>Then navigate to your AlphaFold Workshop directory, where you will note the two folders we examined earlier that have our AlphaFold Outputs:</li> </ul> <ul> <li>In each folder you will note a <code>visuals</code> folder and an ID number. Navigate to the ID number folder and download the file \"ranked_0.pdb\" - this structure is AlphaFold's best prediction of the protein's structure.</li> </ul>"},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/#alphafold-output-in-pymol","title":"AlphaFold Output In PyMOL","text":"<ul> <li>To visualize these protein structures in PyMOL go to File &gt; Open - then choose your pdb files. </li> <li>Loading two objects can make it difficult to see examine both individually, so enter the following command to hide lig1:</li> </ul> <p><pre><code>disable lig1_ranked_0\n</code></pre> - Now you should only see <code>pnca_ranked_0</code>. It would be interesting to see how well this prediction lines up with the know structure for PCNA. We can load that structure with the following command</p> <p><pre><code>fetch 1axc\n</code></pre> - To see how well our predicted structure lines up we can compare the two by aligning them:</p> <pre><code>align pcna_ranked_0, 1axc\n</code></pre> <p></p> <ul> <li>In the history window you'll note that when we aligned our structures we were given an RMSD or root mean square deviation value. The smaller this value is, the better our two structures have aligned. </li> <li>Now that we have aligned the predicted PCNA structure, repeat these steps to align LIG1 (the PDB ID for LIG1 id <code>1x9n</code>).</li> </ul>"},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/#alphafold2-limitations","title":"AlphaFold2 Limitations","text":"<p>AlphaFold2 attempts to predict protein structures based on available structure data in the PDB. But do these structures in the PDB reflect actual protein structures?</p> <ul> <li>PDB structures are usually created from experiments where the context of that structure is specific to the study question. </li> </ul> <p>For example there are lots of studies examining what a particular protein structure looks like when bound to ions, when it\u2019s chemically modified, or when its in larger complexes</p> <ul> <li>Protein interactions/multimers might not be captured in the PDB database. Given this, AlphaFold2\u2019s multimeric prediction might not be reflective of the true interaction structure.</li> <li>Proteins can also contain disordered regions (i.e. loops), which are difficult to crystallize and as such AlphaFold\u2019s prediction of these disordered regions is bound to be poor.</li> </ul> <p>AlphaFold2 is indeed a powerful tool but just be aware of what it is prediciting and if any of the items mentioned above interfere with the study question you are using AlphaFold2 to answer!</p>"},{"location":"2022_workshops/introAlphaFold/06_pymol_viz/#references","title":"References","text":"<p>References</p> <ol> <li>PROTEIN</li> <li>Protein Function</li> <li>Analyzing Protein Structure and Function</li> <li>serial scanning 3D electron microscopy</li> <li>AI revolutions in biology</li> <li>Methods for Determining Atomic Structures</li> <li>X-ray crystallography</li> <li>AlphaFold</li> <li>Levinthal's paradox</li> <li>AlphaFold: a solution to a 50-year-old grand challenge in biology</li> <li>Protein Structure Prediction Center</li> <li>Neural network</li> <li>Understanding LSTMs</li> <li>Attention Is All You Need</li> <li>Transformer Neural Network: Step-By-Step Breakdown of the Beast</li> <li>FASTA Format</li> <li>Multiple Sequence Alignment</li> <li>Origins of coevolution between residues distant in protein 3D structures</li> <li>AlphaFold 2 is here: what\u2019s behind the structure prediction miracle</li> <li>Alphafold Github</li> <li>Q9FX77</li> <li>Limitations of AlphaFold</li> </ol>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/","title":"AlphaFold on Tufts HPC Cluster","text":""},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#general-tufts-hpc-cluster-access-info","title":"General Tufts HPC Cluster Access Info","text":"<p>Please review  https://tufts.box.com/v/Pax-User-Guide before proceeding forward.</p>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#login-and-allocate-computing-resources","title":"Login and Allocate Computing Resources","text":"<p>Tips:</p> <ol> <li>Login. If you have a Mac, use the Terminal app. If you have a </li> <li>If you need to use GPU resources and don't have access to contrib node partitions, \"preempt\" is the best option</li> </ol>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#alphafold","title":"Alphafold","text":"<p>The Alphafold script is available for everyone in <code>/cluster/tufts/hpc/tools/alphafold/2.2.0/runaf2test.sh</code></p> <p>Make a copy of the file to your own folder  (e.g. your home directory):</p> <p><code>$ cp /cluster/tufts/hpc/tools/alphafold/2.2.0/runaf2test.sh /your/own/directory</code></p> <p>Go to your own copy of the script:</p> <pre><code>#!/bin/bash\n#SBATCH -p preempt  #if you DO have ccgpu access, use \"ccgpu\"\n#SBATCH -n 8    # 8 cpu cores\n#SBATCH --mem=64g       #64GB of RAM\n#SBATCH --time=2-0      #run 2 days, up to 7 days \"7-00:00:00\"\n#SBATCH -o output.%j\n#SBATCH -e error.%j\n#SBATCH -N 1\n#SBATCH --gres=gpu:a100:1    # number of GPUs. please follow instructions in \"Pax User Guide\" when submit jobs to different partition and selecting different GPU architectures. \n\nmodule load alphafold/2.2.0\nmodule list\nnvidia-smi\n\nmodule help alphafold/2.2.0 # this command will print out all input options for \"runaf2\" command\n\n#Please use your own path/value for the following variables\n#Make sure to specify the outputpath to a path that you have write permission\noutputpath=/cluster/tufts/hpc/tools/alphafold/2.2.0/test\nfastapath=/cluster/tufts/hpc/tools/alphafold/2.2.0/T1050.fasta\nmaxtemplatedate=2020-06-10\n\nsource activate alphafold2.2.0\n\n#running alphafold 2.2.0\n\nrunaf2 -o $outputpath -f $fastapath -t $maxtemplatedate\n</code></pre> <p>Make sure you specify the <code>outputpath</code> to a path that you have write permission. </p> <p>Make sure you specify the <code>fastapath</code> to FASTA file containing the protein sequence for which you wish to predict the structure.</p> <p>Make sure <code>maxtemplatedate</code> is set to be before the release date of the structure. </p>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#module-help","title":"Module Help","text":"<p>Please see <code>$ module help alphafold/2.2.0</code> for additional input options ( Required Parameters &amp; Optional Parameters) for <code>runaf2</code> command.</p> <pre><code>----------- Module Specific Help for 'alphafold/2.2.0' ------------\n\n    This module adds AlphaFold 2.2.0 to the PATH\nRun AlphaFold 2.2.0 with:\nrunaf2 &lt;Required parameters&gt; &lt;Optional Parameters&gt;\nPlease make sure all REQUIRED parameters are given\n\nRequired Parameters:\n-o &lt;output_dir&gt;       Path to a directory that will store the results, make sure the user has write permission to the directory.\n-f &lt;fasta_path&gt;       Path to a FASTA file containing sequence. If a FASTA file contains multiple sequences, then it will be folded as a multimer\n-t &lt;max_template_date&gt; Maximum template release date to consider (ISO-8601 format - i.e. YYYY-MM-DD). Important if folding historical test sets\n\nOptional Parameters:\n-g &lt;use_gpu&gt;          Enable NVIDIA runtime to run with GPUs (default: true)\n-n &lt;openmm_threads&gt;   OpenMM threads (default: all available cores)\n-a &lt;gpu_devices&gt;      Comma separated list of devices to pass to 'CUDA_VISIBLE_DEVICES' (default: 0)\n-m &lt;model_preset&gt;     Choose preset model configuration - the monomer model, the monomer model with extra ensembling, monomer model with pTM head, or multimer model (default: 'monomer')\n-c &lt;db_preset&gt;        Choose preset MSA database configuration - smaller genetic database config (reduced_dbs) or full genetic database config (full_dbs) (default: 'full_dbs')\n-p &lt;use_precomputed_msas&gt; Whether to read MSAs that have been written to disk. WARNING: This will not check if the sequence, database or configuration have changed (default: 'false')\n</code></pre>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#submit-job","title":"Submit Job","text":"<p>To submit your job, go to the folder that contains <code>runaf2test.sh</code></p> <p>From command line, submit with <code>$ sbatch runaf2test.sh</code></p> <p>Then follow the instructions in  https://tufts.box.com/v/Pax-User-Guide to check your job status.</p>"},{"location":"2022_workshops/introAlphaFold/07_af2_batch/#alphafold-output","title":"AlphaFold output","text":"<p>The outputs will be in a subfolder of <code>output_dir</code> that you specified in <code>runaf2test.sh</code>. </p> <p>They include the computed MSAs, unrelaxed structures, relaxed structures, ranked structures, raw model outputs, prediction metadata, and section timings. The<code>output_dir</code> directory will have the following structure:</p> <pre><code>output_dir/\n\n  features.pkl\n\n  ranked_{0,1,2,3,4}.pdb\n\n  ranking_debug.json\n\n  relaxed_model_{1,2,3,4,5}.pdb\n\n  result_model_{1,2,3,4,5}.pkl\n\n  timings.json\n\n  unrelaxed_model_{1,2,3,4,5}.pdb\n\n  msas/\n\n\u200b    bfd_uniclust_hits.a3m\n\n\u200b    mgnify_hits.sto\n\n\u200b    uniref90_hits.sto\n</code></pre> <p>The contents of each output file are as follows:</p> <ul> <li> <p><code>features.pkl</code> \u2013 A <code>pickle</code> file containing the input feature Numpy arrays used by the models to produce the structures.</p> </li> <li> <p><code>unrelaxed_model_*.pdb</code> \u2013 A PDB format text file containing the predicted structure, exactly as outputted by the model.</p> </li> <li> <p><code>relaxed_model_*.pdb</code> \u2013 A PDB format text file containing the predicted structure, after performing an Amber relaxation procedure on the unrelaxed structure prediction, see Jumper et al. 2021, Suppl. Methods 1.8.6 for details.</p> </li> <li> <p><code>ranked_*.pdb</code> \u2013 A PDB format text file containing the relaxed predicted structures, after reordering by model confidence. Here <code>ranked_0.pdb</code> should contain the prediction with the highest confidence, and <code>ranked_4.pdb</code> the prediction with the lowest confidence. To rank model confidence, we use predicted LDDT (pLDDT), see Jumper et al. 2021, Suppl. Methods 1.9.6 for details.</p> </li> <li> <p><code>ranking_debug.json</code> \u2013 A JSON format text file containing the pLDDT values used to perform the model ranking, and a mapping back to the original model names.</p> </li> <li> <p><code>timings.json</code> \u2013 A JSON format text file containing the times taken to run each section of the AlphaFold pipeline.</p> </li> <li> <p><code>msas/</code> - A directory containing the files describing the various genetic tool hits that were used to construct the input MSA.</p> </li> <li> <p><code>result_model_*.pkl</code> \u2013 A <code>pickle</code> file containing a nested dictionary of the various Numpy arrays directly produced by the model. In addition to the output of the structure module, this includes auxiliary outputs such as distograms and pLDDT scores. If using the pTM models then the pTM logits will also be contained in this file.</p> </li> </ul>"},{"location":"2022_workshops/introGenomeTech/01_intro/","title":"Introduction","text":""},{"location":"2022_workshops/introGenomeTech/01_intro/#genome-technology-status-and-future-directions","title":"Genome Technology, Status and Future Directions","text":"<p>November 29, 2022</p>"},{"location":"2022_workshops/introGenomeTech/01_intro/#guest-speaker-bioe291","title":"Guest Speaker, BIOE291","text":""},{"location":"2022_workshops/introGenomeTech/01_intro/#adelaide-rhodes-phd","title":"Adelaide Rhodes, Ph.D.,","text":"<p>Senior Bioinformatics Scientist</p> <p>Copy of Slides </p>"},{"location":"2022_workshops/introGenomeTech/01_intro/#assignment-before-next-week","title":"Assignment before Next Week","text":"<p>Read brief blog post about Alphafold </p>"},{"location":"2022_workshops/introGenomeTech/01_intro/#computer-set-up-for-hands-on-practice","title":"Computer Set Up for Hands On Practice","text":"<ul> <li>Be prepared  to code (i.e.,  work off a laptop/desktop)</li> <li>Account on Tufts HPC</li> <li>Confirm you can log in to Open OnDemand</li> <li>VPN if accessing the HPC from off campus</li> <li>PyMOL</li> </ul>"},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/","title":"Introduction","text":""},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#intro-to-hpcslurm-for-life-sciences","title":"Intro to HPC/Slurm For Life Sciences","text":""},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#october-192022","title":"October 19,2022","text":""},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Delilah Maloney, Senior High Performing Computing Specialist</li> <li>Adelaide Rhodes, Ph.D,  Senior Bioinformatics Scientist</li> <li>Jason Laird, Bioinformatics Scientist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p>"},{"location":"2022_workshops/introHpcSlurm/01_hpc-intro/#description","title":"Description","text":"<p>This introductory virtual workshop will cover the basics of using the Tufts High Performance Computing Cluster (HPC) using OnDemand. Users will be introduced to basic Linux commands and how to use the SLURM job scheduler to run and monitor jobs on the HPC. This workshop is open to the Tufts community and is appropriate for anyone who would like to work on the Tufts HPC Cluster. No prior experience is required. </p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Kyle Monahan, Christina Divoll, Kayla Sansevere, and Uku Uustalu for their review of this content</p>"},{"location":"2022_workshops/introHpcSlurm/02_request-account/","title":"Cluster Account and Storage","text":""},{"location":"2022_workshops/introHpcSlurm/02_request-account/#cluster-account","title":"Cluster Account","text":"<p>Tufts HPC</p> <p>Cluster Account Request https://it.tufts.edu/research-technology/</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/02_request-account/#cluster-storage","title":"Cluster Storage","text":"<ul> <li>Home Directory</li> </ul> <p>Your Home Directory (10GB, fixed) should be <code>/cluster/home/your_utln</code></p> <ul> <li>Reserach Project Storage</li> </ul> <p>Cluster Storage Request https://it.tufts.edu/research-technology/</p> <p>Your research projet storage (from 50GB and up) path should be <code>/cluster/tufts/yourlabname/</code>, and each member of the lab group has a dedicated directory <code>/cluster/tufts/yourlabname/your_utln</code></p> <p>To see your research project storage quota by running the following command from any node on the new cluster Pax:</p> <p><code>$ df -h /cluster/tufts/yourlabname</code> </p> <p>NOTE: Accessing your research project storage space for the first time, please make sure you type out the FULL PATH to the directory.</p> <p>If your group has existing HPC research project storage space set up, please use the same link to request access. </p>"},{"location":"2022_workshops/introHpcSlurm/03_login/","title":"How to Login to Tufts HPC Cluster","text":"<p>Info</p> <ul> <li>VPN<ul> <li>Non-Tufts Network please connect to Tufts VPN</li> </ul> </li> <li>2FA<ul> <li>Tufts Network (not needed for OnDemand</li> <li>FastX11 from  old OnDemand</li> </ul> </li> <li>SSH<ul> <li>The SSH protocol aka Secure Shell is a method for secure remote login from one computer to another. </li> </ul> </li> <li>X Window System (X11)<ul> <li>The X Window System (X11) is an open source, cross platform,  client-server computer software system that provides a GUI in a  distributed network environment.</li> </ul> </li> <li>Login Info<ul> <li>login.cluster.tufts.edu</li> </ul> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/03_login/#terminal","title":"Terminal","text":"<ul> <li> <p>Shell environment (default: bash):</p> <p><code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p>With GUI (Graphical User Interface):</p> <p><code>$ ssh -XC your_utln@login.cluster.tufts.edu</code></p> <p>or</p> <p><code>$ ssh -YC your_utln@login.cluster.tufts.edu</code></p> <p>X Window System need to be locally installed.</p> <p>Now you are on a Login Node of the cluster (login-prod-0x) and in your Home Directory (~). </p> <p><code>$ [your_utln@login-prod-03 ~]</code></p> </li> <li> <p>Setting up SSH keyless access</p> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/03_login/#ondemand-web-interface","title":"OnDemand Web Interface","text":""},{"location":"2022_workshops/introHpcSlurm/03_login/#httpsondemandpaxtuftsedu","title":"https://ondemand.pax.tufts.edu","text":"<p>Use your Tufts UTLN and password to login. </p> <p></p> <p><code>Clusters</code>, you can start a shell access to the HPC cluster. </p> <p><code>Tufts HPC Shell Access</code> = <code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p>OR</p> <p>Use the <code>&gt;_Open in Terminal</code> button in <code>Files</code> to open a terminal in whichever directory you navigated to.</p> <p></p> <p></p> <p>If you need X11 access through OnDemand to display any GUI applications, please temporarily use our Old OnDemand https://ondemand.cluster.tufts.edu <code>Clusters</code> for this option:</p> <p><code>Tufts HPC FastX11 Shell Access</code> = <code>$ ssh -XC your_utln@login.cluster.tufts.edu</code> (with X11 for GUI applications)</p> <p>OR </p> <p>You also have the option to use the <code>Xfce Terminal</code> under new  OnDemand <code>Interactive Apps</code>.</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/03_login/#windows","title":"Windows","text":"<ul> <li>PowerShell</li> <li>Windows Subsystem for Linux (WSL)</li> <li>PuTTY </li> <li>Cygwin </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/","title":"File Transfer","text":""},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#ondemand-file-transfer","title":"OnDemand File Transfer","text":"<p>Note</p> <p>Only for transfering files size up to 976MB per file.</p> <p>Go to OnDemand </p> <p>Under <code>Files</code></p> <p></p> <p>Using the <code>Upload</code> or <code>Download</code> buttons to transfer. </p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#file-transfer-client","title":"File Transfer Client","text":"<ul> <li> <p>Download one of these free file transfer programs:</p> <ul> <li> <p>WinSCP </p> </li> <li> <p>FileZilla </p> </li> <li> <p>Cyberduck </p> </li> </ul> </li> <li> <p>Then use the following information to connect to the cluster:</p> </li> </ul> <pre><code>Hostname: xfer.cluster.tufts.edu\nProtocol: SCP or SFTP\nUse port 22 for SFTP\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#command-line","title":"Command Line","text":"<ul> <li> <p>Terminology:</p> <ul> <li><code>Local_Path:</code> is the path to your files or directory on your local computer</li> <li><code>Cluster_Path:</code> is the path to your files or directory on the cluster<ul> <li><code>Cluster Home Directory: /cluster/home/your_utln/your_folder</code></li> <li><code>Cluster Research Project Storage Space Directory: /cluster/tufts/yourlabname/your_utln/your_folder</code></li> </ul> </li> </ul> </li> <li> <p>Execute these commands from your local machine terminal using this general format to transfer files:</p> <ul> <li> <p><code>$ scp From_Path To_Path</code></p> </li> <li> <p><code>$ rsync From_Path To_Path</code></p> </li> </ul> </li> </ul> <p>Note</p> <p>If you are transfering very large files that could take hours to finish, we would suggest using <code>rsync</code> as it has ability to restart from where it left if interrupted.</p>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#download-from-cluster","title":"Download from cluster","text":"<pre><code>    $ scp your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path  \n\n    $ rsync your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#upload-to-cluster","title":"Upload to cluster","text":"<pre><code>    $ scp Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n\n    $ rsync Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre> <p>Tip</p> <p>If you are transfering a directory use <code>scp -r</code> or <code>rsync -azP</code></p>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#download-from-cluster_1","title":"Download from cluster","text":"<pre><code>    $ scp -r your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path  \n\n    $ rsync -azP your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/04_file-transfer/#upload-to-cluster_1","title":"Upload to cluster","text":"<pre><code>    $ scp -r Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n\n    $ rsync -azP Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/","title":"Terninologies","text":""},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/#cluster-terminologies","title":"Cluster Terminologies","text":""},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/#what-is-cluster","title":"What is \"Cluster\"","text":"<ul> <li>A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Computer clusters have each node set to perform the same/similar tasks, controlled and scheduled by software. </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/#cpu-vs-gpu","title":"CPU vs GPU","text":"<ul> <li> <p>CPU -- Central Processing Unit </p> <ul> <li> <p>A CPU can never be fully replaced by a GPU</p> </li> <li> <p>Can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks</p> </li> </ul> </li> <li> <p>GPU -- Graphics Processing Unit</p> <ul> <li> <p>GPUs were originally designed to create images for computer graphics and video game consoles</p> </li> <li> <p>GPGPU</p> </li> <li> <p>Performing a narrower range of more specialized tasks</p> </li> </ul> </li> </ul> <p></p>"},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/#cores-vs-node","title":"Cores vs Node","text":"<ul> <li>A node is a single computer in the system, which has a number of computing units, aka cores. </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/05_what-is-the-cluster/#memory-vs-storage","title":"Memory vs Storage","text":"<p>The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the  CPU and slower but less expensive and larger options further away.  Generally the fast volatile technologies (which lose data when off  power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".</p> <ul> <li>Memory<ul> <li>Small, fast, expensive</li> <li>Used to store information for immediate use</li> </ul> </li> <li>Storage<ul> <li>Larger, slower, cheaper</li> <li>Non-volatile (retaining data when its power is shut off) </li> </ul> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/06_compute-resources/","title":"Compute Resources","text":""},{"location":"2022_workshops/introHpcSlurm/06_compute-resources/#cpus","title":"CPUs","text":"<p>Resources are orgnized into partitions on the cluster based on functionality and priority.</p> <p>After logging in on the HPC cluster, you can use command <code>sinfo</code> to check the <code>partition</code> you have access to (all partitions listed in the <code>sinfo</code> output).</p> <pre><code>[your_utln@login-prod-01 ~]$ sinfo\nPARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST \ninteractive     up    4:00:00      1    mix c1cmp064 \ninteractive     up    4:00:00      1   idle c1cmp063 \nbatch*          up 7-00:00:00      1  down* p1cmp005 \nbatch*          up 7-00:00:00      1  drain p1cmp056 \nbatch*          up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nbatch*          up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nbatch*          up 7-00:00:00     17  alloc c1cmp[021-022,053],i2cmp003,p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nbatch*          up 7-00:00:00      2   idle p1cmp[016-017] \nmpi             up 7-00:00:00      1  down* p1cmp005 \nmpi             up 7-00:00:00      1  drain p1cmp056 \nmpi             up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nmpi             up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nmpi             up 7-00:00:00     16  alloc c1cmp[021-022,053],p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nmpi             up 7-00:00:00      2   idle p1cmp[016-017] \ngpu             up 7-00:00:00      1    mix p1cmp073 \ngpu             up 7-00:00:00      2  alloc c1cmp[025-026] \nlargemem        up 7-00:00:00      7    mix c1cmp[027-028,030,057,061-062],i2cmp055 \nlargemem        up 7-00:00:00      2  alloc p1cmp[049-050] \nlargemem        up 7-00:00:00      3   idle c1cmp[032,058-059] \npreempt         up 7-00:00:00      2   mix$ p1cmp[094-095] \npreempt         up 7-00:00:00      4  maint p1cmp[090,092,103,109] \npreempt         up 7-00:00:00      1  down* p1cmp005 \npreempt         up 7-00:00:00      2  drain p1cmp[038,056] \npreempt         up 7-00:00:00      3   resv p1cmp[004,009,054] \npreempt         up 7-00:00:00     71    mix cc1gpu[001-005],i2cmp[010-032,038-043,045-051],p1cmp[003,012,015,018,020-021,070-077,079-080,091,093,096,098-102,104-108,110] \npreempt         up 7-00:00:00     25  alloc c1cmp[025-026],i2cmp[004-006,008-009,033-035,037,052-053],p1cmp[006-008,010-011,013-014,019,022-024,055] \npreempt         up 7-00:00:00     20   idle p1cmp[016-017,031-037,039-042,081-086,097] \n</code></pre> <p>OnDemand <code>Misc</code>--&gt;<code>Inventory</code> shows more node details (core count &amp; memory)</p> <p></p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/06_compute-resources/#gpus","title":"GPUs","text":"<p>NVIDIA GPUs are available in <code>gpu</code> and <code>preempt</code> partitions</p> <ul> <li>Request GPU resources with <code>--gres</code>. See details below.</li> <li>Please DO NOT manually set <code>CUDA_VISIBLE_DEVICES</code>.</li> <li>Users can ONLY see GPU devices that are assigned to them with <code>$ nvidia-smi</code>.</li> </ul> <p><code>gpu</code> partition<code>-p gpu</code>:</p> <ul> <li>NVIDIA P100<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 6 on one node)</li> </ul> </li> <li>NVIDIA Tesla K20xm<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:k20xm:1</code>(one Tesla K20xm GPU, can request up to 1 on one node)</li> </ul> </li> <li>NVIDIA A100 (80GB)<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 4 on one node)</li> </ul> </li> </ul> <p><code>preempt</code> partition <code>-p preempt</code></p> <ul> <li>NVIDIA T4<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:t4:1</code>(one T4 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA P100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA rtx_6000<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000:1</code>(one RTX_6000 GPU, can request up to 8 on one node)</li> </ul> </li> <li>NVIDIA V100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:v100:1</code>(one V100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA A100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> </ul> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/07_software-cluster/","title":"Software on the Cluster","text":""},{"location":"2022_workshops/introHpcSlurm/07_software-cluster/#modules","title":"Modules","text":"<ul> <li>A tool that simplifies shell initialization and lets users easily modify their environment during the session with modulefiles</li> <li>Each modulefile contains the information needed to configure the shell for an application. (PATH, LD_LIBRARY_PATH, CPATH, etc.)</li> <li>Modules are useful in managing different versions of applications. </li> <li>Modules can also be bundled into metamodules that will load an entire set of different applications (dependencies). </li> </ul> <p>Useful Module  Commands</p> <ul> <li><code>module av</code> : to check all available modules</li> <li><code>module load/unload</code> : to load or unload a particular module</li> <li><code>module list</code> : to list modules that are loaded</li> <li><code>module purge</code> : purge any loaded modules</li> </ul> <p>To check ALL available modules installed on the cluster:</p> <p><code>[your_utln@login-prod-01 ~]$ module av</code></p> <p>Upon login, environment <code>PATH</code> is set for the system to search executables:</p> <p><code>[your_utln@login-prod-01 ~]$ echo $PATH</code></p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <p>For example, I would like to use <code>blast</code>, to check what versions of blast are available, load the version I would like to use, and use it:</p> <p><code>[your_utln@login-prod-01 ~]$ module av blast</code></p> <pre><code>---------------------- /opt/shared/Modules/modulefiles-rhel6 ----------------------\nblast/2.2.24 blast/2.2.31 blast/2.3.0  blast/2.8.1\n\n---------------------- /cluster/tufts/hpc/tools/module ----------------------------\nblast-plus/2.11.0\n</code></pre> <p><code>[your_utln@login-prod-01 ~]$ module load blast-plus/2.11.0</code></p> <p><code>[your_utln@login-prod-01 ~]$ module list</code></p> <pre><code>Currently Loaded Modulefiles:\n    1) use.own     2) blast-plus/2.11.0\n</code></pre> <p><code>[your_utln@login-prod-01 ~]$ which blastp</code></p> <p><pre><code>/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin/blastp\n</code></pre> <code>[your_utln@login-prod-01 ~]$ echo $PATH</code></p> <pre><code>/cluster/tufts/bio/tools/edirect:/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin:/cluster/home/your_utln/.iraf/bin:/cluster/home/your_utln/.iraf/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <p><code>[your_utln@login-prod-01 ~]$ blastp -version</code></p> <pre><code>blastp: 2.11.0+\n Package: blast 2.11.0, build Aug 17 2021 06:29:22\n</code></pre> <p>I can also unload a loaded modules:</p> <p><code>[your_utln@login-prod-01 ~]$ module unload blast-plus/2.11.0</code></p> <p><code>[your_utln@login-prod-01 ~]$ echo $PATH</code></p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <p>I can unload ALL of the loaded modules:</p> <p><code>[your_utln@login-prod-01 ~]$ module purge</code></p> <p><code>[your_utln@login-prod-01 ~]$ module list</code></p> <pre><code>No Modulefiles Currently Loaded.\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/07_software-cluster/#install-softwarepackages","title":"Install Software/Packages","text":"<ul> <li>R (R command line recommanded)<ul> <li>R/4.0.0</li> <li>gcc </li> <li>gdal</li> <li>curl</li> </ul> </li> <li>Python (Conda env recommanded)<ul> <li>anaconda/3 (older version, source activate)</li> <li>anaconda/2021.05 (newer version, source activate)</li> <li>Use the same version of conda on one conda env every time</li> </ul> </li> <li>Other software compiled from source<ul> <li>gcc</li> <li>cmake</li> <li>... any dependencies, load if available, install if not.</li> <li>Follow instructions (read it through)</li> <li>Use \"--prefix=\" to install in non-standard locations</li> <li>Modify the environment variables !!! (such as PATH, LD_LIBRARY_PATH, CPATH, .etc)</li> </ul> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/","title":"Tmux Session","text":""},{"location":"2022_workshops/introHpcSlurm/08_tmux/#tmux-101","title":"Tmux 101","text":"<p>tmux is a terminal multiplexer.  It lets you switch easily between several programs in one terminal, detach them (they keep running in the background) and reattach them to a different terminal.</p> <p>Cheat Sheet</p> <p>Useful tmux Commands</p> <ul> <li>New Tmux Window <code>tmux new -s mysession</code></li> <li>Detach it <code>CTRL+b d</code></li> <li>List Sessions <code>tmux ls</code></li> <li>Reattach Session <code>tmux a -t mysession</code></li> </ul>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#how-to-use-tmux-on-tufts-hpc-cluster","title":"How to use tmux on Tufts HPC Cluster","text":""},{"location":"2022_workshops/introHpcSlurm/08_tmux/#load-tmux-module","title":"Load tmux module","text":"<p><code>[your_utln@login-prod-01 ~]$ module load tmux</code> Make a note of the login nodename <code>login-prod-01</code> where your tmux session lives.</p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#start-your-tmux-session","title":"Start your tmux session","text":"<p><code>[your_utln@login-prod-01 ~]$ tmux new -s mysession</code></p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#start-your-interactive-session-inside-the-tmux-session-and-run-your-programs","title":"Start your Interactive session inside the tmux session, and run your programs","text":"<p>(Next Session)</p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#detach-your-tmux-session-or-lose-connection","title":"Detach your tmux session OR lose connection...","text":"<p><code>CTRL+b d</code></p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#get-your-work-session-back","title":"Get your work session back","text":"<p>Log back in to the cluster or start a new terminal</p> <p>If you are allocated on a different login node than where your tmux session lives. Simply do: </p> <p><code>[your_utln@login-prod-03 ~]$ ssh login-prod-01</code></p> <p><code>[your_utln@login-prod-01 ~]$ module load tmux</code></p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#check-tmux-sessions","title":"Check tmux sessions","text":"<p><code>[your_utln@login-prod-01 ~]$ tmux ls</code></p>"},{"location":"2022_workshops/introHpcSlurm/08_tmux/#pick-your-session-to-reattach-it","title":"Pick your session to reattach it","text":"<p><code>[your_utln@login-prod-01 ~]$ tmux a -t mysession</code></p>"},{"location":"2022_workshops/introHpcSlurm/09_interactive-session/","title":"Interactive Session","text":""},{"location":"2022_workshops/introHpcSlurm/09_interactive-session/#interactive-session","title":"Interactive Session","text":"<ul> <li>Particularly good for debugging and working with software GUI. </li> </ul> <p><code>$ srun [options] --pty [command]</code></p> <ul> <li> <p>Command </p> </li> <li> <p>command to run an application, given the module is already loaded.</p> </li> <li> <p><code>bash</code> for a bash shell</p> </li> <li> <p>Options</p> </li> <li> <p>Pseudo terminal <code>--pty</code></p> </li> <li>Partition <code>-p</code> <ul> <li>Default batch if not specified</li> <li>You can start interactive sessions on any partition you have access to</li> </ul> </li> <li>Time <code>-t</code> or <code>--time=</code><ul> <li>Default 15 minutes if not specified on non-\"interactive\" partition</li> </ul> </li> <li>Number of CPU cores <code>-n</code> <ul> <li>Default 1 if not specified</li> </ul> </li> <li>Memory <code>--mem=</code><ul> <li>Default 2GB if not specified</li> </ul> </li> <li>GPU <code>--gres=</code><ul> <li>Default none</li> </ul> </li> <li>X Window <code>--x11=first</code><ul> <li>Default none  </li> </ul> </li> </ul> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 4GB of RAM, with X11 forwarding for 1 day, 2 hours, and 10 minutes (use <code>exit</code> to end session and release resources).</p> <pre><code>[your_utln@login-prod-01 ~]$ srun -p batch --time=1-2:10:00 -n 2 --mem=4g --reservation=bioworkshop --pty bash\n[your_utln@p1cmp025 ~]$\n</code></pre> <p>You will be placed on one of the reserved nodes for the workshop <code>p1cmp[025-026,028-029,043-045]</code></p> <p>The reservation will expire after the workshop. You will no longer have access to the reservation <code>bioworkshop</code>. </p> <p>In that case, you can simply omit the <code>--reservation=bioworkshop</code> option in the srun command</p> <pre><code>[your_utln@login-prod-01 ~]$ srun -p batch --time=1-2:10:00 -n 2 --mem=4g --pty bash\n\n[your_utln@i2cmp003 ~]$ exit\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/10_job-status/","title":"Job Status","text":""},{"location":"2022_workshops/introHpcSlurm/10_job-status/#checking-job-status","title":"Checking Job Status","text":"<ul> <li>Checking your active jobs</li> </ul> <pre><code>[your_utln@c1cmp044 LS]$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n          24063163     batch      job your_utln  R       0:17      1 c1cmp044 \n\n[your_utln@c1cmp044 LS]$ squeue -u your_utln\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n          24063163     batch      job your_utln  R       0:54      1 c1cmp044 \n</code></pre> <p>To check your active jobs in the queue:</p> <p><code>$ squeue -u $USER</code> or <code>$ squeue -u your_utln</code></p> <p>To cancel a specific job:</p> <p><code>$ scancel JOBID</code></p> <p>To cancel all of your jobs:</p> <p><code>$ scancel -u $USER</code> or <code>$ scancel -u your_utln</code></p> <p>To check details of your active jobs (running \"R\" or pending \"PD\"):</p> <p><code>$ scontrol show jobid -dd JOBID</code></p> <pre><code>[your_utln@c1cmp044 LS]$ scontrol show jobid -dd 24063163\nJobId=24063163 JobName=job\n   UserId=your_utln(31003) GroupId=your_utln(5343) MCS_label=N/A\n   Priority=12833 Nice=0 Account=normal QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:01:31 TimeLimit=1-00:00:00 TimeMin=N/A\n   SubmitTime=2022-07-20T12:33:14 EligibleTime=2022-07-20T12:33:14\n   AccrueTime=2022-07-20T12:33:14\n   StartTime=2022-07-20T12:33:15 EndTime=2022-07-21T12:33:15 Deadline=N/A\n   PreemptEligibleTime=2022-07-20T12:33:15 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-07-20T12:33:15\n   Partition=batch AllocNode:Sid=c1cmp044:27677\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=c1cmp044\n   BatchHost=c1cmp044\n   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=8G,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   JOB_GRES=(null)\n     Nodes=c1cmp044 CPU_IDs=2-3 Mem=8192 GRES=\n   MinCPUsNode=1 MinMemoryNode=8G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Reservation=bioworkshop\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/cluster/home/ymalon01/workshop/LS/sbatch.sh\n   WorkDir=/cluster/home/ymalon01/workshop/LS\n   StdErr=/cluster/home/ymalon01/workshop/LS/24063163.err\n   StdIn=/dev/null\n   StdOut=/cluster/home/ymalon01/workshop/LS/24063163.out\n   Power=\n   MailUser=ymalon01 MailType=NONE\n</code></pre> <ul> <li>Checking your finished jobs</li> </ul> <p>You can no longer see these jobs in <code>squeue</code> command output.</p> <p>Tip</p> <p>Querying finished jobs helps users make better decisions on requesting resources for future jobs.</p> <p>Display job CPU and memory usage:</p> <p><code>$ seff JOBID</code></p> <pre><code>[your_utln@c1cmp044 LS]$ seff 24063163\nJob ID: 24063163\nCluster: pax\nUser/Group: your_utln/your_utln\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:03:00\nCPU Efficiency: 50.00% of 00:06:00 core-walltime\nJob Wall-clock time: 00:03:00\nMemory Utilized: 54.16 MB\nMemory Efficiency: 0.66% of 8.00 GB\n</code></pre> <p>Display job detailed accounting data:</p> <p><code>$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j JOBID</code></p> <pre><code>[your_utln@c1cmp044 LS]$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j 24063163\n Partition      State  Timelimit               Start                 End    Elapsed     MaxRSS     ReqMem  MaxVMSize   NNodes      NCPUS        NodeList \n---------- ---------- ---------- ------------------- ------------------- ---------- ---------- ---------- ---------- -------- ---------- --------------- \n     batch  COMPLETED 1-00:00:00 2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00                   8Gn                   1          2        c1cmp044 \n            COMPLETED            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00     55464K        8Gn    198364K        1          2        c1cmp044 \n           OUT_OF_ME+            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00          0        8Gn    108052K        1          2        c1cmp044 \n</code></pre> <p>Note</p> <p>For more format options, see sacct</p>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/","title":"Intro to Command Line","text":"<p>This short workshop provides some basic training on bash and shell scripting on the command line on the Linux-based Tufts HPC cluster.</p> <p>This course is not meant to be comprehensive, but provides some insights into how the command line works as well as some strategic resources for studying and understanding command line on the HPC cluster.</p> <p>Helpful Vocabulary</p> <ul> <li>Command line is a more general term to indicate that you are using text commands on a terminal (linux bash shell or similar). Command line differs from \"Graphical User Interface (GUI)\" because all commands are texts instead of drag-and-drop or interactive formats such as the Windows or Mac Operating Sytems provide.</li> <li>HPC stands for High Performance Computing, \"cluster\" refers to a shared computer resource to enable more powerful computation than regularly available on an individual machine.</li> <li>Linux can refer to any of the free open source version of \"Unix\" from AT&amp;T Bell labs who pioneered the language in 1965. There are a number of Linux operating systems installed on HPC clusters (Ubuntu, Debian, RedHat Enterprise License (RHEL), CentOs, Fedora, etc.) Each of these systems have slight differences that may impact the commands demoed here. Tufts University Research Cluster is currently using RHEL7.</li> <li>Bash is one type of languages used in a \"shell\", the text interface on the Linux system. This lesson introduces a few objectives to help users understand how to use bash commands on the Linux RHEL shell of our HPC. Other shell languages have slight differences that affect how commands are run (e.g. new MacOSX ship with \"zsh\" as the default shell language on their installed terminal programs).</li> </ul>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>What is the shell?</li> <li>How do you access it?</li> <li> <p>How do you use it and what is it good for?</p> </li> <li> <p>Running commands</p> </li> <li>File Directory Structure</li> <li>Manipulating files</li> <li>Simple Bash Scripts</li> </ul>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#what-is-the-shell","title":"What is the shell?","text":"<p>The shell is a program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination.</p> <p>There are many reasons to learn about the shell.  A few specific ones:</p> <ul> <li> <p>For most bioinformatics tools, you have to use the shell. There is no   graphical interface. If you want to work in metagenomics or genomics you're   going to need to use the shell.</p> </li> <li> <p>The shell gives you power. The command line gives you the power to   do your work more efficiently and more quickly. Shell allows users to automate repetitive tasks.</p> </li> <li> <p>To use remote computers or cloud computing, you need to use the shell.</p> </li> </ul>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#knowing-shell-increases-speed-and-efficiency-through-automation","title":"Knowing Shell Increases Speed and Efficiency Through Automation","text":"<p>The most important reason to learn the shell is to learn about automation.  Any time you find yourself doing roughly the same computational task more than few times, it may be worth automating it; the shell is often the best way to automate anything to do with files.</p> <p>In this lesson, we're going to go through how to access Unix/Linux and some of the basic shell commands. We will finish with a demonstration of how to run programs interactively as well by submitting a job to SLURM. Slurm is a scalable cluster management and job scheduling system for Linux clusters. Other job scheduling systems you may be familiar with from other universities are \"PBS\" and \"SGE_Batch\".</p>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#where-to-learn-shell-commands","title":"Where to learn shell commands","text":"<p>The challenge with bash for the command line is that it's not particularly simple - it's a power tool, with its own deep internal logic with lots of details.</p> <p>Practice is the best way to learn, but here are some helpful shell command resources:</p> <ul> <li>Fun With Unix Cheat Sheet</li> <li>Shell Cheatsheet - Software Carpentry</li> <li>Explain shell - a web site where you can see what the different components of a shell command are doing.</li> </ul>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#how-to-access-the-shell-at-tufts","title":"How to Access the Shell at Tufts","text":"<p>Log In through OnDemand</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#mac","title":"Mac","text":"<p>On Mac the shell is available through Terminal Applications -&gt; Utilities -&gt; Terminal Go ahead and drag the Terminal application to your Dock for easy access. Note: newer versions of MacOSX ship with \"zsh\" as the default shell language in their terminal. It is possible to change the preference to \"bash\". However, if you are only using the terminal to log into the Tufts cluster, you don't necessarily need to do this, because you will be using \"bash\" once you are on the cluster. \"zsh\" would only impact scripts and commands run locally on your own machine.</p>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#windows","title":"Windows","text":"<p>For Windows, an easy one to install and use right away is  gitbash. Download and install gitbash Open up the program.</p>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#other-options","title":"Other options:","text":"<ul> <li>Microsoft Window Terminal</li> <li>Conemu</li> </ul>"},{"location":"2022_workshops/introHpcSlurm/11_intro-to-unix/#linux","title":"Linux","text":"<p>You probably already know how to find the shell prompt.</p>"},{"location":"2022_workshops/introHpcSlurm/12_start-w-shell/","title":"Starting with the Shell","text":"<p>We will spend most of our time learning about the basics of the shell by manipulating some experimental data that we download from the internet.</p> For Attendees Using Terminal Programs to Access the Cluster (instead of the Web Browser \"OnDemand <p>If you are using a terminal on your home machine to connect to the tufts cluster, you will first need to log in by sending a simple command. Ignore this if you are using the web browser login tool.</p> <p>Replace \"username01\" with your tufts username.</p> <pre><code>ssh username01@login.pax.tufts.edu\n</code></pre> <p>Your username will have been created when your account was set up. If you do not have a cluster account, you can still follow this tutorial from your laptop or personal computer, except that the file structure will be different from what is described.</p> <p>The login will ask you for your Tufts password.</p> Connection Issues? <p>If you are not on the Tufts network, you will need to set up the Tufts VPN (Virtual Private Network) before logging in:</p> <p>VPN Instructions</p> <p>Best Practices for Logging In</p> <p>If you are logged in to OnDemand, and on a machine called \"login\". If you are not on the login machine, type <code>exit</code> to get there.</p> <p>First, run the <code>tmux</code> commands (remember which login machine you are on, <code>login-prod-#</code></p> <p><pre><code>module load tmux\ntmux new -s OctoberWorkshop\n</code></pre> You will be able to recover this session if you are on the same login node and run <code>tmux a -t OctoberWorkshop</code></p> <p>Second, run the <code>srun</code> command to go to a working machine, remember we have a reservation for this workshop on October 19 so if you are reading this at a different time, just drop the <code>--reservation=bioworkshop</code> parameter from the command.</p> <pre><code>srun -p batch --time=1-2:10:00 -n 2 --mem=4g --reservation=bioworkshop --pty bash\n</code></pre> <p>The beginning of the line is called the 'command line prompt.'</p> <p><pre><code>[username01@login-prod-02 ~]$\n</code></pre> It tells you who you are and what machine you have been assigned.</p> <p>Once you run the <code>srun</code> command, the machine name should change</p> <pre><code>[username01@i2cmp003 ~]$\n</code></pre> <p>Question</p> <p>What machine are you on? Type your answer into the chat box.</p> <p>Tip</p> <p>The <code>$</code> at the end of the line is where you start typing your commands. The <code>$</code> (on some Mac terminals it is a <code>%</code>) is not part of the command. The outputs from commands will not have that piece of information or <code>$</code> at the beginning of the line.</p> <p>Tip</p> <p>The name of the computer you are on is important informatiom when troubleshooting the cluster.  <code>login</code> machines will reject large commands and output an error.  Make sure to switch machines with <code>srun</code> before running programs. </p>"},{"location":"2022_workshops/introHpcSlurm/12_start-w-shell/#using-basic-commands","title":"Using Basic Commands","text":"<p>Open up the shell through a terminal (OnDemand or on your laptop) and type the command::</p> <pre><code>whoami\n</code></pre> <p>and then hit ENTER </p> <p>(This is a good question for Mondays ....)</p> <p>When you are on the Tufts cluster, this will return your username according to the cluster. This username is attached to you wherever you are in the cluster and creates a home where your files can be kept, regardless of which machine you are on in the cluster. [If you are on your laptop or personal computer, the answer to this may be different before you log in.]</p>"},{"location":"2022_workshops/introHpcSlurm/12_start-w-shell/#running-commands","title":"Running Commands","text":"<p>Let's try some simple commands.</p> <p>Much like text shortcuts, shell commands often use abbreviations to get their point across.</p> <p>For example, the command pwd is short for \"print working directory.\" The word \"print\" here means it will output it into the visible screen.</p> <p>Now type the command</p> <pre><code>pwd\n</code></pre> <p>You should see something similar to this:</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>Try this command</p> <pre><code>ls\n</code></pre> <p>It may be empty for the moment, or it may not if this is not your first time using the shell. We will be creating content in the next part of this lesson.</p> <p>Takeaways</p> <p><code>pwd</code> and <code>ls</code> are examples of commands - programs you run at the shell prompt that do stuff. </p> <ul> <li><code>pwd</code> stands for 'print working directory', while</li> <li><code>ls</code> stands for 'list files'. </li> </ul> <p>It is similar to the abbreviations used in texting, it takes less time to get the point across (lol, tbh, imho, afaik, ftw -- you're saying them outloud in your head, right now, correct?)</p>"},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/","title":"Bash Parameters","text":""},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/#making-files-and-directories","title":"Making files and directories","text":"<p>Many bash commands have special parameters, sometimes referred to as flags that open up a lot more possibilities.</p> <p>Let's start by going to your home directory (you choose the command)</p> <p>For new users, this may not return any content (besides <code>privatemodules</code> if you loaded <code>tmux</code> at the beginning.)</p> <p>Let's make a workshop directory and put a file into it.</p> <p>1.) Go to your home directory </p> <pre><code>cd\n</code></pre> <p><code>cd</code> not only changes directory, it allows you to go home by typing the command all by itself without a directory name.</p> <p>2.) Create a new directory for the workshop</p> <pre><code>mkdir Oct22Workshop\n</code></pre> <p>Note</p> <p><code>mkdir</code> is a specific command that allows you to make a directory. <code>rmdir</code> is a command that allows you to remove a directory (but only if it is empty)  When nameing files and directories, avoid spaces and special characters except underscores (\"_\") and hyphens (\"-\").</p> <p>Important</p> <p>Spelling and Capitalization are literal in unix. Be careful when making and using files to be consistent in your process.  This will make it easier to find files later.</p> <p>3.) Let's go into the directory using a very common command <code>cd</code> --&gt; <code>change directory</code></p> <pre><code>cd Oct22Workshop\n</code></pre> <p>4.) Make a new file that is empty</p> <pre><code>touch emptyfile.txt\n</code></pre> <p><code>touch</code> is a bash command that creates an empty file.</p> Why would you want an empty file? <p>Some programs require some pre-existing file names to be created.</p> <p>5.) Make a new file that contains \"Hello World\"</p> <pre><code>echo \"Hello World\" &gt; helloworld.txt\n</code></pre> <p><code>echo</code> is a command that prints the content to the terminal window (sometimes refered to as <code>print to screen</code></p> <p>The <code>&gt;</code> in this command tells the command to place the output into the place it is pointing. </p> <p>In this case, it creates the file <code>helloworld.txt</code> and puts the phrase <code>Hello World</code> into the file. </p> <p>6.) Print out the contents of the file to the terminal</p> <pre><code>cat helloworld.txt\n</code></pre> <p>You should see the output</p> <pre><code>Hello World\n</code></pre> <p>7.) Return to your home directory and run <code>ls</code></p> <pre><code>cd\n</code></pre> <pre><code>ls\n</code></pre> <p>Tip</p> <p>If you want to speed up the execution of commands, you can copy and paste multiple commands at the same time.</p> <pre><code>cd\nls\n</code></pre> <p>Question</p> <p>Please put a green checkmark in your box if you see the new directory when you type <code>ls</code> from your home directory).</p> <p>Be Careful with Redirect</p> <p>Be careful with redirect.</p> <p>When using <code>&gt;</code> to redirect content into a file, if the filename already exists, it will overwrite the file. This means that the original file is gone, and there is no undo in shell.</p> <p>If you want to add to a file (for example if you are running the same command on several files and extracting a piece of information that you want to put together at the end) you can use another form of redirect <code>&gt;&gt;</code>. Using the double redirect will add to the file instead of overwriting it.</p> <p>Which one is used depends on your process. If you are only running a command once, or have an intermediate file in a process that does not need to be retained at the end, then <code>&gt;</code> is okay to use.</p>"},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/#setting-parameters-for-bash-commands","title":"Setting Parameters for Bash Commands","text":"<p>As you start using bash more and more, you will find a mix of files and directories/folders.  If we want to know which is which, we can add a <code>parameter</code> (sometimes referred to as a <code>flag</code>)</p> <p>This is an example of adding a <code>parameter</code> without an <code>argument</code>.</p> <pre><code>ls -F\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/#adding-arguments-to-bash-commands","title":"Adding Arguments to Bash Commands","text":"<p>An <code>argument</code> is a file name or other data that is provided to a command.</p> <pre><code>ls -F Oct22Workshop\n</code></pre> <p>It is possible to list the files and see their types inside a specific directory by adding the <code>argument</code> of the directory name to the <code>ls</code> command.</p> What do you see when you run the two commands above? <p>Anything with a \"/\" after it is a directory. Anything with a <code>*</code> after it are programs. (we will make a program later) If there's nothing there it's an otherwise unremarkable file (e.g. a data file or picture).</p> <p>Depending on which terminal you are using, some of the file types may have different colors. </p> <p>In our ondemand shell:</p> <p>Files are white Directories are blue Programs (also called <code>executables</code>) are green Compressed files are red (e.g. files that end in .zip or .gzip or .tar)</p>"},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/#other-useful-parameters-for-ls","title":"Other Useful Parameters for <code>ls</code>","text":"<p>Show hidden files</p> <pre><code>ls -a\n</code></pre> <p>You should see a file called <code>.bashrc</code> here. This may be a file we need for troubleshooting your work or where you can make shortcuts or add paths to your login.</p> <p>Show the <code>long form</code> of the list command</p> <pre><code>ls -l\n</code></pre> <p>To see whether items in a directory are files or directories. <code>ls -l</code> gives a lot more information too, such as the size of the file.</p> <p>It also shows the permissions of who can read, write or execute a file.</p> <pre><code>drwxrwx--- 2 username05 username05     4096 Jul 18 09:57 JulyWorkshop\n</code></pre> <p>The first 10 letters in this line indicates the permission settings.</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/13_bash-parameters/#getting-help-on-the-command-line","title":"Getting Help on the Command Line","text":"<p>There are an overwhelming number of possibilities with some of these shell commands, so knowing how to find help on demand is important.</p> <p>For example, <code>ls</code> has a lot of flags that can be used.</p> <pre><code>ls --help\n</code></pre> <p>This outputs a list of all the ways that <code>ls</code> can be altered to find information about your files.</p> <p>Parameters can be added together in some cases.</p> <pre><code>ls -ltr\n</code></pre> <p>This can replace <code>ls -l -t -r</code> <code>l</code> is for long form of the list (outputs the permission settings -- something we need to troubleshoot occasionally) <code>t</code> is to order the files chronologically <code>r</code> means to reverse the order of the files to put the newest file at the bottom</p> <p>This command strings together three flags.</p> <p><code>ls -l</code> is list with details <code>ls -t</code> is sort the list by creation time <code>ls -r</code> is sort the list in reverse</p> <p>For very full directories, this is helpful because it outputs the most recent set of files as the last in the list.</p> <p>Another way to get help is to use the <code>man</code> command. Not every unix installation has this installed, but the Tufts cluster does.</p> <p><code>man</code> is short for \"manual\"</p> <p>Navigating a <code>man</code> page</p> <p>Use the <code>spacebar</code> to scroll through the document. Use <code>q</code> to leave the manual and go back to the command line prompt.</p> <pre><code>man ls\n</code></pre> <p>This opens up the manual on the <code>ls</code> command. It spells out the meaning of all the parameters in detail.</p> <p>Most common bash commands have a <code>man</code> page that explains it (I wish they had this for emojis....).</p> <p>Many programs have a help function built in, try adding <code>--help</code> or <code>-h</code> to see if some helpful information pops up. Sometimes just running the command without any arguments or parameters leads to some usage information or describes the correct command to get help.</p> <p>For example, if I want to understand the command <code>tr</code> - which is used to change a word or character to a new value.</p> <p>Most programs recognize when you ask for an incorrect parameter, and will tell you how to get more information, as in this example.  To get help, type the command with the correct parameter.</p> <p>Tip</p> <p>For some programs, the <code>help</code> function may be <code>-h</code>, <code>--help</code> </p> <pre><code>tr -h\n</code></pre> <p>The shell outputs:</p> <pre><code>tr: invalid option -- 'h'\nTry 'tr --help' for more information\n</code></pre> <pre><code>tr --help\n</code></pre> <p>In this case, a <code>man</code> page does exist, so you can get even more direction by typing:</p> <pre><code>man tr\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/14_shell-navigation/","title":"Shell Navigation","text":""},{"location":"2022_workshops/introHpcSlurm/14_shell-navigation/#navigating-in-the-shell","title":"Navigating in the Shell","text":"<p>Best Practices for Naming Files and Directories</p> <p>A directory is like a desk drawer. We create them to store files that relate to each other mostly.</p> <p>When creating directories and filenames it is helpful to put some information about the project and the date of activity.</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/14_shell-navigation/#absolute-and-relative-paths","title":"Absolute and Relative Paths","text":"<p>Let's go into our directory and look around using relative and absolute paths.</p> <p>Go home</p> <p><pre><code>cd\n</code></pre> Go into our workshop directory</p> <pre><code>cd Oct22Workshop\n</code></pre> <p>and then</p> <pre><code>pwd\n</code></pre> <p>You should now see something like this:</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>This is an example of an Absolute Path.</p> <p>It gives an address for where you are located on the cluster, much like a postal address that defines where you are in several layers (e.g. /country/state/city/street/specific_house.</p> <p></p> <p>You can have many files and folders that share the same name in your directories (e.g. scripts, data). An absolute path ensures that you go to the correct file, as it will be unique.</p> <p>If you want to go back to the directory that is in the level above our current file (in this case \"home\"), another common shortcut used in bash is <code>..</code></p> <pre><code>cd ..\n</code></pre> <p><code>..</code> is a reference to a RELATIVE PATH</p> <pre><code>pwd\n</code></pre> <p>You should be back in your home directory.</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>If you want to go back to the directory that you just left, type this command.</p> <p><pre><code>cd -\n</code></pre> Then find your location.</p> <pre><code>pwd\n</code></pre> <p>You should be back in the directory you came from.</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>A *RELATIVE PATH means that the command only works from the relative location that you are in.</p> <p><code>cd ..</code> and <code>cd -</code> are examples of relative path commands.</p> <p>Note</p> <p>Your home directory is not all the way back at the root ('/'), it is set within the cluster as <code>/cluster/home/username01/</code>.</p> <p>You can make sure that you are in the right directory by using the command <code>cd</code> with the absolute path.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre> <p>This command will make sense inside a script, because the exact path is specified.</p>"},{"location":"2022_workshops/introHpcSlurm/14_shell-navigation/#using-bash-commands-with-absolute-paths","title":"Using Bash Commands with Absolute Paths","text":"<p>Many commands in bash can be used with the ABSOLUTE PATH.</p> <pre><code>ls /cluster/home/username01/Oct22Workshop\n</code></pre> <pre><code>helloworld.txt\nemptyfile.txt\n</code></pre> <p>Absolute Paths are better for SLURM</p> <p>This can get confusing if you are moving around a lot in your directories or sending commands to SLURM, so the alternative method to navigating around the cluster is using an ABSOLUTE PATH.</p>"},{"location":"2022_workshops/introHpcSlurm/15_create-manipulate-files/","title":"Creating & Manipulating Files","text":""},{"location":"2022_workshops/introHpcSlurm/15_create-manipulate-files/#reading-file-contents","title":"Reading File Contents","text":"<p>There are a few different ways to see the contents of a file.</p> <p>We already used this first example.</p> <pre><code>cd ~/Oct22Workshops\n</code></pre> <p>Let's look inside the file. We have several methods of viewing the content of files that we have created.</p> <p>A helpful command is <code>cat</code>.</p> <pre><code>cat helloworld.txt\n</code></pre> <p>\"cat\" will open the entire file, so this is not the best command for long files.</p> <p>In that case \"head\" is a good option. Head pulls the top ten lines of the file and prints them to the screen.</p> <pre><code>head helloworld.txt\n</code></pre> <p>It does not look any different from cat in this case because there is only one line in the file.</p> <p>A third way to check file contents is by using a program called \"less\" (or \"more\").</p> <p>\"less\" will open the file interactively, then you can scroll through it and when you are done, push \"q\" on your keyboard to close the file.</p> <pre><code>less helloworld.txt\n</code></pre> <p>Press q to close the file opened by <code>less</code></p> <p>There are many versions of these tools on command line, but \"cat\", \"head\" and \"less\" are very common.</p>"},{"location":"2022_workshops/introHpcSlurm/15_create-manipulate-files/#copying-files","title":"Copying Files","text":"<p>Sometimes we have a file that we want to reuse.</p> <p>When copying within the same directory, make sure to change the name of the file, or the original will be overwritten.</p> <p>When copying to a new directory, the name can stay the same.</p> <p>This command copies the file within the same directory with a new name. Both files are kept.</p> <p><pre><code>cp helloworld.txt helloworld1.txt\n</code></pre> Check this with <code>ls</code></p> <p>These commands make a new directory, and then copies the file into the new directory with the same name.</p> <pre><code>mkdir helloworld\ncp helloworld.txt helloworld\n</code></pre> <p>Check this with <code>ls helloworld</code> (lists the contents of the directory).</p>"},{"location":"2022_workshops/introHpcSlurm/15_create-manipulate-files/#moving-files","title":"Moving Files","text":"<p><code>mv</code> is an option for renaming files, but also has the potential to overwrite existing files.</p> <p>For example, this command changes the name of the file and removes the original file. If <code>helloworld2.txt</code> already existed, it would be replaced.</p> <pre><code>mv helloworld1.txt helloworld2.txt\n</code></pre> <p>Check this with <code>ls</code></p>"},{"location":"2022_workshops/introHpcSlurm/15_create-manipulate-files/#removing-files","title":"Removing Files","text":"<p><code>rm</code> and <code>rmdir</code> are permanent in shell, so make sure you are ready to delete files.</p> <pre><code>rm helloworld/helloworld.txt\n</code></pre> <p>Once the directory is empty, we can remove the directory.</p> <pre><code>rmdir helloworld\n</code></pre> <p>It will throw an error if the directory is not empty.</p> <p>If you are positive that you want to remove a directory and all the files within it, then add two flags, <code>-r</code> for recursive and <code>-f</code> for force.</p> <p>Both commands above could have been replaced with one remove command: <code>rm -rf helloworld</code></p> <p>Tip</p> <p>Until you are confident with file structure and bash commands, it is a good idea to copy instead of move and to    * <code>cp -u</code> will copy files only if they do not already exist.   * <code>cp -r</code> is a good command for copying directories, it means <code>copy recursively</code> which will copy the entire directory.   * <code>cp -rf</code> BE CAREFUL with this, it copies the entire directory AND forces the overwrite of any files that already exist.   * Adding the interactive flag <code>-i</code> on the commands <code>rm</code> and <code>mv</code> to set up a question that you answer <code>y</code> or <code>n</code> to before removing.</p> <pre><code>rm -i helloworld/helloworld.txt\n</code></pre> <p>Generates this question <pre><code>rm: remove regular file \u2018helloworld/helloworld.txt\u2019?\n</code></pre></p> <p><code>mv -i</code> only generates a question if you are in danger of overwriting an existing file.</p> <p>For example:</p> <p>1.) Make a new file from the original file we created</p> <p><pre><code>cp -u helloworld.txt helloworld1.txt\n</code></pre> <code>-u</code> for the copy command will not copy the file if it already exists.</p> <p>2.) Try to rename the file with <code>mv</code>, with the <code>i</code> parameter set to prevent overwriting an existing file.</p> <p><pre><code>mv -i helloworld.txt helloworld1.txt\n</code></pre> Generates the question:</p> <pre><code>mv: overwrite \u2018helloworld1.txt\u2019?\n</code></pre> <p>A great website to look at to understand the nuances of shell commands is:</p> <p>ComputerHope</p>"},{"location":"2022_workshops/introHpcSlurm/16_going-home/","title":"Going Home","text":""},{"location":"2022_workshops/introHpcSlurm/16_going-home/#going-home","title":"Going Home","text":"<p>Sometimes we get lost, so it is useful to know a few ways to get back to where you started.</p> <pre><code>cd\n</code></pre> <p>This command returns you to your home directory. Check by typing this command.</p> <pre><code>pwd\n</code></pre> <p>Other options for going back to your home directory:</p> <pre><code>cd ~\n</code></pre> <pre><code>cd $HOME\n</code></pre> <p>When lost in the file structure, going home is a good place to start.</p>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/","title":"Running an Interactive Session","text":""},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#running-programs-interactively","title":"Running Programs Interactively","text":""},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#hpc-etiquette","title":"HPC Etiquette","text":"<p>Try not to use the login computers for programs or large file management jobs. Looking things up and small commands such as <code>cat</code> or <code>head</code> are fine, but running programs may block others from logging in to the cluster.</p>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#switch-to-an-interactive-session","title":"Switch to an Interactive Session","text":"<p>Do this first before running programs or testing your code.</p> <pre><code>srun -p batch --reservation=bioworkshop -n 2 --mem=8g -t 1-0 --pty bash\n</code></pre> <p>This command only works during the October-November 2022 workshops. To use this command after the workshop is over or if you are working on your own, just remove the reservation flag.</p> <ul> <li> <p><code>-p</code> which partition to use, outside of class it is okay to use <code>interactive</code> or <code>batch</code>, your group may have it's own partition. You can read more about what is available by going to the OnDemand dropdown menu for \"Misc\" and look at \"Scheduler Info\" to find all the partition names.</p> </li> <li> <p><code>--reservation</code> only applies during a specific class or workshop</p> </li> <li><code>-n</code> is the number of cpus to request, 2 or 4 is sufficient for most tests.</li> <li><code>--mem</code> specifies the memory requested, 8g is usually sufficient for small jobs, consult the documentation for a program to find out if a minimum memory requirement is needed.</li> <li><code>-t</code> indicates the time <code>1-0</code> means one day, so for tomorrow's session you will need to rerun this command.</li> <li><code>--pty bash</code> just indicates that the shell opens in <code>bash</code>, meaning that all the commands that we learned today will work.</li> </ul> <p>Question</p> <p>What compute node are you on? Type it into the chat box.</p> <p></p>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#finding-your-files-interactively","title":"Finding your files interactively","text":"<p>When you request a computer using an <code>srun</code> command, the beginning of your command line should change to indicate that you are no longer on a <code>login</code> node and instead are on a <code>compute</code> node. It will tell you which node you are on.</p> <p>Your files will <code>mount</code> to the new node. This means that you can be on any computer in the Tufts HPC and it will recognize your home directory structure.</p> <p>Let's go back into the Oct22Workshop directory, but this time use your ABSOLUTE path by changing <code>username01</code> to your username. If you forget your username, try <code>whoami</code>.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#find-and-tree","title":"Find and Tree","text":"<p>File structures can get complicated quickly.</p> <p>Two tools to understand where your files are that can help are <code>find</code> and <code>tree</code>.</p> <p>From your home directory, you can find your file named <code>helloworld.txt</code> by typing the following:</p> <pre><code>cd\nfind . -name helloworld.txt\n</code></pre> <p><code>find</code> is a bash command <code>.</code> means look from this location and into any subdirectories to this location <code>name</code> is the file that you are looking for</p> <p>From the home directory, the answer is given using the RELATIVE path:</p> <pre><code>./Oct22Workshop/helloworld.txt\n</code></pre> <p><code>.</code> in this case is another RELATIVE path direction that indicates \"from this directoy that I am in currently\". Note that the answer is given in the RELATIVE path format, starting with <code>.</code> = here.</p> <p>It is also possible to provide an ABSOLUTE path to this command.</p> <p><pre><code>find /cluster/home/username01 -name helloworld.txt\n</code></pre> This command will work from anywhere in the cluster. Note that the answer is given in the ABSOLUTE path format.</p> <pre><code>/cluster/home/username01/Oct22Workshop/helloworld.txt\n</code></pre> <p>Tip</p> <ul> <li><code>find</code> using the parameter <code>iname</code> allows the search to be insensitive to case <code>find . -iname \"helloworld.txt\"</code> finds <code>Helloworld.txt</code> AND <code>helloworld.txt</code></li> <li>wildcards allow for partial searches of many filenames that may match.</li> <li>The easiest wildcard is <code>*</code> which means any number of characters can match, such as <code>find . -iname \"hello*.txt\"</code> finds any file that begins with <code>hello</code> and has any number of characters before <code>.txt</code> finds <code>helloworld1.txt</code> AND <code>helloworld2.txt</code>. </li> <li><code>*</code> can be used anywhere in the pattern: <code>hello*.txt</code>, <code>hello*.txt</code>, <code>*.txt</code></li> </ul> <p>Another helpful bash command for finding files is <code>tree</code>.</p> <pre><code>tree\n</code></pre> <p>This outputs your directory structure with lines that indicate the tree-like branches of your file structure.</p> <p>This could be very messy if you already have a lot of files in a directory, so limit the level by adding a flag.</p> <pre><code>tree -L 2\n</code></pre> <p>This just shows the top two levels of the file structure.</p> <p>Tip</p> <p>There are some keyboard shortcuts that can help when writing complex commands and running programs interactively.</p> <ul> <li>Control-C will terminate a running process</li> <li>Control-A will put your cursor at the beginning of the line</li> <li>Control-E will put your cursor at the end of the line</li> <li>Up and down arrows will scroll through recent commands - If you make a mistake, just hit up to reveal the command and work on the part that was a mistake instead of retyping the whole thing.</li> </ul> <p>Note</p> <p>When trouble shooting a command using tickets, screen shots of error messages are a good option. (On Macs, Command-Shift-4)</p>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#what-is-blast","title":"What is BLAST?","text":"<p>BLAST is the Basic Local Alignment Search Tool. It uses an index to rapdily search large sequence databases; it starts by finding small matches between the two sequences and extending those matches.</p> <p></p> <p>For more information on how BLAST works and the different BLAST functionality, check out the summary on Wikipedia or the NCBI's list of BLAST resources.</p> <p>BLAST can be helpful for identifying the source of a sequence, or finding a similar sequence in another organism. In this lesson, we will use BLAST to find zebrafish proteins that are similar to a small set of mouse proteins.</p>"},{"location":"2022_workshops/introHpcSlurm/17_running-interactive/#why-use-the-command-line","title":"Why use the command line?","text":"<p>BLAST has a very nice graphical interface for searching sequences in NCBI's database. However, running BLAST through the commmand line has many benefits:   * It's much easier to run many BLAST queries using the command line than the GUI   * Running BLAST with the command line is reproducible and can be documented in a script   * The results can be saved in a machine-readable format that can be analyzed later on   * You can create your own databases to search rather than using NCBI's pre-built databases   * It allows the queries to be automated   * It allows you to use a remote computer to run the BLAST queries</p> <p>We are next going to write a script that we will send to SLURM which will demonstrate these advantages.</p>"},{"location":"2022_workshops/introHpcSlurm/18_blast-example/","title":"Example with BLAST","text":""},{"location":"2022_workshops/introHpcSlurm/18_blast-example/#return-to-the-workshop-directory","title":"Return to the workshop directory","text":"<pre><code>cd ~/Oct22Workshop\n</code></pre>"},{"location":"2022_workshops/introHpcSlurm/18_blast-example/#loading-modules","title":"Loading Modules","text":"<p>Many common programs are pre-loaded into the Tufts HPC using a system called \"modules\".</p> <p>To see what versions of blast are available as a module, try running this command. </p> <p>Tip</p> <p>You can use the first part of the program name to check if there is a module.</p> <pre><code>module av blast\n</code></pre> <p>As of October 2022, these are the modules you might see displayed.</p> <p></p> <p>Choose the latest blast-plus version of the module and load it. </p> <pre><code>module load blast-plus/2.11.0\n</code></pre> <p>When there is only one version of a module, the full version does not need to be provided, but it is always best to inclue the version as we are loading and updating versions of programs all of the time.</p> <p>Confirm that the module is loaded.</p> <pre><code>module list\n</code></pre> <p>tmux and blast should be listed.</p> <p>If other programs are loaded with the module, they may also show up with this command.</p>"},{"location":"2022_workshops/introHpcSlurm/18_blast-example/#bringing-in-files-from-the-internet","title":"Bringing in Files from the Internet","text":"<p>We need some data!  Let's grab the mouse and zebrafish RefSeq protein data sets from NCBI, and put them in our home directory. (this example is adapted from a lesson from Titus Brown's summer institute. These lessons contain a lot of command line examples.</p> <p>Note</p> <p><code>curl</code> and <code>wget</code> are the two most common tools used to bring in files that are available from a url. We are going to use <code>curl</code> because that command works well for files coming from an <code>ftp://</code> url.</p> Copying files over from NCBI <p>For genomics projects, the files are often stored in pubic repositories and we must go and get those files before proceeding. These files originally came from the   NCBI FTP site, a copy has been placed in our github directory for future reference.</p> <p>Now, we'll use <code>curl</code> to download the files from a Web site onto our computer. You will need to be connected to the internet for these commands to work.</p> <ul> <li><code>-o</code> indicates this is the name we are assigning to our files in our own directory</li> <li><code>-L</code> provides the full path for the download</li> </ul> <p>It is possible to copy and paste both conmands to your terminal, they will run in sequence if there is not an error.</p> <pre><code>curl -o mouse.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/mouse.1.protein.faa.gz\n\ncurl -o zebrafish.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/zebrafish.1.protein.faa.gz\n</code></pre> <p>Another method for pulling files from the internet is <code>wget</code>, which will be demoed tomorrow. <code>curl</code> can pull more file types than <code>wget</code>, but in this simple case, either can be used.</p> <p>If you look at the files in the current directory:</p> <pre><code>ls -l\n</code></pre> <p>You should now see these 3 files with details on who has permissions and when the files were created (notice that the dates are not today).</p> <pre><code>total 29908\n-rw-rw-r-- 1 username01 username01 12553742 Jun 29 08:41 mouse.1.protein.faa.gz\n-rw-rw-r-- 1 username01 username01 13963093 Jun 29 08:42 zebrafish.1.protein.faa.gz\n</code></pre> <p>The three files you just downloaded are the last three on the list - the <code>.faa.gz</code> files.</p> <p>All three of the files are FASTA protein files (that's what the .faa suggests) that are compressed with <code>gzip</code> (that's what the .gz means). Compressed files may have a different color when you use the <code>ls</code> command.</p> <p>Uncompress the files.</p> <pre><code>gunzip *.faa.gz\n</code></pre> <p>Because both files follow a very similar pattern, and we want to decompress all our .gz files, we can use the <code>*</code> wildcard (filenames that have a pattern that matches and number of missing letters before the part of the file name that is the same</p> <p>Regular Expressions</p> <p><code>*</code> and other wildcards are useful to save on typing scripts, because many actions can be combined in one request.</p> <p>Regular Expressions are a set of special characters combined with unix commands.</p> <p>Here is a link that explains the basic syntax){:target=\"_blank\" rel=\"noopener\"}. </p>"},{"location":"2022_workshops/introHpcSlurm/18_blast-example/#checking-the-contents-of-a-file","title":"Checking the contents of a File","text":"<p>We've already used <code>cat</code> and <code>less</code> to look at the content of our helloworld.txt files. Some files are very large and we may only want to check the first few lines to reassure ourselves that the download worked correctly.</p> <p>Let's look at the first few sequences in the file:</p> <pre><code>head mouse.1.protein.faa \n</code></pre> <p>!!! note \"FASTA format</p> <pre><code>These are protein sequences in FASTA format.  FASTA format is something many of you have probably seen in one form or another -- it's pretty ubiquitous.  It's a text file, containing records; each record starts with a line beginning with a '&gt;', and then contains one or more lines of sequence text.\n</code></pre> <p>Let's take those first two sequences and save them to a file.  We'll do this using output redirection with '&gt;', which says \"take all the output and put it into this file here.\"</p> <pre><code>head -n 11 mouse.1.protein.faa &gt; mm-first.faa\n</code></pre> <p><code>-n</code> flag for <code>head</code> specifies a number of lines to pull.</p> <p>The first 11 lines contain two protein sequences. Let's extract those for blasting to test that our process is working.</p> <pre><code>cat mm-first.faa\n</code></pre> <p>Should produce:</p> <pre><code>&gt;YP_220550.1 NADH dehydrogenase subunit 1 (mitochondrion) [Mus musculus domesticus]\nMFFINILTLLVPILIAMAFLTLVERKILGYMQLRKGPNIVGPYGILQPFADAMKLFMKEPMRPLTTSMSLFIIAPTLSLT\nLALSLWVPLPMPHPLINLNLGILFILATSSLSVYSILWSGWASNSKYSLFGALRAVAQTISYEVTMAIILLSVLLMNGSY\nSLQTLITTQEHMWLLLPAWPMAMMWFISTLAETNRAPFDLTEGESELVSGFNVEYAAGPFALFFMAEYTNIILMNALTTI\nIFLGPLYYINLPELYSTNFMMEALLLSSTFLWIRASYPRFRYDQLMHLLWKNFLPLTLALCMWHISLPIFTAGVPPYM\n&gt;YP_220551.1 NADH dehydrogenase subunit 2 (mitochondrion) [Mus musculus domesticus]\nMNPITLAIIYFTIFLGPVITMSSTNLMLMWVGLEFSLLAIIPMLINKKNPRSTEAATKYFVTQATASMIILLAIVLNYKQ\nLGTWMFQQQTNGLILNMTLMALSMKLGLAPFHFWLPEVTQGIPLHMGLILLTWQKIAPLSILIQIYPLLNSTIILMLAIT\nSIFMGAWGGLNQTQMRKIMAYSSIAHMGWMLAILPYNPSLTLLNLMIYIILTAPMFMALMLNNSMTINSISLLWNKTPAM\nLTMISLMLLSLGGLPPLTGFLPKWIIITELMKNNCLIMATLMAMMALLNLFFYTRLIYSTSLTMFPTNNNSKMMTHQTKT\nKPNLMFSTLAIMSTMTLPLAPQLIT\n</code></pre> <p>Now let's BLAST these two sequences against the entire zebrafish protein data set. First, we need to tell BLAST that the zebrafish sequences are (a) a database, and (b) a protein database.  That's done by calling <code>makeblastdb</code></p> <pre><code>makeblastdb -in zebrafish.1.protein.faa -dbtype prot\n</code></pre> <p><code>makeblastdb</code> is a program that was loaded using the <code>module</code> command. If you unload the module, this command may not work.</p> <p>Next, we call BLAST to do the search:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa\n</code></pre> <p>This should run pretty quickly, but you're going to get a lot of output!! To save it to a file instead of watching it go past on the screen, ask BLAST to save the output to a file that we'll name <code>mm-first.x.zebrafish.txt</code>:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa -out mm-first.x.zebrafish.txt\n</code></pre> <p>and then you can 'page' through this file at your leisure by typing:</p> <pre><code>less mm-first.x.zebrafish.txt\n</code></pre> <p>(Type spacebar to move down, and 'q' to get out of paging mode.)</p> <p>What are your questions?</p> <p>Note</p> <p>This command was an example of <code>interactive</code> shell scripting because we are typing in the commands manually and waiting for the results. If we walk away from our machine and the session times out, then the program may be interrupted. <code>tmux</code> allows us to keep running the program even if we take a break.</p> <p>The next session demonstrates how to combine all of these commands into a script that runs on SLURM. </p> <p>SLURM differs from <code>interactive</code> computing because you activate the script instead of manually writing the commands one at a time.</p>"},{"location":"2022_workshops/introHpcSlurm/19_blast-batch/","title":"BLAST Batch Script","text":""},{"location":"2022_workshops/introHpcSlurm/19_blast-batch/#writing-a-bash-script-and-running-it-as-batch","title":"Writing a BASH Script and Running it as \"Batch\"","text":"<p>In this example, we'll repeat the blast command above but refine it by outputting a table which summarizes each blast hit on one line. </p>"},{"location":"2022_workshops/introHpcSlurm/19_blast-batch/#return-to-the-workshop-directory","title":"Return to the Workshop Directory","text":"<p>cd ~/Oct22Workshop</p> <p>First, let's add more sequences to our query file. This will extract the first 186 sequences.  </p> <pre><code>head -n 999 mouse.1.protein.faa &gt; mm-second.faa\n</code></pre> <p>See this link for a description of the possible BLAST output formats.</p> <p>In order to do this, we need to open a text editor.</p>"},{"location":"2022_workshops/introHpcSlurm/19_blast-batch/#opening-a-text-editor","title":"Opening a Text Editor","text":"<p>The easiest text editor to use on command line for beginners is <code>nano</code>, but there are many other types of command line text editors (<code>vi</code>,<code>emacs</code>,<code>vim</code>, etc.)</p> <p>Nano is nice because it puts the instructions at the bottom of the editor in case you forget.</p> <p>Open nano</p> <pre><code>nano\n</code></pre> <p>Control-X to exit, say no and no. Nothing is saved, because we did not type into the file.</p> <p>Let's reopen and copy and paste our script into the file.</p> <p>Sometimes it is good to give a file name, so let's nano with a filename for our script.</p> <pre><code>nano blast_sbatch.sh\n</code></pre> <p>Before closing, let's put some text into the file. </p> <p>Make sure to change the email address to your own email.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=blast\n#SBATCH --nodes=1\n#SBATCH -n 2\n#SBATCH --partition=batch\n#SBATCH --reservation=bioworkshop\n#SBATCH --mem=8Gb\n#SBATCH --time=0-24:00:00\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-user=youremail@tufts.edu\n\nmodule load blast-plus/2.11.0\nblastp -query mm-second.faa -db zebrafish.1.protein.faa -out mm-second.x.zebrafish.tsv -outfmt 6\n</code></pre> <p>Control -X to close and save and use the same file name (blast_sbatch.sh)</p> <p>Because it is going to one or several virtual locations in the cluster, we need to reload the module as part of the script before running the script. This will make the command recognizable to the machine where the job is running.</p> <pre><code>cat blast_sbatch.sh\n</code></pre> <p>Does it have all the elements?</p> <p>If it does, a simple way to run it is by telling shell that it is a program to run on SLURM.</p> <pre><code>sbatch blast_sbatch.sh\n</code></pre> <p>Check that the job is running</p> <pre><code>squeue -u $USER\n</code></pre> <p>Let's go ahead and run it from the workshop directory where you copied your data to.</p> <p>Because we did not add any ABSOLUTE paths, then the sbatch command will look for the files where the program is running.</p> <p>The results will also show up in that directory.</p> <p>You can look at the output file with <code>less -S</code>, the flag allows scrolling from left to right instead of wrapping text or cutting it off:</p> <pre><code>less -S mm-second.x.zebrafish.tsv\n</code></pre> <p>(and again, type 'q' to get out of paging mode)</p> <p>The command line may move stuff around slightly, but it is a tab delimited file that can be downloaded to your computer and loaded into your spreadsheet program of choice.</p> <p><code>blastp</code> is a versatile tool for finding similar sequences, to see all the options, type <code>blastp -help</code></p> <p>Tip</p> <p>If writing the script on your laptop before copying and pasting, make sure to use a compatible text editor.</p> <p>Even though you can't see it, popular word processors will add hidden symbols and change punctuation to your code.</p> <p>There are several free tools available to avoid these errors.</p> <ul> <li>Notepad+ is free to download and use.</li> <li>BBEdit has a free version.</li> </ul> <p>Other options are Sublime and PyCharm, which have some features to help edit files.</p>"},{"location":"2022_workshops/introHpcSlurm/19_blast-batch/#resources-for-further-training-in-command-line","title":"Resources for Further Training in Command Line","text":"<ul> <li>Udemy (free to the Tufts community)</li> <li>Coursera</li> <li>LinkedIn Learning</li> </ul> <p>What are your favorites?</p>"},{"location":"2022_workshops/introMetagenomics/01_intro/","title":"Introduction","text":""},{"location":"2022_workshops/introMetagenomics/01_intro/#intro-to-whole-shotgun-sequencing-metagenomics","title":"Intro To Whole Shotgun Sequencing Metagenomics","text":"<p>November 9,2022</p>"},{"location":"2022_workshops/introMetagenomics/01_intro/#bioinformatics-instructors","title":"Bioinformatics Instructors","text":"<ul> <li>Adelaide Rhodes, Ph.D, Senior Bioinformatics Scientist </li> <li>Jason Laird, M.S., Bioinformatics Scientist</li> </ul>"},{"location":"2022_workshops/introMetagenomics/01_intro/#special-guest-speaker","title":"Special Guest Speaker","text":"<ul> <li>Neveda Naz, Ph.D., Department of Chemistry</li> </ul>"},{"location":"2022_workshops/introMetagenomics/01_intro/#hpc-support","title":"HPC Support","text":"<ul> <li>Delilah Maloney, Senior High Performance Computing Specialist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"2022_workshops/introMetagenomics/01_intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"2022_workshops/introMetagenomics/01_intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Delilah Maloney, Kyle Monahan, Klara Chura, Christina Divoll, Kayla Sansevere and Uku Uustalu for their review of this content</p>"},{"location":"2022_workshops/introMetagenomics/02_setup/","title":"Setup","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"2022_workshops/introMetagenomics/02_setup/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>4</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>16GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshop</code> ---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p>"},{"location":"2022_workshops/introMetagenomics/02_setup/#copy-over-the-workshop-folder-into-your-home-directory","title":"Copy Over the Workshop Folder Into Your Home Directory","text":"<p>Copy this code chunk by clicking on the \"copy\" icon that is in the upper right corner. Paste the code chunk into the console window in R studio (lower left window)</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/metagenomics/Metagenomics2022\",to=\"~/\", recursive = TRUE)\n</code></pre>"},{"location":"2022_workshops/introMetagenomics/02_setup/#project-setup","title":"Project Setup","text":"<p>We are going to create a new project in the existing directory that we just copied over:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li>Click \"Don't Save\" if it asks about saving the workspace.</li> <li><code>Existing Directory</code></li> <li>Choose the directory we just copied over (e.g. <code>~/Metagenomics2022</code>)</li> </ol> <p>What if my screen goes blank?</p> <p>Don't worry if the screen goes blank for a moment. It just means that a shiny new workspace is being created so you can start on your project.</p>"},{"location":"2022_workshops/introMetagenomics/02_setup/#open-the-r-notebook","title":"Open the R Notebook","text":"<p>Now that we have our data and scripts copied, let's navigate to our scripts folder and open up <code>Metagenomics.Rmd</code>.</p>"},{"location":"2022_workshops/introMetagenomics/02_setup/#what-to-expect-from-today","title":"What to Expect from Today","text":"<p>Here is a rendered version of the notebook that displays only the text and the code and not the output.</p> <p>Today's Notebook</p>"},{"location":"2022_workshops/introRStudio/01_r-intro/","title":"Introduction","text":""},{"location":"2022_workshops/introRStudio/01_r-intro/#intro-to-rstudio-for-life-sciences","title":"Intro to RStudio For Life Sciences","text":"<p>October 26, 2022</p>"},{"location":"2022_workshops/introRStudio/01_r-intro/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Jason Laird, Bioinformatics Scientist</li> <li>Christina Divoll, Research Instrumentation and HPC Specialist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"2022_workshops/introRStudio/01_r-intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"2022_workshops/introRStudio/01_r-intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Kyle Monahan, Christina Divoll, Adelaide Rhodes, Kayla Sansevere, and Uku Uustalu for their review of this content</p>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/","title":"Introduction To RStudio For Life Sciences","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN or the Tufts Secure Network</li> </ul>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#learning-objectives","title":"Learning objectives","text":"<p>Today we are going to learn about</p> <ul> <li>project organization</li> <li>R packages and how to access them on the tufts HPC</li> <li>working with variables and data frames</li> <li>visualizing data</li> <li>and finally writing a markdown report of our findings</li> </ul>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>4GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshops</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Launch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p> Are you connected to RStudio? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#introduction-to-rstudio","title":"Introduction To RStudio","text":"<p>RStudio is what is known as an Integrated Development Environment or IDE. Here you can write scripts, run R code, use R packages, view plots, and manage projects. This pane is broken up into three panels:</p> <ul> <li>The Interactive R console/Terminal (left)</li> <li>Environment/History/Connections (upper right)</li> <li>Files/Plots/Packages/Help/Viewer (lower right)</li> </ul> <p></p>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#project-management","title":"Project Management","text":"<p>Before we dive into R it is worth taking a moment to talk about project management. Often times data analysis is incremental and files build up over time resulting in messy directories:</p> <p></p> <p>Sifting through a non-organized file system can make it difficult to find files, share data/scripts, and identify different versions of scripts. To remedy this, It is reccomended to work within an R Project. Before we make this project, we should make sure you are in your home directory. To do this click on the three dots in the files tab:</p> <p></p> <p>Then enter in a ~ symbol to go home!</p> <p></p>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#r-project","title":"R Project","text":"<p>To Create a new R project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>R-Practice</code>)</li> <li><code>Create Project</code></li> </ol> <p>You will notice that your RStudio console switches to this project directory. When you log out of RStudio you can open this project again by clicking the <code>.Rproj</code> file in the project directory. </p> <p>Note</p> <p>The paths will be relative to this project directory as a safe guard against referencing data from outside sources. </p> Have you created the project? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#file-organization","title":"File Organization","text":"<ul> <li>You noticed now that you are inside your project folder</li> <li>Let's start by creating some folders to you organize our files</li> <li>In the files window click new folder and enter scripts</li> <li>Let's do this again to create a data folder and a results folder</li> </ul>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#data-principles","title":"Data Principles","text":"<ul> <li>Treat data as read-only</li> <li>Store raw data separately from cleaned data if you do need to manipulate it</li> <li>Ensure scripts to clean data are kept in a separate <code>scripts</code> folder</li> <li>Treat reproducible results as disposable</li> </ul> <p>Tip</p> <p>Result files are good candidate files to cut if you are getting low on storage.</p>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#getting-data","title":"Getting Data","text":"<ul> <li>Today we will be using a fake dataset assessing the taxa count on the mouse microbiome before and after antibiotic usage.</li> <li>To copy over this data we will use an R function called file.copy. </li> <li>A function takes some input and delivers an output. </li> <li>In this case we specify two inputs the location of our file and where we want to copy it to. </li> <li>The function's output is copying over this file. So let's try it copy over using the following commands:</li> </ul> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta.tsv\", to=\"./data/\")\nfile.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta2.tsv\", to=\"./data/\")\n</code></pre> <p>So here you'll note we copied over the file metadata.tsv to the data folder. Let's copy over our script:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/scripts/intro-to-r.Rmd\", to=\"./scripts\")\n</code></pre> <p>Here we copy over our script intro-to-r to the scripts folder.</p>"},{"location":"2022_workshops/introRStudio/02_r-ondemand/#opening-the-script","title":"Opening the Script","text":"<p>Now let's start by opening our script. Go to scripts and then double click on intro-to-r.Rmd!</p>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/","title":"Intro To R Script","text":""},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#markdown-language","title":"Markdown Language","text":"<ul> <li>Way of writing HTML content without having to deal with HTML code</li> <li>At the top of the page you'll notice a header section</li> <li>this header section is defined by two sets of three dashes </li> <li>contains <ul> <li>the title of our markdown report</li> <li>the output format of our markdown report</li> </ul> </li> <li>In the body of the document headers can be specified by adding hastags before the text</li> <li>Lists can be specified by adding a dash or asterisk before the text</li> <li>for more information on markdown formatting visit:   https://www.markdownguide.org/basic-syntax/</li> <li>While we will be working with an R markdown document today you can also run code in an R script. To open an R script you can go to File &gt; New File &gt; R Script. </li> </ul> <p>NOTE: R scripts will end in \".R\", while R markdowns will end in \".Rmd\"</p>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#code-chunks","title":"Code chunks","text":"<ul> <li>code chunks can be included in our markdown document with two sets of three tick marks</li> <li>you'll notice in the brackets we add in, <code>r</code>, which indicates we are running R code</li> </ul> <p>Let's start with R by defining what is called a variable. We can run this chunk of code by clicking the play button in the corner of the code chunk:</p> <pre><code>num &lt;- 18\nnum\n</code></pre> <p>output</p> <pre><code>[1] 18\n</code></pre> <p>What did we do:</p> <ul> <li>assigned the value 18 to the word \"num\" </li> <li>assign value with  the \"&lt;-\" operator</li> <li>call the value of this variable with the word \"num\" </li> <li>NOTE: our variable appears in the environment window to the right.</li> <li>NOTE: when we ran the code chunk our console window shrank! This is because our output is appearing below the code chunk. We can always reopen it by clicking on it!</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#variable-names","title":"Variable Names","text":"<ul> <li>variable names are case sensitive</li> <li>they can include any combination of:</li> <li>lower-case letters</li> <li>upper-case letters</li> <li>underscores/periods/numbers (however, these cannot be the first character)</li> </ul> <pre><code>second.number.2 &lt;- 2\nthird_number_3 &lt;- 3\nfourthNumber4 &lt;- 4\n</code></pre> <p>Whate did we do:</p> <ul> <li>we assigned three numbers 2,3,4 to the variables second.number.2, third_number_3,fourthNumber4</li> <li>all are valid variable names</li> <li>keep you variable names as short as possible to still convey what they represent</li> <li>just be consistent with your naming convention</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#variable-properties","title":"Variable Properties","text":"<ul> <li>When we define variables we can treat that variable name as the value itself</li> <li>We can also add variable names together</li> <li>We can assign more than one value to a variable name</li> </ul> <p>Let's try this out in code!</p> <pre><code># add 5 to num\nnum &lt;- num + 5\nnum\n\n# assign 20 to new num and add it to num\nnew_num &lt;- 20\nnew_num + num\n\n#create a variable with multiple values\ncombined &lt;- c(3,4,6)\ncombined\n</code></pre> <p>output</p> <pre><code>[1] 23\n\n[1] 43\n\n[1] 3 4 6\n</code></pre> <p>What did we do:</p> <ul> <li>First we added 5 to the variable <code>num</code></li> <li>We assigned that num + 5 back to the variable <code>num</code> which overwrote the original value of 18! (now it's 23)</li> <li>we assigned a new variable <code>new_num</code> to the value 20 and then showed we can add the values of <code>new_num</code> and <code>num</code> together with just their names</li> <li>we then assigned multiple values to the variable <code>combined</code> by separating values by commas and enclosing them in <code>c()</code></li> <li> <p>this variable with multiple values is called a vector!</p> </li> <li> <p>You'll also note we add text inside our code block by putting a hashtag in front of it. This is called a comment and they are very useful in giving your code context. </p> </li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#accessingmanipulating-values-in-a-vector","title":"Accessing/Manipulating Values in a Vector","text":"<ul> <li>Suppose we want to access one value in our vector <code>combined</code></li> <li>We can do this by specifying the value number in that vector.</li> <li>Let's try grabbing the second value in <code>combined</code></li> </ul> <pre><code># call the second value in combined\ncombined[2]\n\n# replace second value in combined\ncombined[2] &lt;- 10\ncombined[2]\n</code></pre> <p>output</p> <pre><code>[1] 4\n\n[1] 10\n</code></pre> <p>What did we do:</p> <ul> <li>grabbed the second value in <code>combined</code> by specifying the vector and then the number value we want in brackets</li> <li>vectors in R are one-indexed meaning that when you want the first value in a vector you use <code>[1]</code>, second value you would use <code>[2]</code> and so on</li> <li>we also replaced the second value of <code>combined</code> by calling <code>combined[2]</code> and reassigning it to <code>10</code></li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#libraries","title":"Libraries","text":"<ul> <li>R has a collection of base functions (we just used the file.copy() function!)</li> <li>However, there are thousands of other functions we can use by importing different libraries</li> <li>Tufts HPC has a collection of different libraries pre-installed we can use!</li> </ul> <p>Let's access that collection and import a library:</p> <pre><code>.libPaths(\"/cluster/tufts/hpc/tools/R/4.0.0/\")\nlibrary(tidyverse)\n</code></pre> <p>output</p> <pre><code>Registered S3 methods overwritten by 'dbplyr':\nmethod         from\nprint.tbl_lazy     \nprint.tbl_sql      \n\u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3.0 \u2500\u2500\n\u2713 ggplot2 3.3.5     \u2713 purrr   0.3.4\n\u2713 tibble  3.1.6     \u2713 dplyr   1.0.8\n\u2713 tidyr   1.2.0     \u2713 stringr 1.4.0\n\u2713 readr   1.4.0     \u2713 forcats 0.5.1\n\u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n</code></pre> <p>What did we do:</p> <ul> <li>we used the .libPaths() function to point to where Tufts is keeping this collection of R packages</li> <li>after pointing to this location we can import a package!</li> <li>here we imported the tidyverse package using the library() function</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#importing-data","title":"Importing Data","text":"<p>So what does this package do?  - the tidyverse package contains all sorts of functions to load, manipulate, and visualize data!</p> <p>Let's try use the read_delim() function to import some data:</p> <pre><code># load our data\nmeta &lt;- read_delim(file=\"../data/meta.tsv\",\n                   delim = \"\\t\")\n</code></pre> <p>What did we do:</p> <ul> <li>specified where our data  is</li> <li>it is one folder up (a.k.a. \"../\") and in the data folder (\"data/\")</li> <li>we specified the delimiter or the separator between our data</li> <li>here we say \"\\t\" to indicate our file is separated by tabs</li> <li>assign our data to the variable \"meta\"</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#inspecting-data","title":"Inspecting Data","text":"<ul> <li>It is good practice to inspect your data before using it</li> <li>we can use the str() function to get a high level summary of our data :</li> </ul> <pre><code>str(object=meta)\n</code></pre> <p>output</p> <pre><code>spec_tbl_df [9 \u00d7 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SampleID       : chr [1:9] \"sample 1\" \"sample 2\" \"sample 3\" \"sample 4\" ...\n $ AntibioticUsage: chr [1:9] \"None\" \"None\" \"None\" \"None\" ...     \n $ Day            : chr [1:9] \"Day0\" \"Day0\" \"Day0\" \"Day0\" ...     \n $ Organism       : chr [1:9] \"mouse\" \"mouse\" \"mouse\" \"mouse\" ...    \n $ TaxaCount      : num [1:9] 1174 1474 1492 1451 314 ...    \n - attr(*, \"spec\")=     \n  .. cols(     \n  ..   SampleID = col_character(),  \n  ..   AntibioticUsage = col_character(),\n  ..   Day = col_character(),\n  ..   Organism = col_character(),\n  ..   TaxaCount = col_double()\n  .. )\n</code></pre> <p>What did we do:</p> <ul> <li>we input our variable \"meta\" into the str() function which takes some <code>object</code>, here specify that <code>object</code> is our variable <code>meta</code></li> <li>Our output indicates a few things:</li> <li>the dimensions of our data (9 rows by 5 columns)</li> <li>our data is a table/data.frame</li> <li>the names of our columns (SampleID, AntibioticUsage, etc.)</li> <li>the data type of our columns (chr = character data, num = numeric data)</li> <li>how many values per column</li> <li>a preview of the first few values</li> </ul> <p>NOTE: If you want more information on R data types and how to convert between data types, visit: https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/</p> <p>To view the entire data frame, click on the variable in the environment window:   - here we can see the entire data frame and even search for values   - You'll note that our rows are different samples   - and our columns are different attributes about those samples</p>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#accessing-values-by-number","title":"Accessing Values By Number","text":"<ul> <li>We can access values in our data frame by specifying their row and column</li> <li>Let's try finding the value in the second row and the third column:</li> </ul> <pre><code>meta[[2,3]] # [[row,column]]\nmeta[[3]][2] # [[column]][row]\n</code></pre> <p>output</p> <pre><code>[1] \"Day0\"\n\n[1] \"Day0\"\n</code></pre> <p>What did we do:</p> <ul> <li>accessed our value using double brackets</li> <li>single brackets would subset our data frame instead of accessing our value</li> <li>we can either specify the row then column  inside the double brackets</li> <li>or specify our column in double brackets and then the second element in single brackets</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#accessing-values-by-name","title":"Accessing Values By Name","text":"<ul> <li>But what if we don't have our index number? What if we wanted to determine the antibiotic usage of \"sample 5\"?</li> <li>Let's see how we can do this:</li> </ul> <pre><code>#data[[column name]]\nmeta[[\"AntibioticUsage\"]]\n</code></pre> <p>output</p> <pre><code>[1] \"None\"         \"None\"         \"None\"         \"None\"         \"Streptomycin\" \"Streptomycin\" \"Streptomycin\"\n\n[8] \"Streptomycin\" \"Streptomycin\"\n</code></pre> <pre><code># data[[column name]][data[[column name]]==pattern]\nmeta[[\"AntibioticUsage\"]][meta[[\"SampleID\"]]==\"sample 5\"] \n\n#data$ColumnName[data$ColumnName == pattern]\nmeta$AntibioticUsage[meta$SampleID==\"sample 5\"] \n</code></pre> <p>output</p> <pre><code>[1] \"Streptomycin\"\n\n[1] \"Streptomycin\"\n</code></pre> <p>What did we do:</p> <ul> <li>first we accessed our AntibioticUse column by calling our data frame, then in double brackets we reference our column name.</li> <li>we accessed our value by:</li> <li>specifying the column in double brackets</li> <li>we then use single brackets to select some value in that column</li> <li>we then specify a condition:<ul> <li>where the column \"SampleID\" is equal to \"sample 5\"</li> </ul> </li> <li>second we accessed our value by using the \"$\" operator</li> <li>when we are dealing with a data frame we can use the \"$\" operator to avoid having to write double brackets!</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#comparison-operators","title":"Comparison Operators","text":"<ul> <li>You'll have noticed above we used a comparison operator</li> <li>We asked which value in the \"SampleID\" column was equal to \"sample 5\"</li> <li>Let's look at some other comparison operators:</li> <li><code>==</code> equals</li> <li><code>!=</code> does not equal</li> <li><code>&lt;</code> less than</li> <li><code>&gt;</code> greater than</li> <li><code>=&lt;</code> less than or equal to</li> <li><code>&gt;=</code> greater than or equal to</li> <li><code>%in%</code> is a value in another set of values</li> <li><code>&amp;</code> and</li> <li><code>|</code> or</li> <li>Let's try to use these operators to ask a few questions about our data:</li> <li>Do we have any samples with over 1000 different taxa?</li> <li>Is \"sample 8\" in our SampleID column?</li> </ul> <pre><code># first let's see if there are any samples with over 1000 different taxa\n# df$column_name1[df$column_name2&gt;threshold]\nmeta$SampleID[meta$TaxaCount&gt;1000]\n\n# now let's see if there is a \"sample 8\" in our SampleID column\n# pattern %in% df$column_name\n\"sample 8\" %in% meta$SampleID\n</code></pre> <p>output</p> <pre><code>[1] \"sample 1\" \"sample 2\" \"sample 3\" \"sample 4\"\n\n[1] TRUE\n</code></pre> <p>What did we do:</p> <ul> <li>To identify samples with over 1000 different taxa we:</li> <li>specified our SampleID column</li> <li>specified our condition column (here it is TaxaCount)</li> <li> <p>used the greater than operator and threshold to specify we only want to identify samples with a TaxaCount greater than 1000</p> </li> <li> <p>To identify if \"sample 8\" was in our Sample ID column we:</p> </li> <li>specifed our pattern (here it is \"sample 8\")</li> <li>specified our column of interest (SampleID)</li> <li>used the %in% operator to see if our pattern was in our column of interest</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#applying-subsetting-to-data-frames","title":"Applying Subsetting To Data Frames","text":"<ul> <li>So far we have accessed individual values in a data frame. But what about filtering our data frame?</li> <li>Let's filter or subset our data frame into two data frames:</li> <li>one with just samples and their antibiotic usage</li> <li>another with samples on Day 5 of treatment</li> </ul> <pre><code># filter data frame for just samples and their antibiotic usage\n# df[c(\"column_name1\",\"column_name2\")]\nsamples_antibiotics &lt;- meta[,c(\"SampleID\",\"AntibioticUsage\")]\nhead(samples_antibiotics)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage\n  &lt;chr&gt;    &lt;chr&gt;          \n1 sample 1 None           \n2 sample 2 None           \n3 sample 3 None           \n4 sample 4 None           \n5 sample 5 Streptomycin   \n6 sample 6 Streptomycin \n</code></pre> <pre><code># filter data frame for just samples on Day 5 of treatment\n#df[df$column_name == pattern,]\nday_5 &lt;- meta[meta$Day == \"Day5\",]\nhead(day_5)\n</code></pre> <p>output</p> <pre><code>SampleID AntibioticUsage Day   Organism TaxaCount\n&lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 sample 5 Streptomycin    Day5  mouse          314\n2 sample 6 Streptomycin    Day5  mouse          189\n3 sample 7 Streptomycin    Day5  mouse          279\n4 sample 8 Streptomycin    Day5  mouse          175\n5 sample 9 Streptomycin    Day5  mouse          452\n</code></pre> <p>What did we do:</p> <ul> <li>To filter the data frame for just samples and their antibiotic usage:</li> <li>specified our data frame (meta)</li> <li>identified which columns we wanted to keep within <code>c()</code></li> <li>specified we are grabbing columns by placing our column names behind the comma</li> <li>saved this filtered data frame to <code>samples_antibiotics</code></li> <li>used the <code>head()</code> function to view the first 6 rows of our new data frame</li> <li>To filter the data frame to just samples on Day 5 of treatment:</li> <li>specified our data frame (<code>meta</code>)</li> <li>specified the column we intend to filter (<code>Day</code>)</li> <li>used <code>==</code> to filter for only values that are equal to \"Day5\"</li> <li>specified we are filtering rows by placing the comma after our pattern</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#merging-data-frames","title":"Merging Data Frames","text":"<ul> <li>Often times you may want to merge in data from another data frame</li> <li>Let's see how to do this!</li> </ul> <pre><code># read in second meta data file\nmeta2 &lt;- read_delim(\"../data/meta2.tsv\",delim = \"\\t\")\nhead(meta2)\n</code></pre> <p>output</p> <pre><code>  SampleID   RBC\n  &lt;chr&gt;    &lt;dbl&gt;\n1 sample 1    12\n2 sample 2    17\n3 sample 3    14\n4 sample 4    16\n5 sample 5     3\n6 sample 6     7\n</code></pre> <pre><code># merge with existing meta data file\nmerged &lt;- inner_join(\n  x = meta,\n  y = meta2,\n  by = c(\"SampleID\")\n)\nhead(merged)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage Day   Organism TaxaCount   RBC  \n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   \n1 sample 1 None            Day0  mouse         1174    12\n2 sample 2 None            Day0  mouse         1474    17 \n3 sample 3 None            Day0  mouse         1492    14 \n4 sample 4 None            Day0  mouse         1451    16\n5 sample 5 Streptomycin    Day5  mouse          314     3\n6 sample 6 Streptomycin    Day5  mouse          189     7 \n</code></pre> <p>What did we do:</p> <ul> <li>we read in another data frame from our <code>data</code> folder and named this data frame <code>meta2</code></li> <li>we then previewed this data frame to see that we have our SampleID column and a new column <code>RBC</code></li> <li>we then use the inner_join function to merge the two data frames, which takes:</li> <li><code>x</code>  data frame 1</li> <li><code>y</code> data frame 2,</li> <li><code>by</code> the column to merge on in both data frames</li> <li>we then use the <code>head()</code> command to preview our merged data frame</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#adding-columns","title":"Adding Columns","text":"<ul> <li>Sometimes you may want to create columns in your data frame based on data in your existing data frame:</li> </ul> <pre><code># add column based on data on data\nmerged$RBC_Status &lt;- ifelse(\n  test = merged$RBC &gt; 13,\n  yes = \"High RBC Count\",\n  no = \"Low RBC Count\"\n)\nhead(merged)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage Day   Organism TaxaCount   RBC RBC_Status    \n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 sample 1 None            Day0  mouse         1174    12 Low RBC Count \n2 sample 2 None            Day0  mouse         1474    17 High RBC Count\n3 sample 3 None            Day0  mouse         1492    14 High RBC Count\n4 sample 4 None            Day0  mouse         1451    16 High RBC Count\n5 sample 5 Streptomycin    Day5  mouse          314     3 Low RBC Count \n6 sample 6 Streptomycin    Day5  mouse          189     7 Low RBC Count   \n</code></pre> <p>What did we do:</p> <ul> <li>we added a new column by specifying the name of our data frame, <code>merged</code> and then the new column name after the <code>$</code> symbol</li> <li>used the <code>ifelse()</code> function to add different values based on some <code>test</code></li> <li>here our test was to see if the value in the <code>RBC</code> column was over 13</li> <li>if the answer was <code>yes</code>, it was over 13, then we input the value \"High RBC Count\"</li> <li>if the answer was <code>no</code>, it was under 13, then we input the value \"Low RBC Count\"</li> <li>we again use the <code>head()</code> function to preview our updated data frame</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#creating-a-factor","title":"Creating a Factor","text":"<ul> <li>We have two data types in our data frame, character values, and numeric values</li> <li>Sometimes a character value will have an order to it (i.e. low, medium, high) </li> <li>In R when you provide an order to a character variable it is a factor data type</li> <li>Let's make our RBC Status column a factor specifying the order should be Low then High RBC count</li> </ul> <pre><code># make the day column a factor\nmerged$RBC_Status &lt;- factor(\n  merged$RBC_Status,\n  levels = c(\n    \"Low RBC Count\",\n    \"High RBC Count\"\n  )\n)\nmerged$RBC_Status\n</code></pre> <p>output</p> <pre><code>[1] Low RBC Count  High RBC Count High RBC Count High RBC Count Low RBC Count  Low RBC Count  Low RBC Count \n[8] Low RBC Count  Low RBC Count \nLevels: Low RBC Count High RBC Count\n</code></pre>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#visualizing-data","title":"Visualizing Data","text":"<ul> <li>Now for the fun part of R: data visualization!</li> <li>There are a few different ways to plot in R, but today we will show you how to plot using the <code>ggplot2</code> package as it is widely popular among R users.</li> <li>NOTE: <code>ggplot2</code> is a part of the <code>tidyverse</code> package that we already loaded so we don't need to load it again.</li> <li>Here we will plot:</li> <li>RBC counts versus Taxa Counts</li> <li>Antibiotic Usage versus Taxa Counts</li> </ul> <pre><code>rbc_v_taxa &lt;- ggplot(merged,                        # data to use\n                     aes(x=RBC,                     # x axis data\n                         y = TaxaCount,             # y axis data\n                         color=AntibioticUsage))+   # column to color data by\n  geom_point()+                                     # this plot is a scatterplot\n  theme_bw()+                                       # the theme is theme_bw()\n  labs(                                             \n    x=\"RBC Counts\",                                 # x axis title\n    y=\"Taxa Counts\",                                # y axis title\n    color=\"Antibiotic Usage\",                       # legend title\n    title=\"RBC Counts v. Taxa Counts\"               # figure title\n  )\nrbc_v_taxa\n</code></pre> <p>What did we we do:</p> <ul> <li>Created a scatter plot where:</li> <li>we used the <code>ggplot()</code> function to specify our data, and inside this function we used the <code>aes()</code> function to specify which columns we wanted to plot (x axis being the <code>RBC</code> column and the y axis being the <code>TaxaCount</code> column)</li> <li>inside the <code>aes()</code> function we specified the <code>color</code> argument to indicate we want to color by the column <code>Antibiotic Usage</code></li> <li>we used the <code>geom_point()</code> function to specify this is a scatter plot</li> <li>we used the <code>theme_bw()</code> function to style this plot using the <code>theme_bw()</code> style</li> <li>we used the <code>labs()</code> function to specify our X axis title, y axis title, legend title and figure title</li> <li>we then saved this figure to the variable <code>rbc_v_taxa</code></li> <li>For more information on plotting with ggplot visit:</li> </ul> <p>http://www.sthda.com/english/wiki/be-awesome-in-ggplot2-a-practical-guide-to-be-highly-effective-r-software-and-data-visualization</p>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#saving-plotsdata","title":"Saving Plots/Data","text":"<ul> <li>Now that we have created all this wonderful data and plots we should learn how to save them!</li> </ul> <pre><code># to save new data frame\nwrite_delim(x = merged,\n            file = \"../results/merged.tsv\",\n            delim = \"\\t\")\n\n# to save our plot\nggsave(filename = \"../results/rbc_v_taxa.png\",\n       plot = rbc_v_taxa)\n</code></pre> <p>What did we do:</p> <ul> <li>To save our new merged data frame:</li> <li>we used the <code>write_delim</code> function</li> <li>specified our data frame, or <code>x</code> argument, to be the variable <code>merged</code></li> <li>we said we wanted to save our <code>file</code> one folder up \"../\" in the results folder, \"results/\" as \"merged.tsv\"</li> <li> <p>we also noted that our file should be separated or delimited, <code>delim</code>, by tabs <code>\\t</code></p> </li> <li> <p>To save our plot:</p> </li> <li>we used the <code>ggsave</code> function</li> <li>we said we wanted to save our file (<code>filename</code>) one folder up \"../\" in the results folder, \"results/\" as \"rbc_v_taxa.png\"</li> <li>specified our plot, <code>plot</code>, to be the variable <code>rbc_v_taxa</code></li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#getting-help","title":"Getting Help","text":"<ul> <li>Sometimes we won't know what every function does. </li> <li>Let's investigate the <code>aes()</code> function we just used to create our plot!</li> </ul> <pre><code>?aes\n</code></pre> <p>What did we do:</p> <ul> <li>To investigate the <code>aes()</code> function we:</li> <li>put a <code>?</code> in front of the function of interest. </li> <li>then in the help window we see a description of the function and examples on how to use it!</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#creating-the-markdown-report","title":"Creating the Markdown Report","text":"<ul> <li>Now this combination of text and code can be \"knitted\" into a report of our choice. </li> <li>Today we will be creating an HTML page of our results. </li> <li>For a full list of R markdown output options visit:</li> </ul> <p>https://rmarkdown.rstudio.com/lesson-9.html</p> <ul> <li>To create our output file go to the top of the script window and click \"Knit\"!</li> </ul> <p>Thanks for taking part in the Intro To R for the Life Sciences Tutorial!</p> <p>So as a summary we learned about:</p> <ul> <li>project organization</li> <li>R packages and how to access them on the tufts HPC</li> <li>working with variables and data frames</li> <li>visualizing data</li> <li>and finally writing a markdown report of our findings</li> </ul>"},{"location":"2022_workshops/introRStudio/03_r-workshop-markdown/#references","title":"References","text":"<ol> <li>A Gentle Introduction to R</li> <li>R for Reproducible Scientific Analysis</li> <li>Markdown Syntax</li> <li>Programming With R</li> <li>Be Awesome in ggplot2: A Practical Guide to be Highly Effective - R software and data visualization</li> <li>Output Formats</li> </ol>"},{"location":"2023_workshops/2023_workshops/","title":"Schedule","text":""},{"location":"2023_workshops/2023_workshops/#2023-workshops","title":"2023 Workshops","text":"<p>The TTS Research Technology Bioinformatics Team has prepared the following workshops:</p> <ul> <li>Finding &amp; Analyzing Metagenomics Data</li> <li>Intro to RNA-Seq Using Galaxy</li> <li>Cas12a Variant Structure Prediction</li> <li>Trajectory Analysis</li> </ul> <p>If you have suggestions for future workshops, please reach out to bioinformatics-workshop-questions@elist.tufts.edu. Additionally, if you'd like to be kept up to date on current workshops consider subscribing to our e-list: best@elist.tufts.edu</p>"},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/","title":"Introduction","text":""},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/#cas12a-variant-structural-predictions-with-alphafold2","title":"Cas12a Variant Structural Predictions With AlphaFold2","text":"<p>Content developed by TTS Research Technology</p> <ul> <li>Jason Laird, MSc, Bioinformatics Scientist<sup>1</sup></li> </ul> <ol> <li>Research Technology, Tufts Technology Services, Tufts University</li> </ol>"},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/#the-research-technology-team","title":"The Research Technology Team","text":"<ul> <li>Consultation on Projects and Grants</li> <li>High Performance Compute Cluster</li> <li>Workshops</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download PyMOL</li> <li>Account on the HPC (Note: everyone in CHBE-165 should have an account already set up. To test your account log into OnDemand. If you can log in, you are all set, if not please reach out to tts-research@tufts.edu)</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/#schedule","title":"Schedule","text":"<ol> <li>Introduction To Cas12a Variant Structural Predictions With AlphaFold2</li> <li>AlphaFold2 Accuracy Assessment</li> <li>Cas12a Variant Visualization</li> </ol>"},{"location":"2023_workshops/cas12aAlphaFold2/00_introduction/#e-list","title":"E-List","text":"<p>To find out about Bioinformatics Education, Software and Tools you can subscribe to our e-list: best@elist.tufts.edu </p>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/","title":"AlphaFold2 Accuracy Assessment","text":""},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#alphafold2-accuracy-assessment","title":"AlphaFold2 Accuracy Assessment","text":"<p>We can assess the accuracy of the AlphaFold prediction using:</p> <ul> <li>Predicted Local Distance Difference Test (pLDDT)</li> <li>Predicted Alignment Error</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#predicted-local-distance-difference-test-plddt","title":"Predicted Local Distance Difference Test (pLDDT)","text":"<ul> <li>per-residue confidence metric  ranging from 0-100 (100 being the highest confidence)</li> <li>Regions below 50 could indicate disordered regions</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#predicted-alignment-error-pae","title":"Predicted Alignment Error (PAE)","text":"<ul> <li>The Predicted Alignment Error (PAE) gives us an expected distance error based on each residue.</li> <li>If we are more confident that the distance between two residues is accurate, then the PAE is lower (darker green). If we are less confident that the distance between two residues is accurate, the PAE is higher (lighter green)</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<ul> <li> <p>Now that we have an idea of what these metrics mean, let's try generating these plots for the Cas12a-CWF mutant on the cluster. First navigate to: https://ondemand.pax.tufts.edu/</p> </li> <li> <p>Log in with your Tufts credentials</p> </li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <p></p> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <pre><code>[tutln01@login001 ~]$\n</code></pre> <ul> <li>This indicates you are logged in to the login node of the cluster. Please do not run any program from the login node.</li> </ul> Are you logged into OnDemand? <ul> <li>Yes</li> <li>No</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<ul> <li>To run our analyses we will need to move from the login node to a compute node. We can do this by entering:</li> </ul> <pre><code>srun -p batch --time=3:00:00 -n 2 --mem=4g --reservation=chbe165 --pty bash\n</code></pre> Explanation of Commands <ul> <li><code>srun</code>: SLURM command to run a parallel job</li> <li><code>-p</code>: asking for a partition, here we are requesting the batch partition</li> <li><code>--time</code>: time we need here we request 3 hours</li> <li><code>-n</code>:  number of CPUs needed here we requested 2</li> <li><code>--mem</code>:  memory we need here we request 4 Gigabytes</li> <li><code>--reservation</code>: the reservation of compute resources to use here we use the <code>chbe165</code> reservation</li> <li><code>--pty</code>: get a pseudo bash terminal</li> </ul> <p>The <code>chbe165</code> reservation will be unavailable a few days after this workshop. This reservation is a temporary reservation for this class. To get on an interactive node outside of this session just remove --reservation=chbe165 from the command!</p> <ul> <li>When you get a compute node you'll note that your prompt will no longer say login and instead say the name of the node:</li> </ul> <pre><code>[tutln01@c1cmp048 ~]$\n</code></pre>"},{"location":"2023_workshops/cas12aAlphaFold2/02_AlphaFold2_Accuracy_Assessment/#set-up-for-analysis","title":"Set Up For Analysis","text":"<ul> <li>To get our AlphaFold data we will enter:</li> </ul> <pre><code>cp -r /cluster/tufts/bio/tools/training/cas12a_af2 ./\n</code></pre> <ul> <li>Now let's go into this folder with the following command:</li> </ul> <pre><code>cd cas12a_af2 \n</code></pre> <ul> <li>Given that AlphaFold2 can take anywhere from a few hours to a few days to run - AlphaFold2 predictions have already been generated for the Cas12a-CWF mutants from our study. We will use a script from the VIB Bioinformatics Core to visualize the accuracy of AlphaFold2's predictions. First we will need to load the software needed to run that script:</li> </ul> <pre><code>module load alphafold/2.1.1\n</code></pre> <ul> <li> <p>Now we will need to feed our script three arguments:</p> <ul> <li><code>--input_dir</code> input directory with model files </li> <li><code>--output_dir</code> output directory to put our plots of model information</li> <li><code>--name</code> optional prefix to add to our file names</li> </ul> </li> </ul> <pre><code>python af2_accuracy_viz.py --input_dir mut2cwf/5XUS_mut2cwf --output_dir ./ --name mut2cwf\n</code></pre> <ul> <li> <p>Running this will generate two images in your current directory:</p> <ul> <li><code>mut2cwf_coverage_LDDT.png</code> - plots of your msa coverage and pLDDT scores per residue per model</li> <li><code>mut2cwf_PAE.png</code> - plots of your predicted alignment error </li> </ul> </li> <li> <p>The following is are the pLDDT and PAE scores for the Cas12a-CWF mutant:</p> </li> </ul> <p></p> <ul> <li>You'll note that for the pLDDT plots, that the multiple sequence alignment is plotted with a bar on the side to tell you how similar those sequences were to your query sequence (in this case each of the Cas12a-CWF mutant)</li> </ul> Do you see a region in the MSA plot that seems more conserved? How about less conserved? How does this coverage seem to affect the confidence in each residues position in the pLDDT plots?"},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/","title":"Cas12a Variant Visualization","text":""},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/#cas12a-variant-visualization","title":"Cas12a Variant Visualization","text":"<ul> <li>In the previous slide we plotted our MSA alignment, the pLDDT scores, and the predicted alignement error. However, it is also useful to visualize the actual predicted protein structure and compare it to the known structure if there is one. Here we use a software called PyMOL to do just that:</li> </ul> <ul> <li>Here we see that PyMOL takes either the PDB ID or a PDB file and creates a vizualization for us to examine. If you have not done so already please download PyMOL and open the app. You should see a window like the follwing:</li> </ul> <ul> <li>Here we have a:</li> <li>History Window with log of previous commands</li> <li>Command Interface to enter PyMOL commands</li> <li>List of Objects Loaded which list of objects/proteins that have been loaded into PyMOL</li> <li> <p>Visualization Window to visualize protiens loaded into PyMOL</p> </li> <li> <p>Let's try on our data!</p> </li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/#download-alphafold-output","title":"Download AlphaFold Output","text":"<ul> <li>First we will need to download our predicted structure pdb file for the mutant, Cas12a mut2-CWF. To this, go to <code>Files &gt; Home Directory</code>:</li> </ul> <ul> <li>Now, click on cas12a_af2 and download the following file, <code>mut2cwf.pdb</code>:</li> </ul>"},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/#importing-structures","title":"Importing Structures","text":"<ul> <li>To visualize this protein structure in PyMOL, open PyMOL on your computer</li> <li>Go to File &gt; Open - then choose your pdb file.</li> <li>The file we have loaded is the Cas12a-CWF mutant, let's fetch the structure for the wild type Cas12a protein:</li> </ul> <pre><code>fetch 5xus\n</code></pre> <ul> <li>We will now align this structure with the Cas12a-CWF mutant. So in the PyMOL command prompt enter:</li> </ul> <pre><code>align mut2cwf, 5xus\n</code></pre>"},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/#viewingcoloring-structures","title":"Viewing/Coloring Structures","text":"<ul> <li>To view one structure at a time, you can use the <code>disable</code> command to hide one of the structures:</li> </ul> <pre><code>disable 5xus\n</code></pre> <ul> <li>To see this structure again we can simply use the <code>enable</code> command:</li> </ul> <pre><code>enable 5xus\n</code></pre> <ul> <li>To change the color of our structures we can use the following command:</li> </ul> <pre><code>color lightblue, 5xus\n</code></pre> <pre><code>color salmon, mut2cwf\n</code></pre>"},{"location":"2023_workshops/cas12aAlphaFold2/03_Cas12a_Variant_Vizualization/#visualizing-variants","title":"Visualizing Variants","text":"<ul> <li>Now that we have aligned/colored our structures, let's select residues on the RuvC Domain on the Cas12a mut2-CWF and the Cas12a wild-type:</li> </ul> <pre><code>select resi 863+952+965+1214 and name CA\n</code></pre> <p>Note</p> <p>Note we are only selecting the alpha carbons so that when we label these residues we only have one label per residue</p> <ul> <li>To label these residues we can use the following:</li> </ul> <pre><code>label sele, \" %s%s\" % (resn,resi)\n</code></pre> <ul> <li>With these residues selected we can color them to visualize them easier:</li> </ul> <pre><code>color red, sele\n</code></pre> <ul> <li>Let's now zoom into this region:</li> </ul> <pre><code>zoom sele\n</code></pre> <ul> <li>To capture this image we can go to <code>File &gt; Export Image As &gt; PNG... &gt; Save PNG image as ...</code> and enter a name for your image!</li> <li>Alternatively, you can take a screen shot and save.</li> </ul> Can you spot any disordered regions that AlphaFold2 may not have predicted well? Include an image displaying one of these regions.  Ma et al. 2022 note that the Cas12a mut2-CWF has a more open active site than the wild type Cas12a. Disable the Cas12a mut2-CWF mutant and include an image of the wild type Cas12a. Then enable Cas12a mut2-CWF mutant, disable the wild type Cas12a and include an image of the Cas12a mut2-CWF mutant. Do you agree that the catalytic site appears more open in comparison to the wild type structure (5xus)?"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/","title":"Introduction","text":""},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#introduction-to-galaxy","title":"Introduction to Galaxy","text":"<p>Galaxy is a web-based platform for running data analysis and integration, geared towards bioinformatics. - Open-source, public servers - Developed at Penn State, Johns Hopkins, OHSU and Cleveland Clinic with many more outside contributions - Large and extremely responsive community </p>"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#galaxy-on-the-tufts-university-high-performance-compute-hpc-cluster","title":"Galaxy on the Tufts University High Performance Compute (HPC) Cluster","text":"<ul> <li>Our Galaxy server runs on the HPC cluster, storing data on HPC drives and using compute nodes and Slurm scheduler to run user jobs</li> <li>In practice, this means that Tufts users have more resources on Tufts' Galaxy compared to public servers</li> <li>We also have the ability to install and configure open source and custom tools</li> <li>More information on getting access to Tufts Galaxy can be found on the Research Technology website</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#galaxy-ui","title":"Galaxy UI","text":"<ul> <li>We'll spend some time getting familiar with the Galaxy interface, layout, and key functions</li> <li>The Galaxy User Interface has a top menu bar and three panels: Tools, Main, and History</li> </ul> <ul> <li>The panels can be expanded/minimized using the small arrows and dragging the three small lines in the bottom left and right corners of the page</li> <li>To get back to the home page click the logo next to Galaxy Tufts on the top left of the page or the Analyze Data tab on the top of the page</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#histories","title":"Histories","text":"<ul> <li>Histories are where a dataset and a set of analysis are stored</li> <li>Similar to a \"working directory\".</li> <li>You can have multiple histories under the same account and you can view them all by clicking the + as shown below</li> </ul> <ul> <li>Here you can view all your histories, switch to another history, and drag and drop data between histories.</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#uploading-data","title":"Uploading Data","text":"<ul> <li>Data can be uploaded to Galaxy in a number of ways by clisking the \"Upload\" icon on the top of the Tools panel.</li> </ul> <ul> <li>\"Choose local File\" will select a file from your local computer, and \"Choose FTP File\" will select from your user drive on the HPC cluster, which has an extension \"/cluster/tufts/galaxy/xfer/username\" </li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/00_Galaxy_introduction/#tools","title":"Tools","text":"<ul> <li>On the Tools panel, either search by name or click the category that your tool would fall under</li> <li>Our tools for today will mostly be stored under the RNA-seq category</li> </ul> <ul> <li>Click the name of the tool you would like to use and the tool menu will appear in the Main panel</li> </ul> <p>Next: Setup</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup/","title":"Intro to RNA-Seq using Tufts Galaxy","text":"<p>The introductory Slides give an overview of RNAseqencing technologies and our workflow.</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup/#dataset","title":"Dataset","text":"<p>Our dataset is from the publication:</p> <p>Chang et al. Next-Generation Sequencing Reveals HIV-1-Mediated Suppression of T Cell Activation and RNA Processing and Regulation of Noncoding RNA Expression in a CD4+T Cell Line. mBio 2011doi: 10.1128/mBio.00134-11</p> <p>HIV infects CD4+ T cells, the same cells which are critical to mounting an immune response to the virus infection.  </p> <p> </p> <p>Image Source</p> <p>The experiment aims to compare the mRNA produced by Mock and HIV infected CD4+ T cells, both 12 hr and 24 hr after infection.</p> <p> </p> <p>The raw reads from the study have been downsampled to 1 million reads per file in order to speed up computation. The full dataset is available from NCBI under accession SRP013224.</p> <p>The following steps will walk you through how to run the tools. In each step certain parameters are set. If a parameter option appears on the screen but this tutorial doesn't mention how to set it, leave it at the default. There are questions throughout, which serve to guide you through the results and check your understanding.</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup/#create-a-new-history","title":"Create a new history","text":"<ul> <li>Click the + at the top of the history panel</li> <li>To rename the history, click on the box Unnamed History and type rnaseq day 1 and press enter</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup/#import-the-raw-data-from-a-shared-data-library-on-our-server","title":"Import the raw data from a shared data library on our server","text":"<ul> <li>On the top menu bar, click Shared Data and select Data Libraries</li> <li>Select chang_2011_2rep</li> <li>Just under the top menu bar, next to the search bar, click Export to History and select As a Collection. You'll see a list of fastq files.</li> <li>Click Continue to add all files as a list to our current history (No need to click on individual files)</li> <li>Name the collection chang_2011</li> <li>Click Create list</li> <li>Click on Galaxy Tufts on the top left to go back to the main panel.</li> </ul> <p>You\u2019ll see the collection (or list) chang_2011 in your history. </p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup/#view-fastq-files","title":"View Fastq files","text":"<ul> <li>Click on the list chang_2011 to expand, you\u2019ll see 11 sequencing files with extension fastq.qz</li> <li>Click on the eye icon  on the first sequence file HIV_12hr_rep1 and look at the fastq reads</li> </ul> <p>The first 4 lines constitute the first sequencing read: <pre><code>@SRR497699.30343179.1 HWI-EAS39X_10175_FC61MK0_4_117_4812_10346 length=75\nCAGATGGCCGCAGAGGAAGCCATGAAGGCCCTGCATGGGGAGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGAC\n+\nIIIIGIIHFIIIIBIIDII&gt;IIDHIIHDIIIGIFIIEIGIBDDEFIG&lt;EIEGEEG;&lt;DB@A8CC7&lt;&gt;&lt;C@BBDDB\n</code></pre></p> <ol> <li>Sequence identifier</li> <li>Sequence</li> <li> <ul> <li>(optionally lists the sequence identifier again)</li> </ul> </li> <li>Quality string</li> </ol> <p>Next: Process Raw Reads</p> <p>Previous: Introduction to Galaxy</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/","title":"Processing Raw Reads","text":""},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#import-raw-reads-from-shared-library","title":"Import Raw Reads from Shared Library","text":""},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#dataset","title":"Dataset","text":"<p>Our dataset is from the publication:</p> <p>The experiment aims to compare the mRNA produced by Mock and HIV infected CD4+ T cells, both 12 hr and 24 hr after infection.</p> <p></p> <p>The following steps will walk you through how to run tools needed for our workflow. In each step certain parameters are set. If a parameter option appears on the screen but this tutorial doesn't mention how to set it, leave it at the default. There are questions throughout, which serve to guide you through the results and check your understanding.</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#create-a-new-history","title":"Create a new history","text":"<ul> <li>Click the + at the top of the history panel</li> <li>To rename the history, click on the box Unnamed History and type rnaseq and press enter</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#import-the-raw-data-from-a-shared-data-library-on-our-server","title":"Import the raw data from a shared data library on our server","text":"<p>We'll import The raw reads from a shared library on our server. They have been downsampled to 1 million reads per file in order to speed up computation. The full dataset is available from NCBI under accession SRP013224.</p> <ul> <li>On the top menu bar, click Shared Data and select Data Libraries</li> <li>Click on subsampled_chang_2011 and then select all files in this directory</li> <li>Just under the top menu bar, next to the search bar, click Export to History and select As a Collection. You'll see a list of fastq files.</li> <li>Click Continue to add all files as a list to our current history (No need to click on individual files)</li> <li>Name the collection chang_2011</li> <li>Click Create Collection</li> <li>Click on Galaxy Tufts on the top left to go back to the main panel.</li> </ul> <p>You\u2019ll see the collection (or list) chang_2011 in your history. </p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#view-fastq-files","title":"View Fastq files","text":"<ul> <li>Click on the list chang_2011 to expand, you\u2019ll see 6 sequencing files with extension fastq.qz. These are two 12 hr HIV replicates, two 12 hr Mock replicates, and two 24 hour HIV replicate.</li> <li>Click on the eye icon on the first sequence file HIV_12hr_rep1 and look at the fastq reads</li> </ul> <p>The first 4 lines constitute the first sequencing read:</p> <pre><code>@SRR497699.30343179.1 HWI-EAS39X_10175_FC61MK0_4_117_4812_10346 length=75\nCAGATGGCCGCAGAGGAAGCCATGAAGGCCCTGCATGGGGAGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGAC\n+\nIIIIGIIHFIIIIBIIDII&gt;IIDHIIHDIIIGIFIIEIGIBDDEFIG&lt;EIEGEEG;&lt;DB@A8CC7&lt;&gt;&lt;C@BBDDB\n</code></pre> <ol> <li>Sequence identifier</li> <li>Sequence</li> <li> <ul> <li>(optionally lists the sequence identifier again)</li> </ul> </li> <li>Quality string</li> </ol>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#perform-quality-control-on-raw-reads","title":"Perform Quality Control on Raw Reads","text":"<p>FastQC provides several modules:</p> <ul> <li>Sequence Quality</li> <li>GC content</li> <li>Per base sequence content</li> <li>Adapters in Sequence</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#run-fastqc","title":"Run FastQC","text":"<ul> <li>In the Tools panel search bar, type FastQC</li> <li>Select FastQC under FASTQ Quality Control</li> <li>In the Main panel, under Raw read data from your current history select the folder icon and the dataset chang_2011 will appear as an option</li> </ul> <ul> <li>Scroll down and click Execute.The job should first appear orange and then green after a minute or so.</li> <li>The result will be two lists, one containing the raw data and one the webpage (html) results for convenient viewing in the browser.</li> </ul> <ul> <li>Click to expand the second list FastQC on collection 7: Webpage and click on the eye icon next to the sample HIV_12hr_rep1. The first table gives Basic Statistics of the sample. The Main panel will show metrics and plots. You may have to adjust the size of the panel in order to view.</li> </ul> <p>Question 1: How many sequences are in the sample HIV_12hr_rep1? What is their average length?</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#aggregate-qc-data-with-multiqc","title":"Aggregate QC data with MultiQC","text":"<p>The tool MultiQC allows us to view our QC results from all samples side by sides, in order to check for consistency across replicates. It can use the Raw Data output from FastQC and generate plots for all modules.</p> <p>Steps to run:</p> <ul> <li>In the Tools panel search bar, type MultiQC</li> <li>Select MultiQC under FASTQ Quality Control</li> <li>In the middle panel, under Which tool was used generate logs? select FastQC</li> <li>Under FastQC output click Insert FastQC output and select the collection 14: FastQC on collection 7: Raw Data </li> </ul> <p>Note</p> <p>note that the numbers 14 and 12 are tracking the dataset number in your history and might vary if you have not followed the exact sequence in this document</p> <ul> <li>Enter the Report Title \u201cRaw data QC\u201d</li> <li>Scroll down and click Execute.</li> <li>The result will again be two collections (you may have to click \"back to rnaseq\" on the top of the History panel). Select the collection titled MultiQC on data 21, data 19, and others: Webpage and click the eye icon to view.</li> </ul> <p>The first panel gives summary statistics:</p> <p></p> <p>The second figure is a bar graph showing \"Sequence Counts\" of unique and duplicate reads for each sample. The remaining figures show each FastQC metric, displaying all samples on a single graph. There is a rectangle at the top that summarizes the pass/fail status of samples. </p> <p></p> <p>Question 2: Which metrics show one or more failed samples?</p>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#trim-adapters-and-low-quality-read-ends-with-trim-galore","title":"Trim adapters and low quality read ends with Trim Galore!","text":"<ul> <li>In the Tools panel search bar, type Trim Galore!</li> <li>Select Trim Galore! under FASTQ Quality Control</li> <li>Under Is this library paired- or single-end? select \"Single-end\"</li> <li>Under Reads in FASTQ format click the folder icon and select chang_2011</li> <li>Scroll down and click Execute.</li> <li>The result will be a single collection titled Trim Galore! on collection 12: trimmed reads. Next, we\u2019ll rerun FastQC in order to see how the trimming performed</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/01_Introduction_and_Setup_noqual/#rerun-fastqc-and-multiqc","title":"Rerun FastQC and MultiQC","text":"<ul> <li>Follow the steps for Run FastQC and Aggregate QC data with MultiQC above, except select the trimmed reads generated in the previous step as the input to FastQC</li> </ul> <p>Question 3: Were any reads completely removed from the samples? Note:The MultiQC \"General Statistics\" tables shows a rounded value, so use the \"Sequence Counts\" graph.</p> <p>Question 4: Is the adapter problem solved? What about the GC content? Note: HIV replication is ramping up rapidly in these cells in the first 24 hours.</p> <p>Next: Read Alignment</p> <p>Previous: Introduction to Galaxy</p>"},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/","title":"Perform Quality Control on Raw Reads","text":""},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/#introduction-to-fastqc","title":"Introduction to FastQC","text":"<p>FastQC provides several modules (as discussed in intro Slides) - Sequence Quality - GC content - Per base sequence content - Adapters in Sequence</p>"},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/#run-fastqc","title":"Run FastQC","text":"<ul> <li>In the Tools panel search bar, type FastQC</li> <li>Select FastQC under FASTQ Quality Control</li> <li>In the Main panel, under Short read data from your current history select the folder icon  and the dataset chang_2011 will appear as an option</li> </ul> <ul> <li>Scroll down and click Execute.The job should first appear orange and then green after a minute or so.</li> <li>The result will be two lists, one containing the raw data and one the webpage (html) results for convenient viewing in the browser.</li> </ul> <ul> <li>Click to expand the second list FastQC on collection 12: Webpage and click on the  next to the first file for sample HIV_12hr_rep1. The first table gives Basic Statistics of the sample. The Main panel will show metrics and plots. You may have to adjust the size of the panel in order to view.</li> </ul> Question 2: Were you right about your guess of quality encoding?"},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/#aggregate-qc-data-with-multiqc","title":"Aggregate QC data with MultiQC","text":"<p>The tool MultiQC allows us to view our QC results from all samples side by sides, in order to check for consistency across replicates. It can use the Raw Data output from FastQC and generate plots for all modules.</p> <p>Steps to run: - In the Tools panel search bar, type MultiQC - Select MultiQC under FASTQ Quality Control - In the middle panel, under Which tool was used generate logs? select FastQC - Under FastQC output click the  and select the collection 14: FastQC on collection 12: Raw Data (note that the numbers 14 and 12 are tracking the dataset number in your history and might vary if you have not followed the exact sequence in this document) - Enter the Report Title \u201cRaw data QC\u201d - Scroll down and click Execute. - The result will again be two collections (you may have to click \"back to rnaseq day 1\" on the top of the History panel). Select the collection titled MultiQC on data 36, data 34, and others: Webpage and click the  to view. (If panels show \"loading\" for more than a few seconds, click the  a second time to refresh)</p> <p></p> Question 3: Using what we learned in lecture, which metrics show one or more failed samples?"},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/#trim-adapters-and-low-quality-read-ends-with-trim-galore","title":"Trim adapters and low quality read ends with Trim Galore!","text":"<ul> <li>In the Tools panel search bar, type Trim Galore!</li> <li>Select Trim Galore! under FASTQ Quality Control</li> <li>Under Reads in FASTQ format click the  and select chang_2011</li> <li>Scroll down and click Execute.</li> <li>The result will be a single collection titled Trim Galore! on collection 12: trimmed reads. Next, we\u2019ll rerun FastQC in order to see how the trimming performed</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/02_Process_raw_reads/#rerun-fastqc-and-multiqc","title":"Rerun FastQC and MultiQC","text":"<ul> <li>Follow the steps for Run FastQC and Aggregate QC data with MultiQC above, except select the trimmed reads generated in the previous step as the input to FastQC</li> </ul> Question 4: Were any reads completely removed from the samples? Note: The MultiQC \"General Statistics\" table may show a rounded value, so be sure to double check with the individual sample FastQC tables.  Question 5: Is the adapter problem solved? What about the GC content? Note that HIV replication is ramping up rapidly in these cells in the first 24 hours. <p>Next: Read Alignment</p> <p>Previous: Setup</p>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/","title":"Read Alignment","text":""},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#read-alignment","title":"Read Alignment","text":"<ul> <li>RNAseq data originates from spliced mRNA (no introns)</li> <li>When aligning to the genome, our aligner must find a spliced alignment for reads</li> <li>We use a tool called STAR (Spliced Transcripts Alignment to a Reference) that has a exon-aware mapping algorithm.</li> </ul> <p>STAR: ultrafast universal RNA-seq aligner</p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#sam-format","title":"SAM format","text":"<p>STAR produces a file in Sequence Alignment Map (SAM) format or the compressed version BAM.</p> <p>SAM File Format</p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#genome-annotation-standards","title":"Genome Annotation Standards","text":"<ul> <li>STAR can use an annotation file gives the location and structure of genes in order to improve alignment in known splice junctions </li> <li>Annotation is dynamic and there are at least three major sources of annotation </li> <li>The intersection among RefGene, UCSC, and Ensembl annotations shows high overlap. RefGene has the fewest unique genes, while more than 50% of genes in Ensembl are unique. </li> <li>Be consistent with your choice of annotation source! </li> </ul> <p>A comprehensive evaluation of ensembl, RefSeq, and UCSC annotations in the context of RNA-seq read mapping and gene quantification</p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#gtf-gene-annotation","title":"GTF Gene Annotation","text":"<ul> <li>In order to count genes, we need to know where they are located in the reference sequence</li> <li>STAR uses a Gene Transfer Format (GTF) file for gene annotation </li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#import-a-gene-annotation-file-from-a-data-library-to-be-used-for-feature-counting","title":"Import a gene annotation file from a Data Library to be used for feature counting","text":"<ul> <li>Click Shared Data on the top menu bar and select Data Libraries</li> <li>Click on annotation_files </li> <li>Select the box next to hg38_genes.gtf and hg38_genes.bed.</li> <li>Click Export to History next to the Search bar and choose as Datasets</li> <li>Click Import to add the file to our current history (rnaseq)</li> <li>Click Galaxy Tufts in the top left to return to the homepage</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#align-the-reads-to-the-human-genome-using-star-aligner","title":"Align the reads to the human genome using STAR aligner","text":"<ul> <li>In the Tools panel search bar, type STAR</li> <li>Scroll down and select RNA STAR under RNA-seq</li> <li>Under Single-end or paired-end reads select Single-End</li> <li>Under RNA-Seq FASTQ/FASTA file click the folder icon and select the trimmed reads 42: Trim Galore! on collection 7: trimmed reads</li> <li>STAR gives us the option of using a genome that includes a database of known splice junction locations or providing a gtf file so that STAR can create the database. We\u2019ll select a reference genome on our server that already includes the splice junctions listed in our GTF file. Under Reference genome with or without an annotation select use genome reference with built-in gene-model.</li> <li>Under Select reference genome select hg38-with-genes.</li> <li>Scroll down and click Execute</li> <li>The result will be three collections, giving the bam, splice junctions and log files for the alignments</li> </ul>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#run-multiqc-on-the-star-log-files-to-check-the-result-of-the-alignment","title":"Run MultiQC on the STAR log files to check the result of the alignment","text":"<ul> <li>Follow the steps from the previous section to run MultiQC except: <ul> <li>Under Which tool was used generate logs?  select STAR</li> <li>Under STAR log output click the folder icon and select the collection 61: RNA STAR on collection 27:log</li> </ul> </li> <li>After the job finished, click the eye icon to view the webpage.</li> </ul> <p>Question 5: In RNAseq, the percentages of uniquely aligned reads are typically lower than for DNAseq, due to the presence of unremoved ribosomal RNA. These are present in multiple copies throughout the genome and cause reads not to be mapped confidently. RNAseq is expected to be above 75% for an uncontaminated human sample. Is the \"% Aligned\" above 75% for these samples?</p>"},{"location":"2023_workshops/galaxyRNASeq/03_Read_alignment/#view-bam-file-using-jbrowse","title":"View bam file using JBrowse","text":"<ul> <li>In the Tools panel search bar, type JBrowse and select JBrowse genome browser</li> <li>Under Select a reference genome select hg38</li> </ul> <p>Next we'll add two Track groups, each with an annotation track:</p> <ul> <li>Under Track Group click + Insert Track Group</li> <li>Click + Insert Annotation Track</li> <li>Select track type BAM Pileups and under BAM Track Data click the folder icon and select the list RNA STAR on collection: mapped.bam</li> <li>Scroll down and click Insert Annotation Track</li> <li>Select track type GFF/GFF3/BED Features and under GFF/GFF3/BED Track Data select hg38_genes.bed.</li> </ul> <p>Finally, run the job: - Scroll down and click Execute. - Once the job is complete (green) click the eye icon to view the data.  - In the Available Tracks panel select the HIV and Mock samples from 12 hr, as well as the bed file.</p> <p></p> <ul> <li>We'll zoom in on one gene MYC. To do this, click on the search bar to the left of the Go button and type <code>chr8:127735434-127742951</code>. </li> </ul> <p>Note</p> <p>Note that you can't search by gene name in this tool.</p> <ul> <li>The bam tracks will show the reads that align to the region for each sample. </li> <li>The color will show whether the read aligns to the + or \u2013strand and grey lines show splice regions where a read spans an intron. </li> <li>The gene track at the top called hg38_genes.bed will display the gene MYC, by clicking on the gene representation you will be able to see the different feature types (exon, CDS, start_codon, stop_codon):</li> </ul> <p></p> <p>Question 6: Which samples appear to show higher expression of MYC, the Mock or HIV?</p> <p>Question 7: How many exons do you think MYC has? Hint: When we aligned our reads we mapped to exons. Do we see distinct blocks of reads and if so how many?</p> <p>Next: Gene Quantification</p> <p>Previous: Process Raw Reads</p>"},{"location":"2023_workshops/galaxyRNASeq/04_Gene_quantification/","title":"Gene Quantification","text":""},{"location":"2023_workshops/galaxyRNASeq/04_Gene_quantification/#counting-reads-for-each-gene","title":"Counting reads for each gene","text":"<p>Our next step is to quantify the spliced reads that aligned to each gene in our GTF file For two non-overlapping, multiple-exon genes, our alignment may look like this:</p> <p></p> <p>The tool featureCounts is part of the subRead package:</p> <ul> <li>The mapped coordinates of each read are compared with the features in the GTF file</li> <li>Reads that overlap with a gene by &gt;=1 bp are counted as belonging to that feature </li> <li>In default mode, ambiguous reads will be discarded</li> </ul> <p>Subread Package</p> <p></p> <p>The result is a gene count matrix:</p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/04_Gene_quantification/#running-featurecounts","title":"Running featureCounts","text":"<ul> <li>In the Tools panel search bar, type featureCounts</li> <li>Select featureCounts under RNA-seq</li> <li>Under Alignment file click the folder icon and select the bam collection generated by STAR</li> <li>Under Gene annotation file select in your history</li> <li>Select hg38_genes.gtf</li> <li>Click Execute</li> <li>The result will be two collections: Summary and Counts</li> <li>View the Counts file for a sample by clicking the collection and clicking the eye icon </li> <li>Run MultiQC on the Summary collection (similar to previous steps, except selecting the appropriate tool (featureCounts) and input folder (featureCounts Summary)).</li> </ul> <p>Question 8: Locate the \"featureCounts:Assignment\" plot, which shows whether reads were assigned to genes (features) or whether they failed to be assigned. What is the main reason for reads not being \"Assigned\"?</p> <p>Next: Differential Expression</p> <p>Previous: Read Alignment</p>"},{"location":"2023_workshops/galaxyRNASeq/05_Diff_expression/","title":"Testing for Differential Expression using DESeq2","text":"<p>Much of this explanation has been adapted from these two sources: - HBC bioinformatics core - DESeq2 Vignette</p> <p>The following DESeq2 steps are run all at once in Galaxy.</p> <p>HBC Differential Gene Expression</p> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/05_Diff_expression/#create-separate-collections-for-the-counts-files-for-mock-12-hr-and-hiv-12-hr","title":"Create separate collections for the counts files for Mock 12 hr and HIV 12 hr.","text":"<ul> <li>In the Tools panel search bar, type Apply Rules</li> <li>Under Collection Operations select Apply Rules</li> <li>In the main panel, under Input Collection choose 125:featureCounts on collection 85: Counts</li> </ul> <p>First, we filter for the samples of interest: - Under Rules click Edit - Click Filter, then Using a Regular Expression - Under Regular Expression?, type HIV_12 (Do not put any extra spaces following the expression). This is a regular expression which will match any file that contains the string HIV_12 - Click Apply and the list of files in column A should show two samples:</p> <p></p> <p>Galaxy requires that lists have an identifier column - Click Rules, and then Add/Modify Column Definitions  - Click Add Definition, then List Identifier(s), select A, click Apply - Click Save - Click Execute - Rename the collection by clicking to expand the collection, then clicking on the title, typing HIV_12 , and pressing enter.</p> <p></p> <ul> <li>Do the same for Mock 12 hour samples, and modify the regular expression appropriately.</li> <li>The two collections should appear in the history (you may have to refresh history in order to see the modified names)</li> </ul> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/05_Diff_expression/#test-for-differential-expression-using-deseq2","title":"Test for Differential Expression using DESeq2","text":"<ul> <li>In the Tools panel search bar, type DESeq2 and under RNA-seq select DESeq2 </li> </ul> <p>DESeq2 will take the count tables that we generated, one per sample, and make a comparison for each gene between two conditions: HIV and Mock.  The term that DESeq2 uses for this condition is \"factor\" and the ordering of our factor levels will determine how we interpret the resulting expression fold changes. Here, we'll set Factor 1 to HIV and Factor 2 to Mock. Any resulting upregulated genes, with log2 fold change &gt; 0, can then be interpreted as being upregulated in HIV samples with respect to Mock.</p> <ul> <li>Under 1: Factor , specify the factor name Condition</li> <li>Under 1: Factor Level, specify the base factor level HIV</li> <li>Under Counts file(s) select the folder icon and select the HIV_12 collection</li> <li>Set the 2: FactorLevel to Mock</li> <li>Under Counts file(s) select the folder icon and select the Mock_12 collection</li> <li>Scroll down and click Execute</li> <li>DESeq2 will produce two output datasets: One containing plots and another containing a results table.</li> </ul> <p></p>"},{"location":"2023_workshops/galaxyRNASeq/05_Diff_expression/#view-and-interpret-deseq2-output-files","title":"View and interpret DESeq2 output files","text":"<ul> <li>Results file: View the results table by clicking on the history item DESeq2 result file on data ... and others and clicking on the eye icon.</li> </ul> <p>Question 9: What are the top two most significant genes? Does the direction of change for gene MYC agree with our observation in Question 6?</p> <ul> <li>Plots: View the plots by clicking on the history item DESeq2 plots on data ... and others and clicking the eye icon.</li> </ul> <p>Question 10: What observations can you make from the PCA plot? Do samples cluster as expected?</p> <p>The p-value plot shows a histogram of p-values for all the genes that were examined. P-values give the probability of getting a logFC as extreme as observed if the true logFC = 0 for that gene (null hypothesis).  Random P-values are expected to be uniform, if you have true positives you should see a peak close to zero.</p> <p>Variance Explained</p> <p></p> <p>Question 11: What observation can you make about the pvalue distribution, does it look like there are many true significant results? Note that the published dataset has been downsampled for instructional purposes.</p> <p>Previous: Gene quantification</p>"},{"location":"2023_workshops/metagenomeData/00_introduction/","title":"Finding and Analyzing Metagenomic Data","text":"<p>Adelaide Rhodes, Ph.D., Senior Bioinformatics Scientist</p> <p>Jason Laird, MSc, Bioinformatics Scientist</p> <p>Adapted from the NCBI Tutorial by Cooper Park, Ph.D. (thanks Cooper!)</p>"},{"location":"2023_workshops/metagenomeData/00_introduction/#learning-objectives","title":"Learning Objectives:","text":"<ul> <li>Introduction</li> <li>Objective 1 - Searching for SRA Data &amp; Metadata on the NCBI Website </li> <li>Objective 2 - Exploring Taxonomic Composition of SRA reads using STAT</li> <li>Objective 3 - Creating a Taxonomic Report using Kraken</li> <li>Objective 4 - Compare Kraken and STAT output</li> </ul>"},{"location":"2023_workshops/metagenomeData/00_introduction/#introduction","title":"Introduction","text":"<p>This workshop, and this Jupyter Notebook, is designed to introduce you to:</p> <ul> <li>How Metagenomic Sequence Data is stored and manipulated within NCBI</li> <li>How to Download a subset of data from the NCBI</li> <li>How to Generate a Taxonomic Report using Kraken</li> <li>How to Visualize the Kraken data using Krona</li> </ul>"},{"location":"2023_workshops/metagenomeData/00_introduction/#case-study","title":"Case Study","text":"<p>The case study for this workshop involves a single patient named \"Patient B\" who was clinically diagnosed with Microbial Keratitis (bacterial infection of the eye's cornea). This disease is typically caused by three species:</p> <ul> <li>Pseudomonas aeruginosa</li> <li>Staphylococcus aureus</li> <li>Bacillus subtilis. </li> </ul> <p>Diagnosis of this disease can be unreliable as it depends on labratory culture tests that are often false-negatives and can take up to 48hrs. </p> <p>The authors of the paper used in this case study used metagenomic sequencing of a corneal scraping to characterize potential bacterial contamination of both of Patient B's eyes to pilot a method for diagnosing Microbial Keratitis via sequencing. </p> <p>Today, we will use that same sequence data to validate their results and demonstrate how the NCBI tool STAT and Kraken2 provide similar results.</p>"},{"location":"2023_workshops/metagenomeData/00_introduction/#navigating-to-ondemend","title":"Navigating to OnDemend","text":"<p>We will be working on the Tufts HPC cluster today! To get there use the following link and log in with your Tufts credentials:</p> <p>Tufts OnDemand</p> <p>Once you get there we will need to open up Jupyter Notebook to run our code! Navigate to Interactive apps and then scroll down to Jupyter Lab. Enter the following specifications:</p> <p></p> <p>Click the Launch button to start your session!</p>"},{"location":"2023_workshops/metagenomeData/00_introduction/#copying-over-the-data","title":"Copying over the Data","text":"<p>For class today we will be copying over a folder with all the data/scripts we will need for today's exercise. To copy it over we will click on the Terminal button to start a terminal session:</p> <p></p> <p>Now enter the following into command line:</p> <pre><code>cp -r  /cluster/tufts/bio/tools/training/metagenomeData ./\n</code></pre> <p>Now on the right hand side of your screen you should see the folder, SOMEFOLDER! Double click on this folder and then double click on <code>metagenome_exercises.ipynb</code>. Now you have opened a Jupyter Notebook!</p>"},{"location":"2023_workshops/metagenomeData/00_introduction/#what-is-a-jupyter-notebook","title":"What is a Jupyter Notebook?","text":"<ul> <li> <p>Jupyter Notebooks are a web-based approach to interactive code. </p> </li> <li> <p>A single notebook (the file you are currently reading) is composed of many \"cells\" which can contain either text, or code. </p> </li> <li> <p>To navigate between cells, either click on the cell, or use the arrow keys on your keyboard.</p> </li> <li> <p>The order of the cells is important. Please do not skip any - the output from a cell higher up on the page may be needed as input for a cell lower down on the page.</p> </li> <li> <p>A text cell will look like... well... this! </p> </li> <li> <p>A code cell will look something like what you see below. </p> </li> <li> <p>To run the code inside a code cell, click on it, then click the \"Run\" button at the top of the screen. </p> </li> <li> <p>Try it on the code cell below!</p> </li> </ul> <pre><code>#This is a code cell\nprint('You ran the code cell!')\n</code></pre> <p>output</p> <pre><code>You ran the code cell!\n</code></pre> <p>If it worked, you should have seen text pop up underneath the cell saying <code>You ran the code cell!</code>. </p> <ul> <li> <p><code>In [1]:</code> that appeared next to the cell. This tells you the order you have run code cells throughout the notebook. </p> </li> <li> <p>The next time you run a code cell, it will say <code>In [2]:</code>, then <code>In [3]:</code> and so on... This will help you know if/when code has been run.</p> </li> <li> <p>The remainder of the notebook below has been pre-built by the workshop organizer. </p> </li> <li> <p>You will not need to create any new cells, and you will be explicitly told if/when to execute a code cell.</p> </li> <li> <p>The code in this workshop is either Bash (i.e., terminal commands) or Python. Bash commands are prefixed with <code>!</code> while Python commands are not. </p> </li> <li> <p>If you are not familiar with code, don't feel pressured to interpret it very deeply. Descriptions of each code block will be provided!</p> </li> </ul>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/","title":"Objective 1 - Searching for SRA Data &amp; Metadata on the NCBI Website","text":""},{"location":"2023_workshops/metagenomeData/01_searchSRA/#objective-goals","title":"Objective Goals","text":"<ol> <li>Search the NCBI website for SRA sequence data and BioSample metadata</li> <li>Use STAT to gain preliminary insights into sequence read taxonomy distribution</li> </ol>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-1-get-to-ncbi","title":"Step 1 - Get to NCBI","text":"<p>Navigate to https://www.ncbi.nlm.nih.gov/</p>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-2-find-the-bioproject","title":"Step 2 - Find the BioProject","text":"<p>Using the search bar at the top of the screen, set the database to <code>BioProject</code> and type <code>PRJEB37709</code> into the search box to look for today's BioProject.</p> <p></p> <p>The BioProject page provides some background about the project including:</p> <pre><code>- Description (in this case, it appears to be the abstract from the paper)\n- Submission info (*e.g.*, accession number and submitter information)\n- Associated BioProject data (*e.g.* SRA experiments and BioSamples)\n</code></pre> <p></p>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-3-find-the-biosamples","title":"Step 3 - Find the BioSamples","text":"<p>Click on the <code>15</code> next to the BioSample category in the Project Data section. This will show us a list of all BioSamples stored in the BioProject</p> <p>We are looking for two samples from this list:</p> <p>Patient_B_unaffected_eye</p> <p>Patient_B_affected_eye</p> <p>So find those two accessions and open their pages in new tabs (or open the links above). We will do the following steps for each sample, but let's start with <code>Patient_B_unaffected_eye</code> first.</p>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-4-find-the-sra-experiments","title":"Step 4 - Find the SRA Experiments","text":"<p>The BioSample page contains all of the metadata associated with where/how the sequence reads were obtained</p> <p>To get the SRA run accession (where the actual sequence data is stored), click on the <code>SRA</code> button in the Related Information tab on the right-hand side of the screen.</p> <p></p>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-5-find-the-sra-runs","title":"Step 5 - Find the SRA Runs","text":"<p>This new SRA page displays the SRA experiment. This is the metadata associated with how the sequence data was generated (e.g. sequencing machine, sequencing type, etc.). At the bottom of the page there is a <code>Runs</code> section which shows the sequence run accession. Click on that link.</p> <p></p>"},{"location":"2023_workshops/metagenomeData/01_searchSRA/#step-6-find-the-run-statistics","title":"Step 6 - Find the Run Statistics","text":"<p>This page is the SRA Run Browser where we can explore details and statistics about the reads themselves (e.g. number of bases, GC content, quality scores, and links back to the parent categories like BioSample and BioProject). This is also where we can see the results from the STAT analysis done on the reads.</p> <p>To see the STAT results, click the <code>Analysis</code> tab near the top of the page</p> <p></p> <p>It is possible to click on the \"+\" signs within the taxonomic tree to expand the view to show lower taxonomic levels.</p> <p></p> <p>For example, clicking on the plus sign next to Alphaprotobacteria expands the list.</p> <p></p>"},{"location":"2023_workshops/metagenomeData/02_stat/","title":"Objective 2 - Exploring Taxonomic Composition of SRA reads using STAT","text":""},{"location":"2023_workshops/metagenomeData/02_stat/#objective-goals","title":"Objective Goals","text":"<ol> <li>Run the SRA Taxonomy Analysis Tool (STAT) on our two case study samples</li> <li>Identify abundant species in each sample</li> </ol>"},{"location":"2023_workshops/metagenomeData/02_stat/#note-1-","title":"NOTE 1 -","text":"<ul> <li>Krona and its supporting software has been preinstalled on our cluster. For more information on how to install Krona on your own machine and how the tool works, please see the documetnation here: https://github.com/marbl/Krona/wiki/KronaTools</li> </ul>"},{"location":"2023_workshops/metagenomeData/02_stat/#note-2-","title":"NOTE 2 -","text":"<ul> <li>Currently the only programmatic way to get the STAT data for visualization from SRA is in the cloud. Cloud data access is outside the scope of this workshop so I have provided the data for use today. For information on accessing that data see here: https://www.ncbi.nlm.nih.gov/sra/docs/sra-cloud/</li> </ul>"},{"location":"2023_workshops/metagenomeData/02_stat/#background-sra-and-stat","title":"Background - SRA and STAT","text":"<p>On our website, the Analysis page gives a broad overview of the top taxonomic hits for this accession thanks to a program designed by the SRA team called STAT which uses kmers.</p> <p></p> <ul> <li>STAT characterizes taxonomic distribution of reads in every SRA submission.</li> <li>Usually measured as a % of reads within a run</li> <li>Reads may be mapped to multiple taxa. So, reads are usually assigned to the lowest taxonomic group<ul> <li>e.g., two species share a genus so the read is assigned to the genus </li> </ul> </li> <li>STAT is relatively conservative in their taxonomic assignment and loses sensitivity as you increase the taxonomic breadth</li> </ul>"},{"location":"2023_workshops/metagenomeData/02_stat/#note-3-","title":"NOTE 3 -","text":"<p>STAT contains more conservative in taxonomic assignment assignment (in comparison to programs like Kraken2, which we will discuss in a bit)</p> <p>However, the data can be a bit awkward for us to navigate on the web when dealing with metagenomic data. Instead, we can employ another tool called Krona to visualize this data in a fun and interactive way.</p>"},{"location":"2023_workshops/metagenomeData/02_stat/#step-1-make-a-krona-plot","title":"Step 1 - Make a Krona Plot","text":"<p>Run the following block of code to recreate the Krona chart for the <code>Patient_B_unaffected_eye</code> sample using the STAT data downloaded from NCBI:</p> <pre><code>!/cluster/tufts/bio/tools/conda_envs/kraken/2.1.2/bin/ktImportTaxonomy \\\n-t 1 -m 3 -k -i \\\n-o ./results/ERR4836973_unaffected_stat.html -q 2 -t 1 -s 3 ./data/ERR4836973_unaffected_stat.txt\n</code></pre>"},{"location":"2023_workshops/metagenomeData/02_stat/#step-2-explore-krona","title":"Step 2 - Explore Krona!","text":"<p>The following code block will open <code>B_unaffected_stat.html</code>:</p> <ul> <li>This Krona pie-chart is organized so that higher taxonomic groups are in the center of the chart. </li> <li>As you move out towards the edge of the pie-chart, the taxonomic classification will become more specific. </li> <li>You can also click on non-species clades to zoom in on that region of the pie-chart.</li> </ul>"},{"location":"2023_workshops/metagenomeData/02_stat/#try-this","title":"Try this:","text":"<p>Try finding and clicking on <code>Firmicutes</code> to see the focused list of species present.</p> <ol> <li> <p>Use the center of the piechart to navigate back to higher-order clades. If you want the original pie-chart, click on the sample ID in the middle of the pie-chart.</p> </li> <li> <p>Each segment of the Krona pie chart has a <code>Magnitude</code> that can be seen in the upper-right corner of the screen. This count is relative to the tool that generated it. </p> <p>For STAT results the <code>Magnitude</code> is the total number of Kmer hits found in the sample for a given SRA sample.</p> </li> <li> <p>Right click on the highlighted \"Taxon:\" number and open a new tab or window to find information about the classification.</p> </li> </ol>"},{"location":"2023_workshops/metagenomeData/02_stat/#unaffected-eye-stat-krona-plot","title":"Unaffected Eye - STAT Krona Plot","text":"<pre><code>from IPython.display import IFrame\nIFrame(src='results/ERR4836973_unaffected_stat.html', width=900, height=600)\n</code></pre>"},{"location":"2023_workshops/metagenomeData/02_stat/#step-3-understand-the-sample","title":"Step 3 - Understand the Sample","text":"<p>Use the navigation skills you've learned for Krona to answer the following questions about this sample. Use the Jupyter Notebook cell below this to write your answers so you can refer back to it later!</p>"},{"location":"2023_workshops/metagenomeData/02_stat/#unaffected-eye-questions","title":"Unaffected Eye Questions","text":"<ol> <li> <p>What is the total <code>Magnitude</code> for the entire unaffected eye sample?</p> </li> <li> <p>What is the total <code>Magnitude</code> for just the Bacteria in the sample?</p> </li> <li> <p>There are three species commonly associated with microbial keratitis. Can you find any of them in the sample? If so, what is their Magnitude?</p> <ul> <li>Pseudomonas aeruginosa</li> <li>Staphylococcus aureus</li> <li>Bacillus subtilis</li> </ul> </li> </ol>"},{"location":"2023_workshops/metagenomeData/02_stat/#hint-the-search-box-will-help-you-to-locate-these-species","title":"Hint: the search box will help you to locate these species","text":""},{"location":"2023_workshops/metagenomeData/02_stat/#step-4-do-it-again-for-the-affected-eye","title":"Step 4 - Do it again for the affected eye!","text":"<p>Run the following code block to get a local version of the Krona graph for Patient B's affected eye. After this code runs, open the file <code>B_affected_stat.html</code> and answer the same questions for this sample:</p> <pre><code>!/cluster/tufts/bio/tools/conda_envs/kraken/2.1.2/bin/ktImportTaxonomy \\\n-t 1 -m 3 -k -i \\\n-o ./results/ERR4836970_affected_stat.html -q 2 -t 1 -s 3 ./data/ERR4836970_affected_stat.txt\n</code></pre> <p>NOTE: If you see an error that looks like this, don't worry. This error is caused by the mapping file from NCBI being slightly out of date.</p>"},{"location":"2023_workshops/metagenomeData/02_stat/#affected-eye-stat-krona-plot","title":"Affected Eye - STAT Krona Plot","text":"<pre><code>from IPython.display import IFrame\nIFrame(src='results/ERR4836970_affected_stat.html', width=900, height=600)\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/","title":"Objective 3 - Generate Taxonomy from Raw Reads with Kraken2","text":""},{"location":"2023_workshops/metagenomeData/03_kraken/#objective-goals","title":"Objective Goals","text":"<ol> <li>Run Kraken to assign SRA taxonomy against an NCBI database</li> <li>Visualize the Kraken results using Krona</li> </ol>"},{"location":"2023_workshops/metagenomeData/03_kraken/#kraken2","title":"Kraken2","text":"<ul> <li>Kraken2 is a taxonomic classification system using exact k-mer matches, like STAT.</li> <li>This k-mer based approach has very fast classification speeds with high accuracy.</li> <li>This approach differs from homology based approaches that try to match sequences to each other and score them based on the number of mismatches, deletions and inserts. </li> <li>Kraken uses the entire kmer composition of the sequence to match it to a database where reference sequences have been broken down into kmer \u201chashes\u201d or a database of what the kmers look like for a particular organism.</li> </ul>"},{"location":"2023_workshops/metagenomeData/03_kraken/#how-does-it-work","title":"How does it work?","text":"<ul> <li> <p>To classify a sequence, each k-mer in the sequence is mapped to the lowest common ancestor (LCA) of the genomes that contain that k-mer in a database. </p> </li> <li> <p>The taxa associated with the sequence\u2019s k-mers, as well as the taxa\u2019s ancestors, form a pruned subtree of the general taxonomy tree, which is used for classification. </p> </li> <li> <p>In the classification tree, each node has a weight equal to the number of k-mers in the sequence associated with the node\u2019s taxon. </p> </li> <li> <p>Each root-to-leaf (RTL) path in the classification tree is scored by adding all weights in the path, and the maximal RTL path in the classification tree is the classification path (nodes highlighted in yellow). </p> </li> <li> <p>The leaf of this classification path (the orange, leftmost leaf in the classification tree) is the classification used for the query sequence.</p> </li> </ul> <p>At the core of Kraken is a database that contains records consisting of a k-mer and the LCA of all organisms whose genomes contain that k-mer. This database, built using a user-specified library of genomes, allows a quick lookup of the most specific node in the taxonomic tree that is associated with a given k-mer. </p> <p>Wood, D.E., Salzberg, S.L. Kraken: ultrafast metagenomic sequence classification using exact alignments. Genome Biol 15, R46 (2014). https://doi.org/10.1186/gb-2014-15-3-r46</p>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-1-run-kraken2-on-the-affected-eye-sample","title":"Step 1 - Run Kraken2 On The Affected Eye Sample","text":"<ul> <li>Run the code block below to run Kraken2 on our affected eye sample</li> </ul> <pre><code>!/cluster/tufts/bio/tools/conda_envs/kraken/2.1.2/bin/kraken2 \\\n--use-names --threads 4 \\\n--db /cluster/tufts/bio/tools/training/metagenomics/kraken_virus \\\n--report results/ERR4836970_affected_kraken_report.txt --quick data/ERR4836970_affected.fastq \\\n--classified-out results/ERR4836970_affected_classified_sequences.tsv &gt; results/sequences.kraken\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-2-investigate-the-affected-eye-sequence-report","title":"Step 2 - Investigate the Affected Eye Sequence Report","text":"<ul> <li>We can open up the sequence report and scroll through the findings. Let's look at the first 10 lines using the bash command \"head\" with the parameter \"-n\" which specifies the number of lines to look at.</li> </ul> <p>The columns from left to right are:</p> Column Number Description 1 Percentage of fragments covered by the clade rooted at this taxon 2 Number of fragments covered by the clade rooted at this taxon 3 Number of fragments assigned directly to this taxon 4 A rank code, indicating (U)nclassified, (R)oot, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. Taxa that are not at any of these 10 ranks have a rank code that is formed by using the rank code of the closest ancestor rank with a number indicating the distance from that rank. E.g., \"G2\" is a rank code indicating a taxon is between genus and species and the grandparent taxon is at the genus rank. 5 NCBI taxonomic ID number 6 Indented scientific name (indented using space, according to the tree structure specified by the taxonomy.) <ul> <li>Another option <code>--use-mpa-style</code> can be used in conjunction with <code>--report</code>. This option provides output in a format similar to MetaPhlAn's output. </li> </ul> <pre><code>!head -n 10 results/ERR4836970_affected_kraken_report.txt\n</code></pre> <p>output</p> <p>45.77   189824  189824  U   0   unclassified 54.23   224908  19  R   1   root 54.22   224880  4347    R1  131567    cellular organisms 33.50   138950  111 D   2759        Eukaryota 33.43   138647  0   D1  33154         Opisthokonta 33.43   138647  0   K   33208           Metazoa 33.43   138647  0   K1  6072              Eumetazoa 33.43   138647  0   K2  33213               Bilateria 33.43   138647  0   K3  33511                 Deuterostomia 33.43   138647  0   P   7711                    Chordata</p>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-3-create-the-affected-eye-krona-plot","title":"Step 3 - Create the Affected Eye Krona Plot","text":"<ul> <li>Run the code block below to create a Krona plot for our affected eye sample</li> </ul> <pre><code>!/cluster/tufts/bio/tools/Krona/KronaTools/bin/ktImportTaxonomy \\\n-t 5 -m 3 -k -i \\\n-o results/ERR4836970_affected_kraken.html \\\nresults/ERR4836970_affected_kraken_report.txt\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#affected-eye-kraken2-krona-plot","title":"Affected Eye Kraken2 Krona Plot","text":"<pre><code>from IPython.display import IFrame\ndisplay(IFrame(src='results/ERR4836970_affected_kraken.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-4-run-kraken2-on-the-unaffected-eye-sample","title":"Step 4 - Run Kraken2 On The Unaffected Eye Sample","text":"<ul> <li>Run the code block below to run Kraken2 on our unaffected eye sample</li> </ul> <pre><code>!/cluster/tufts/bio/tools/conda_envs/kraken/2.1.2/bin/kraken2 \\\n--use-names --threads 4 \\\n--db /cluster/tufts/bio/tools/training/metagenomics/kraken_virus \\\n--report results/ERR4836973_unaffected_kraken_report.txt --quick data/ERR4836973_unaffected.fastq \\\n--classified-out results/ERR4836973_unaffected_classified_sequences.tsv &gt; results/sequences.kraken\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-5-investigate-the-unaffected-eye-sequence-report","title":"Step 5 - Investigate the Unaffected Eye Sequence Report","text":"<pre><code>!head -n 10 results/ERR4836973_unaffected_kraken_report.txt\n</code></pre> <p>output</p> <p>18.05   250 250 U   0   unclassified 81.95   1135    0   R   1   root 81.95   1135    104 R1  131567    cellular organisms 73.94   1024    488 D   2       Bacteria 28.30   392 66  P   1224          Proteobacteria 11.12   154 35  C   1236            Gammaproteobacteria 5.85    81  25  O   91347             Enterobacterales 3.54    49  10  F   543             Enterobacteriaceae 1.30    18  0   G   561               Escherichia 1.30    18  17  S   562                 Escherichia coli</p>"},{"location":"2023_workshops/metagenomeData/03_kraken/#step-6-create-the-unaffected-eye-krona-plot","title":"Step 6 - Create the Unaffected Eye Krona Plot","text":"<pre><code>!/cluster/tufts/bio/tools/Krona/KronaTools/bin/ktImportTaxonomy \\\n-t 5 -m 3 -k -i \\\n-o results/ERR4836973_unaffected_kraken.html \\\nresults/ERR4836973_unaffected_kraken_report.txt\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#unaffected-eye-kraken2-krona-plot","title":"Unaffected Eye Kraken2 Krona Plot","text":"<pre><code>from IPython.display import IFrame\ndisplay(IFrame(src='results/ERR4836973_unaffected_kraken.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/03_kraken/#kraken2-questions","title":"Kraken2 Questions:","text":"<ol> <li> <p>Which sample, the affected eye or unaffected eye had a higher percentage of classified sequences?</p> </li> <li> <p>Which sample, the affected eye or unaffected eye had a higher percentage of bacteria?</p> </li> <li> <p>Does anyone spot a potential non-bacterial taxa in the affected eye?</p> </li> </ol>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/","title":"Objective 4 - Compare STAT and Kraken2 output","text":""},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#objective-goals","title":"Objective Goals:","text":"<ol> <li>Compare species distribution from Kraken2 to preliminary list gathered from STAT</li> <li>Understand the influence of the database when classifying taxonomy</li> </ol>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#step-1-compare-unaffected-eye-stat-v-kraken2","title":"Step 1 - Compare Unaffected Eye STAT v. Kraken2","text":""},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#unaffected-eye-stat-krona-plot","title":"Unaffected Eye - STAT Krona Plot","text":"<pre><code>from IPython.display import IFrame\ndisplay(IFrame(src='results/ERR4836973_unaffected_stat.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#unaffected-eye-kraken2-krona-plot","title":"Unaffected Eye - Kraken2 Krona Plot","text":"<pre><code>from IPython.display import IFrame\ndisplay(IFrame(src='results/ERR4836973_unaffected_kraken.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#affected-eye-stat-krona-plot","title":"Affected Eye - STAT Krona Plot","text":"<pre><code>from IPython.display import HTML\ndisplay(IFrame(src='results/ERR4836970_affected_stat.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#affected-eye-kraken2-krona-plot","title":"Affected Eye - Kraken2 Krona Plot","text":"<pre><code>from IPython.display import HTML\ndisplay(IFrame(src='results/ERR4836970_affected_kraken.html', width=900, height=600))\n</code></pre>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#conclusions","title":"Conclusions","text":"<p>This concludes our look on finding and analyzing metagenomic data with NCBI! Let's recap some of the things we covered today:</p> <ol> <li>Metagenomic data is stored primarily in the Sequence Read Archive (SRA) database due to the size and nature of the data</li> <li>SRA data is organized into 4 hierarchical layers - BioProject, BioSample, SRA Experiment, and SRA Run. Sequencing data is stored in the SRA run layer.</li> <li>STAT is a tool designed by SRA developers to quickly catalog the taxonomic abundance/distribution of every SRA accession at NCBI using a kmer comparison approach where each kmer match is one count towards the taxa</li> <li>STAT and Kraken2 use a kmer based approach which uses the frequency of kmers in a read to create a hash fingerprint which can help classify a read as counting towards a taxa. However, STAT</li> </ol>"},{"location":"2023_workshops/metagenomeData/04_compareStatKraken/#homework-questions","title":"Homework Questions","text":"<ol> <li> <p>How similar are the species predictions? In particular, how well is the suspected case study culprit, Bacillus subtilis represented in each method? What is the Bacillus subtilis magnitude in each eye and each method?</p> </li> <li> <p>Does STAT accomplish its goal of being a good \"first pass\" for an SRA sample to detect contamination and general taxonomic composition?</p> </li> <li> <p>Ultimately, how did each tool do at answering the questions of our case study?</p> <ul> <li>\"Is the taxonomic distribution of each \"cornea\" microbiome\" different between each eye?\"</li> <li>\"Do the taxonomic distributions of the eyes match our expectations for healthy and infected eyes?\"</li> </ul> </li> </ol>"},{"location":"2023_workshops/trajectoryAnalysis/00_background/","title":"Introduction","text":""},{"location":"2023_workshops/trajectoryAnalysis/00_background/#introduction-to-trajectory-analysis","title":"Introduction To Trajectory Analysis","text":"<p>Content developed by Data Intensive Studies Center &amp; TTS Research Technology at Tufts University</p> <ul> <li>Rebecca Batorsky, PhD, Data Scientist I <sup>1</sup></li> <li>Albert Tai, PhD, Research Assistant Professor<sup>3</sup> </li> <li>Jason Laird, MSc, Bioinformatics Scientist<sup>2</sup></li> </ul> <ol> <li>Data Intensive Study Center, Tufts University</li> <li>Research Technology, Tufts Technology Services, Tufts University</li> <li>Immunology, Tufts School of Medicine</li> </ol>"},{"location":"2023_workshops/trajectoryAnalysis/00_background/#schedule","title":"Schedule","text":"<p>Lecture</p> <ol> <li>Introduction to Single cell RNA-Seq</li> <li>Introduction to Trajectory Analysis</li> </ol> <p>Hands On Activity</p> <ol> <li>Setup and Monocle3 Objects </li> <li>Monocle Workflow and Trajectory Analysis</li> <li>Pseudotime and Differential Expression</li> </ol>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/","title":"Setup and Monocle3 Objects","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster<ul> <li>Note if you signed up for the Introduction to Single-Cell RNA-Seq Time Series and Trajectory Analysis workshop this will have been already taken care of for you!</li> </ul> </li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p>OnDemand Layout</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>5</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>16GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshops</code><ul> <li>NOTE: This reservation closed on April 26th 2023, use <code>Default</code> if running through the materials after that date.</li> </ul> </li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Launch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p> Are you connected to RStudio? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#data-scripts","title":"Data &amp; Scripts","text":"<p>To copy over the data and scripts we will need for the workshop into our home directory, enter the following command into the console:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/trajectory_analysis\",to=\"~/\", recursive = TRUE)\n</code></pre>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#project-setup","title":"Project Setup","text":"<p>Now we are going to use this folder to create a new R project. R projects are great for managing analyses in a portable, self-contained folder. To create an R project from within our <code>trajectory_analysis</code> directory we will:</p> <ul> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>Existing Directory</code></li> <li>Browse for the <code>trajectory_analysis</code> folder</li> <li>Click <code>Create Project</code></li> </ul> <p>Let's navigate to our project in our home directory and open up our workshop script:</p> <ul> <li>Click on the <code>Files</code> tab in the lower right hand Rstudio pane</li> <li>Click on the <code>scripts</code> folder</li> <li>Click on the <code>trajectory_analysis.Rmd</code> script</li> </ul>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#todays-data","title":"Today's Data","text":"<p>Today we will be working with data from  Paulson et al. 2022 which found cell-type-specific neurodevelopmental abnormalities that were shared across ASD risk genes. To this end they leveraged organoid single-cell RNA-seq data to investigate these abnormalities:</p> <p>Paulson et al. 2022</p> <p></p>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#monocle3-cell-data-objects","title":"Monocle3 Cell Data Objects","text":"<ul> <li>We will be working with single-cell RNA-seq data in R today. Today, we will be performing trajectory analysis using the R package Monocle3. Monocle3 stores single-cell RNA-seq data as a cell data set object, which has the following structure:</li> </ul> <p>Monocle3's Cell Data Set Object</p> <p></p>"},{"location":"2023_workshops/trajectoryAnalysis/01_setup_and_monocle/#loading-libraries-and-data","title":"Loading Libraries and Data","text":"<p>Before we analyze our single-cell RNA-Seq data we will need to load the libraries needed for our analysis:</p> <pre><code># set the libPath and load the libraries\nLIB='/cluster/tufts/hpc/tools/R/4.0.0/'\n.libPaths(c(\"\",LIB))\nlibrary(monocle3)\nlibrary(tidyverse)\n</code></pre> <ul> <li>Let's start by loading the input matrices we need to create our cell data set object!</li> </ul> <pre><code># read in cells by gene count matrix\ncounts &lt;- read.csv(\"/cluster/tufts/bio/data/projects/2023_02_time_series_scrnaseq/rds/SUV420H1_Mito210_d35_counts.csv\", \n                   row.names=1,\n                   check.names = F)\n\nhead(counts)\n</code></pre> <p>output</p> <pre><code>              1_AAAGGTACACAGCTGC 1_AAAGGTATCTGCCTGT 1_AACAACCCACACTGGC\nFO538757.2                     0                  1                  0\nAP006222.2                     0                  0                  0\nRP11-206L10.9                  0                  0                  0\nLINC00115                      0                  0                  0\n</code></pre> <pre><code># read in sample meta data\nmeta &lt;- read.csv( \"/cluster/tufts/bio/data/projects/2023_02_time_series_scrnaseq/rds/SUV420H1_Mito210_d35_meta.csv\", row.names=1)\n\nhead(meta)\n</code></pre> <p>output</p> <pre><code>                   treat         dataset            CellType\n1_AAAGGTACACAGCTGC    wt SUV_Mito210_d35 Cycling Progenitors\n1_AAAGGTATCTGCCTGT    wt SUV_Mito210_d35                 aRG\n1_AACAACCCACACTGGC    wt SUV_Mito210_d35 Cycling Progenitors\n1_AACAAGATCGAAGCAG    wt SUV_Mito210_d35                 aRG\n1_AACAGGGAGGACAGCT    wt SUV_Mito210_d35                 aRG\n1_AACCTGAGTATACGGG    wt SUV_Mito210_d35 Cycling Progenitors\n</code></pre> <pre><code># can optionally list more annotation of genes\ngene_meta &lt;- read.csv(\"/cluster/tufts/bio/data/projects/2023_02_time_series_scrnaseq/rds/SUV420H1_Mito210_d35_genemeta.csv\",\n                      row.names=1)\nhead(gene_meta)\n</code></pre> <p>output</p> <pre><code>              gene_short_name\nFO538757.2         FO538757.2\nAP006222.2         AP006222.2\nRP11-206L10.9   RP11-206L10.9\nLINC00115           LINC00115\nFAM41C                 FAM41C\nRP11-54O7.1       RP11-54O7.1\n</code></pre> <ul> <li>Now that we have loaded our gene expression matrix, our meta data, and our gene metadata, we can use these to create the cell data set object!</li> </ul> <pre><code># note the colnames(counts) must match rownames(meta)\n\ncds &lt;- new_cell_data_set(expression_data = as.matrix(counts),\n                         cell_metadata = meta, \n                         gene_metadata = gene_meta)\n\n# let's take a look at our new object!\ncds\n</code></pre> <p>output</p> <pre><code>class: cell_data_set \ndim: 12962 4000 \nmetadata(1): cds_version\nassays(1): counts\nrownames(12962): FO538757.2 AP006222.2 ... AC004556.1 AC240274.1\nrowData names(0):\ncolnames(4000): 1_AAAGGTACACAGCTGC 1_AAAGGTATCTGCCTGT ... 6_TTTGACTGTACCATAC 6_TTTGGTTGTTACGTAC\ncolData names(4): treat dataset CellType Size_Factor\nreducedDimNames(0):\naltExpNames(0):\n</code></pre> <ul> <li>Here we will highlight that we have 12962 rows and 4000 columns, our rownames are gene names, our column names are the cell names, we have one assay (<code>counts</code>), we have 4 columns of meta data under <code>colData</code>, and we have no dimension reductions under <code>reducedDimNames</code>.</li> <li>Let's investage a few helpful functions that can help access these data:</li> </ul> <pre><code># access the gene names\nrownames(cds)[1:5]\n</code></pre> <p>output</p> <pre><code>[1] \"FO538757.2\"    \"AP006222.2\"    \"RP11-206L10.9\" \"LINC00115\"     \"FAM41C\" \n</code></pre> <pre><code># access the cell names\ncolnames(cds)[1:5]\n</code></pre> <p>output</p> <pre><code>[1] \"1_AAAGGTACACAGCTGC\" \"1_AAAGGTATCTGCCTGT\" \"1_AACAACCCACACTGGC\" \"1_AACAAGATCGAAGCAG\" \"1_AACAGGGAGGACAGCT\"\n</code></pre> <pre><code># access the feature data\nhead(rowData(cds))\n</code></pre> <p>output (Notice we have no gene meta data)</p> <pre><code>DataFrame with 6 rows and 0 columns\n</code></pre> <pre><code># access the meta data\nhead(colData(cds))\n</code></pre> <p>output</p> <pre><code>DataFrame with 6 rows and 4 columns\n                         treat         dataset            CellType Size_Factor\n                   &lt;character&gt;     &lt;character&gt;         &lt;character&gt;   &lt;numeric&gt;\n1_AAAGGTACACAGCTGC          wt SUV_Mito210_d35 Cycling Progenitors    1.074965\n1_AAAGGTATCTGCCTGT          wt SUV_Mito210_d35                 aRG    0.584379\n1_AACAACCCACACTGGC          wt SUV_Mito210_d35 Cycling Progenitors    1.248093\n1_AACAAGATCGAAGCAG          wt SUV_Mito210_d35                 aRG    1.115985\n1_AACAGGGAGGACAGCT          wt SUV_Mito210_d35                 aRG    0.682049\n1_AACCTGAGTATACGGG          wt SUV_Mito210_d35 Cycling Progenitors    0.997981\n</code></pre> <pre><code># access the assay data\nhead(assay(cds)[,1:3])\n</code></pre> <p>output</p> <pre><code>6 x 3 sparse Matrix of class \"dgCMatrix\"\n              1_AAAGGTACACAGCTGC 1_AAAGGTATCTGCCTGT 1_AACAACCCACACTGGC\nFO538757.2                     .                  1                  .\nAP006222.2                     .                  .                  .\nRP11-206L10.9                  .                  .                  .\nLINC00115                      .                  .                  .\nFAM41C                         .                  .                  .\nRP11-54O7.1                    .                  .                  .\n</code></pre> What is a dgCMatrix? <p>a dgCMatrix is a type of sparse, compressed, column-oriented numeric matrix where non-zero elements in each column are sorted into increasing row order. Essentially, this is a way of storing matrices with less memory which is important for single-cell RNA-seq data given so many matrices are generated.</p>"},{"location":"2023_workshops/trajectoryAnalysis/02_monocle_setup_and_trajectory_analysis/","title":"Monocle3 Workflow and Trajectory Analysis","text":""},{"location":"2023_workshops/trajectoryAnalysis/02_monocle_setup_and_trajectory_analysis/#running-monocle3","title":"Running Monocle3","text":"<p>There are typically four main commands when running Monocle3:</p> <p>Monocle3 Workflow</p> <p></p> <ul> <li><code>preprocess_cds()</code>: normalizes the data by log and size factor to address depth differences and calculates a lower dimensional space that will be used as the input for further dimensionality reduction like tSNE and UMAP.</li> </ul> <code>preprocess_cds()</code> options <ul> <li><code>method</code> : initial method of dimention reduction - options are  \"PCA\" and \"LSI\"</li> <li><code>num_dim</code> : the dimensionality of the reduced space - default is 50</li> <li><code>norm_method</code> : methods of normalizing the expression data - options are \"log\", \"size_only\", \"none\"</li> <li><code>use_genes</code> : when this option is not NULL, only the list of genes provided will be used for dimension reduction - default is NULL</li> <li><code>pseudo_count</code> : amount to increase expression values before normalization and dimensionality reduction. If the value is NULL a pseudo count of 1 is added - default is NULL</li> <li><code>scaling</code> : this will scale each gene before running trajectory reconstruction if set to TRUE - default is TRUE</li> </ul> <ul> <li><code>reduce_dimension()</code>: reduces the dimensionality of the data using algorithms like UMAP or tSNE. Trajectories will be calculated through this space.</li> </ul> <code>reduce_dimension()</code> options <ul> <li><code>max_components</code> : the dimensionality of the reduced space - default is 2</li> <li><code>reduction_method</code> : the method used for dimension reduction - options are \"UMAP\", \"tSNE\", \"PCA\", \"LSI\", and \"Aligned\"</li> <li><code>preprocess_method</code> : what preprocess method was used to initially reduce the dimensionality of the gene expression data - default is LSI</li> <li><code>umap.metric</code> : the distance metrice used when calculating the UMAP - default is \"cosine\"</li> <li><code>umap.min_dist</code> : the minimum distance to be input into the UMAP function - default is 0.1</li> <li><code>umap.n_neighbors</code> : the number of neighbors to use during kNN graph construction - default is 15L</li> <li><code>umap.fast_sgd</code> : option to use Stochastic Gradient Descent when caclulating the UMAP to speed up the computation - default is FALSE</li> <li><code>umap.nn_method</code> : the nearest neighbor method to be used by UMAP - default is \"annoy\"</li> </ul> <ul> <li><code>cluster_cells()</code>: clusters the cells using Louvain/Leiden community detection, and returns a cell_data_set with internally stored cluster assignments. These cluster assignments can then be assigned to cell types given that cells in a cluster are likely to be the same cell type as cells of the same type have similar gene expression patterns.</li> </ul> <code>cluster_cells()</code> options <ul> <li><code>reduction_method</code> : The dimensionality reduction method upon which to base clustering - options are \"UMAP\", \"tSNE\", \"PCA\", \"LSI\", and \"Aligned\"</li> <li>k : the number of nearest neighbors to use when creating the k nearest neighbor graph for Louvain/Leiden clustering. k is related to the resolution of the clustering result, a bigger k will result in lower resolution and vice versa - default is 20</li> <li><code>cluster_method</code> :  the clustering method to use, if \"louvain\" is used the resolution paramter is ignored - default is \"leiden\" </li> <li><code>num_iter</code> : number of iterations used for Louvain/Leiden clustering - defualt is 2</li> <li><code>partition_qval</code> : q-value cutoff to determine when to partition - default is 0.05</li> <li><code>weight</code> : to determine whether or not to use Jaccard coefficients for two nearest neighbors (based on the overlapping of their kNN) as the weight used for Louvain clustering - default is FALSE</li> <li><code>resolution</code> : controls the resolution of clustering. If NULL, the parameter is determined automatically - default is NULL</li> <li><code>random_seed</code> : used by the random number generator in louvain-igraph package. This argument will be ignored if num_iter is larger than 1 - default is NULL</li> </ul> <ul> <li><code>learn_graph()</code>: constructs the trajectory through clusters in a lower dimensional space to \"learn the sequence of gene expression changes each cell must go through as part of a dynamic biological process\"</li> </ul> <code>learn_graph()</code> options <ul> <li><code>use_partition</code> : determines whether to use partitions calculated during cluster_cells and therefore to learn disjoint graph in each partition. When use_partition = FALSE, a single graph is learned across all partitions - default is TRUE</li> <li><code>close_loop</code> : determines whether or not to perform an additional run of loop closing after estimating the principal graphs to identify potential loop structure in the data space - default is TRUE</li> <li><code>learn_graph_control</code> = a list of control parameters to be passed to the reversed graph embedding function - default is NULL</li> </ul> <p>Let's start with the pre-processing step:</p> <pre><code># preprocess_cds does both 1) normalization and 2) preliminary dimension reduction.  By default 1) gene expression count for each cell is divided by the total counts of that cell, multiplied by a scale factor, and log-transformed. This is done  to address differences in sequencing depths for different cells. 2) PCA is used to calculate a lower dimensional space that will be used as the input for downstream steps like UMAP visualization. \ncds &lt;- preprocess_cds(cds, \n                      num_dim = 30) \n\n# look at the percentage of variance explained by our principal components\nplot_pc_variance_explained(cds)\n</code></pre> <p></p> <p>Here we see that the first few PCA components account for about half the variance. Note that adding components helps with explaining the variation in your data, but comes at the cost of increased computational time. Given this is a subsampled data set, this is less of a concern. Now, let's run the reduce dimentions step!</p> <pre><code># Note here we are using the results of PCA dimension reduction in the last step and visualizing the results in 2 dimensions.\n# umap.fast_sgd=FALSE and cores = 1 are needed for reproducible results \ncds &lt;- reduce_dimension(cds,\n                        preprocess_method = 'PCA',\n                        umap.fast_sgd=FALSE, \n                        cores = 1)\n# let's take another look at our cell data set object\ncds\n</code></pre> <p>output</p> <pre><code>class: cell_data_set \ndim: 12962 4000 \nmetadata(2): cds_version citations\nassays(1): counts\nrownames(12962): FO538757.2 AP006222.2 ... AC004556.1 AC240274.1\nrowData names(1): gene_short_name\ncolnames(4000): 1_AAAGGTACACAGCTGC 1_AAAGGTATCTGCCTGT ...\n6_TTTGACTGTACCATAC 6_TTTGGTTGTTACGTAC\ncolData names(4): treat dataset CellType Size_Factor\nreducedDimNames(2): PCA UMAP\naltExpNames(0)\n</code></pre> <p>Here we would like to highlight that after running our pipeline, we now have two <code>reducedDimNames</code> slots in our object! We can access them with the <code>reducedDims</code> function:</p> <pre><code># let's examine our new `reducedDimNames` slot!\nhead(reducedDims(cds)$UMAP)\n</code></pre> <p>output</p> <pre><code>                        [,1]        [,2]\n1_AAAGGTACACAGCTGC -4.371685 -1.90491864\n1_AAAGGTATCTGCCTGT -9.110986  1.60710314\n1_AACAACCCACACTGGC -8.257574 -0.05704084\n1_AACAAGATCGAAGCAG -6.810462  2.03861049\n1_AACAGGGAGGACAGCT -9.367511  1.91848877\n1_AACCTGAGTATACGGG -9.553589  0.20396432\n</code></pre> <p>To understand why we only ran through the first three steps, we should examine how our cells are distributed in our dimension reduced UMAP plot:</p> <pre><code># view the cell types in our UMAP plot\nplot_cells(cds,\n           color_cells_by = \"CellType\", \n           show_trajectory_graph = FALSE,\n           group_label_size = 3,\n           cell_size = 0.5)\n</code></pre> <p></p> <p>Grouping cells into clusters is an important step in identifying the cell types represented in your data. Monocle uses a technique called community detection to group cells into cluster and partitions.</p> <pre><code># Grouping cells into clusters is an important step in identifying the cell types represented in your data. Monocle uses a technique called community detection to group cells into cluster and partitions.\ncds &lt;- cluster_cells(cds, \n                     k = 20,\n                     partition_qval = 0.05)\n\nplot_cells(cds,\n           color_cells_by = \"partition\", \n           show_trajectory_graph = FALSE,\n           group_label_size = 3,\n           cell_size = 0.5)\n</code></pre> <p></p>"},{"location":"2023_workshops/trajectoryAnalysis/02_monocle_setup_and_trajectory_analysis/#subsetting-our-data","title":"Subsetting Our Data","text":"<p>Above we can see that we have multiple clusters (usually representing our cells) and multiple partitions (usually representing groups of different cells). When Monocle3 calculates it's trajectory it will typically do so through one of these partitions. So we will subset our data to just grab the partition that contains the Cycling Progenitors, Newborn PNs, and Newborn DL PNs. By doing this we can assess how gene expression changes during cell differentiation from a Cycling Progenitors to a Newborn DL PN:</p> <pre><code># when we examine a trajectory in monocle3 it is useful to look at one\n# partition as you are examining how gene expression changes between clusters\n# in some group\ncds_2 = choose_cells(cds)\n</code></pre> <p>When we go to subset our cells we will choose the following cells:</p> <p></p> <p>Now that we subset our cells, let's examine how our UMAP has changed!</p> <pre><code># check the subset of cells\nplot_cells(cds_2, \n           color_cells_by = \"partition\",\n           show_trajectory_graph = FALSE,\n           group_label_size = 3,\n           cell_size = 0.5)\n</code></pre> <p></p>"},{"location":"2023_workshops/trajectoryAnalysis/02_monocle_setup_and_trajectory_analysis/#trajectory-analysis","title":"Trajectory Analysis","text":"<p>Now that we have subsampled cells moving from Cycling Progenitors to Newborn DL PNs, we will need to re-run the Monocle3 workflow on our data given that our clustering was done on the larger data set.</p> <pre><code># re-run the monocle3 workflow on our subset data:\n# use the same # of top PCs as used for clustering\ncds_2 &lt;- preprocess_cds(cds_2, \n                        num_dim = 20)\n\n# take another look at the variance explained \nplot_pc_variance_explained(cds_2)\n</code></pre> <p></p> <p>You'll note again that the first few PCA components explain about half the variance. Here we include fewer PCA components to account for the variance as we reduced the number the of total cells we are examining. We can now move on and apply the reduce_dimensions/cluster_cells functions to our data:</p> <pre><code># reduce the dimensions \ncds_2 &lt;- reduce_dimension(cds_2,\n                          preprocess_method = 'PCA',\n                          umap.fast_sgd=FALSE, \n                          cores = 1)\n\n# force few partitions with partition q-value set higher\ncds_2 &lt;- cluster_cells(cds_2, \n                       k = 20,\n                       partition_qval = 1)\n\n# do we only have one partition now?\nplot_cells(cds_2,\n           color_cells_by = \"partition\", \n           show_trajectory_graph = FALSE,\n           group_label_size = 3,\n           cell_size = 0.5)\n</code></pre> <p></p> <p>Note that in the above plot that all of our cells are now in one partition. This time we will calculate trajectories in our data now that we have a subset of cells in which it makes sense to do so. You will note that we specify two options when we \"learn our graph\". One being 'use_partition = FALSE' which we will not use as we have already subset our cell population. The other being 'close_loop = TRUE', which we specify as this allows for the trajectory to form loops. This is important if we want to to identify populations of cells with cyclic behavior.</p> <pre><code># additionally we will run learn_graph to calculate trajectories on our subset!\ncds_2 &lt;- learn_graph(cds_2,\n                     use_partition = FALSE,\n                     close_loop = TRUE)\n\n# now that we have subset out data, re-run our monocle workflow, and calculated \n# trajectories, let's see where we should set our root node!\nplot_cells(cds_2, \n           color_cells_by = \"CellType\",\n           cell_size = 0.5,\n           labels_per_group = 0) \n</code></pre> <p></p> <p>We can see that we have a trajectory now connecting the Cylcing Progenitor cells and the Newborn DL PNs!</p>"},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/","title":"Pseudotime and Differential Expression","text":""},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#pseudotime-and-differential-expression","title":"Pseudotime and Differential Expression","text":""},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#pseudotime","title":"Pseudotime","text":"<p>Monocle introduced the concept of pseudotime which they define as: \"Pseudotime is a measure of how much progress an individual cell has made through a process such as cell differentiation.\" We will assess \"progress\" by a cell's differentiation status. So we will manually chose the starting point to be the cycling progenitors: </p> <pre><code># monocle introduced the concept of pseudotime which they define as:\n# \"Pseudotime is a measure of how much progress an individual cell has made \n# through a process such as cell differentiation.\"\n# we will assess \"progress\" by a cell's differentiation status. So we will\n# manually chose the starting point to be the cycling progenitors\ncds_2 = order_cells(cds_2) \n</code></pre> <p></p> <p>Now let's visualize pseudotime in our UMAP plot to understand what is early pseudotime (low values - dark purple) and late pseudotime (high values - yellow):</p> <pre><code># plot pseudo time after choosing root nodes \n# (defined as the bottom in the cycling progenitors cell type)\nplot_cells(cds_2,\n           show_trajectory_graph = T,\n           color_cells_by = \"pseudotime\",\n           graph_label_size=3,\n           cell_size = 0.5) \n</code></pre> <p></p>"},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#distribution-of-wild-type-and-mutant-cells","title":"Distribution of Wild-Type and Mutant Cells","text":"<p>Paulson et al. 2022 note that there is a difference in the distribution of Wild-Type and Mutant Cells within this UMAP. Let's try coloring our UMAP plot by the treatment (Wild-Type v. Mutant Cells):</p> <pre><code># how are our wild-type and mutant cells distributed in this graph?\nplot_cells(cds_2, \n           color_cells_by = \"treat\", \n           show_trajectory_graph = F, \n           label_cell_groups = F,\n           cell_size = 0.5) \n</code></pre> <p></p> <p>Optional: Pseudotime Ridgeplot View</p> <p>This view can be a bit busy, we can better see the distribution of the number of cells in each condition through pseudotime with a ridge plot like in the paper!</p> <pre><code># isolate meta data\ncds_meta = colData(cds_2)\n\n# add pseudo-time to meta data\ncds_meta$pseudotime = pseudotime(cds_2)\n\n# filter data for where pseudo time is finite\ncds_meta = cds_meta[is.finite(cds_meta$pseudotime),]\n\n# plot pseudo-time for cells that assessed for pseudo-time\nggplot(as.data.frame(cds_meta), \n       aes(x=pseudotime,\n           y=treat)) + \n  ggridges::geom_density_ridges(aes(fill=treat))  \n</code></pre> <p></p> <p>Here we can see that there is a spike in the number of cells in the mutant population at later pseudotime values. </p>"},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#differential-expression","title":"Differential Expression","text":"<p>We will now create a model to to assess how genes vary with pseudotime and treatment. However, before we do that we need to factor our variables of interest!</p> <pre><code># create pseudotime bins for visualization later\ncds_meta$pseudotime_bins &lt;- as.numeric(\n  cut_number(\n    cds_meta$pseudotime,\n    6))\n\n# factor our treatment variable\ncds_meta$treat &lt;- factor(cds_meta$treat, levels=c(\"wt\",\"mut\"))\n\n# swap our meta data back into the cell data set object\ncolData(cds_2) &lt;- cds_meta\n</code></pre> <p>Let's run our model to assess genes that vary due to pseudotime and treatment. We then print the column names and the dimensions of the results data frame. Just note that this will take a bit of time to run!</p> <pre><code># create a model to assess how genes vary with predictors: pseudotime and treatment \ngene_fits &lt;- fit_models(cds_2, \n                        model_formula_str = \"~ pseudotime + treat\",\n                        cores = 1)\n\n# grab the coefficients table\nfit_coefs &lt;- coefficient_table(gene_fits)\n\ncolnames(fit_coefs)\ndim(fit_coefs)\n</code></pre> <p>output</p> <pre><code>[1] 38886    14\n</code></pre> <p>Here you will notice that we have 14 columns pertaining to model information and 38886 rows corresponding to terms in those models.</p>"},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#filter-for-significant-genes","title":"Filter For Significant Genes","text":"<p>Now that we have a data frame with our model results, we can subset it to grab the columns/rows we need. One popular package to do just this in R is dplyr. Let's use dplyr to select the columns we need:</p> <pre><code># now that we have a data frame with our model results, we can subset it to grab\n# the columns/rows we need. One popular package to do just this in R is dplyr\n# Let's use dplyr to select the columns we need:\nfit_coef_filter &lt;- fit_coefs %&gt;% \n  dplyr::select(c(gene_short_name, term, status, estimate, std_err, p_value, q_value))\n\ndim(fit_coef_filter)\n</code></pre> <p>output</p> <pre><code>[1] 38886     7\n</code></pre> <p>You'll note that we now only have 7 columns!</p> <pre><code># we can also use dplyr to filter the rows we need\n# let's use the filter function to remove lines that correspond to the\n# intercept and for genes where the q-value is less than 0.05:\nfit_coef_filter &lt;- fit_coef_filter %&gt;%\n  dplyr:: filter(term != \"(Intercept)\" &amp; q_value&lt;0.05) \n\ndim(fit_coef_filter)\n</code></pre> <p>output</p> <pre><code>[1] 3964    7\n</code></pre> <p>You'll note that we only have 3964 genes!</p> <pre><code># view our results data frame to and sort genes by q-value for \n# both genes that vary due to pseudotime and the treatment\nview(fit_coef_filter)\n</code></pre>"},{"location":"2023_workshops/trajectoryAnalysis/03_pseudotime_and_differential_expression/#plot-genes-that-vary-over-pseudotimetreatment","title":"Plot Genes That Vary Over Pseudotime/Treatment","text":"<pre><code># example of genes that vary over pseudotime \n# NOTE: After plotting RTN1, try plotting another gene!\ncds_subset &lt;- cds_2[rownames(cds_2) %in% c(\"RTN1\"),]\n\n# plot genes that vary over pseudotime\nplot_genes_violin(cds_subset, \n                  group_cells_by=\"pseudotime_bins\",\n                  ncol=2) +  \n  theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> <p>output</p> <p></p> <p>Here you can see that as pseudotime increases so does the expression of RTN1. Let's try out a gene that varies due to the treatment effect:</p> <pre><code># example of genes that vary as a result of the treatment\n# NOTE: After plotting MT-ND3, try plotting another gene!\ncds_subset &lt;- cds_2[rownames(cds_2) %in% c(\"MT-ND3\"),]\n\n# plot genes that vary as a result of the treatment\nplot_genes_violin(cds_subset, \n                  group_cells_by=\"treat\",\n                  label_by_short_name = F) +\n  theme(axis.text.x=element_text(angle=45, hjust=1))\n</code></pre> <p>output</p> <p></p> <p>Here you'll notice that MT-ND3 is slightly upregulated in the Wild-Type when compared to the Mutant! </p>"},{"location":"2023_workshops/trajectoryAnalysis/0X_seurat_manipulation/","title":"0X seurat manipulation","text":"<p>Seurat is a user friendly R package used to analyze single-cell RNA-Seq data. Seurat objects have the following structure:</p> <p>Seurat Objects</p> <p></p> <p>Let's invetstigate some single cell data - first we will need to load the necessary libraries:</p> <pre><code># --- Load Libraries -----------------------------------------------------------\nLIB='/cluster/tufts/hpc/tools/R/4.0.0/'\n.libPaths(c(\"\",LIB))\n.libPaths()\nlibrary(Seurat)\nlibrary(monocle3)\nlibrary(clusterProfiler)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# --- Load Data ----------------------------------------------------------------\n\n# start with the day 35 seurat object \nseur &lt;- readRDS(\"./results/asd_organoids/suv420h1_mito210_d35_sub.rds\")\n</code></pre> <p>Now let's see what is in our Seurat object and how we can access our data:</p> <pre><code># --- Explain the Seurat Object ------------------------------------------------\n\n# what is in this Seurat Object?\nseur\n\n# gene names\nrownames(seur)\n\n# cell names\ncolnames(seur)\n\n# what assays do I have?\nSeurat::Assays(seur)\n\n# how do I access these assays?\nGetAssayData(object = seur, \n             assay = \"RNA\",\n             slot = \"counts\")\n\n# how do I switch the default assay to be used?\nDefaultAssay(seur) &lt;- \"RNA\"\n\n# how do I access the meta data?\nhead(seur@meta.data)\n\n# how can I access the dimension reductions?\nEmbeddings(object = seur, reduction = \"pca\")\nEmbeddings(object = seur, reduction = \"umap\")\n\n# How can I visualize my clustering?\nDimPlot(object = seur,\n        reduction = \"umap\")\n\n# what are the identities?\nIdents(object = seur)\n\n# how can I change the identities to the cell type?\nIdents(object = seur) &lt;- seur$CellType\n\n# how can I see if this changed the identities?\nDimPlot(object = seur,\n        reduction = \"umap\")\n\n# meta data in umap\nFeaturePlot(object = seur,\n            reduction = \"umap\",\n            features =c(\"percent.mito\",\"percent.ribo\"))\n\n# genes in umap\nFeaturePlot(object = seur,\n            reduction = \"umap\",\n            features =\"TOP2A\")\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics101/","title":"Introduction to Tufts HPC Cluster","text":"<p>Date: 10/09/2024</p> <p>Delilah Maloney Sr. HPC Specialist</p> <p>Questions, Issues, Requests? </p> <p>Contact us at: tts-research@tufts.edu</p> <p>Research Technology Website</p> <ul> <li>Introduction to Tufts HPC Cluster<ul> <li>TTS - Research Technology</li> </ul> </li> <li>Tufts HPC Terminologies<ul> <li>What is \"HPC\"</li> <li>What is \"Cluster\"</li> <li>CPU vs GPU</li> <li>Cores vs Node</li> <li>Memory vs Storage</li> </ul> </li> <li>Getting to Know Tufts HPC Cluster Resources<ul> <li>Account &amp; Storage Requests</li> <li>Tufts HPC Cluster</li> <li>Cluster Storage</li> <li>CPUs</li> <li>GPUs</li> <li>Cluster Resource Limit</li> <li>Cluster Computing Resource Availability</li> </ul> </li> <li>Tufts HPC Cluster Access<ul> <li>OnDemand</li> <li>Shell Access<ul> <li>Mac OSX &amp; Linux</li> <li>Windows</li> </ul> </li> <li>File Transfers To/From Tufts HPC Cluster<ul> <li>File Transfer Clients</li> <li>OnDemand</li> <li>Terminal</li> <li>Globus - Coming Soon!</li> </ul> </li> <li>Software, Modules, Packages<ul> <li>What are modules?</li> <li>What's NEW?</li> <li>How to use modules?</li> <li>Install Software/Packages</li> </ul> </li> </ul> </li> <li>Getting Work Done on Tufts HPC Cluster with SLURM<ul> <li>OnDemand</li> <li>Slurm Information</li> <li>Utilize New hpctools Module !!!</li> <li>Batch Job</li> <li>Job Status</li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#tts-research-technology","title":"TTS - Research Technology","text":""},{"location":"2024_workshops/2024_bioinformatics101/#tufts-hpc-terminologies","title":"Tufts HPC Terminologies","text":""},{"location":"2024_workshops/2024_bioinformatics101/#what-is-hpc","title":"What is \"HPC\"","text":"<ul> <li> <p>High-performance computing (HPC) is the ability to process data and perform complex calculations at high speeds.</p> </li> <li> <p>Everything is relative</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#what-is-cluster","title":"What is \"Cluster\"","text":"<ul> <li>A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Computer clusters have each node set to perform the same/similar tasks, controlled and scheduled by software. </li> </ul> <p>To parallel, or not to parallel? That is the question.</p>"},{"location":"2024_workshops/2024_bioinformatics101/#cpu-vs-gpu","title":"CPU vs GPU","text":"<ul> <li> <p>CPU -- Central Processing Unit </p> </li> <li> <p>A CPU can never be fully replaced by a GPU</p> </li> <li> <p>Can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks</p> </li> <li> <p>GPU -- Graphics Processing Unit</p> </li> <li> <p>GPUs were originally designed to create images for computer graphics and video game consoles</p> </li> <li> <p>GPGPU</p> </li> <li> <p>Performing a narrower range of more specialized tasks</p> <p></p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#cores-vs-node","title":"Cores vs Node","text":"<ul> <li>A node is a single computer in the system, which has a number of computing units, cores. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#memory-vs-storage","title":"Memory vs Storage","text":"<p>The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the  CPU and slower but less expensive and larger options further away.  Generally the fast volatile technologies (which lose data when off  power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".</p> <ul> <li>Memory</li> <li>Small, fast, expensive</li> <li>Used to store information for immediate use</li> <li>Volatile.</li> <li>Storage</li> <li>Larger, slower, cheaper</li> <li>Non-volatile (retaining data when its power is shut off)</li> </ul> <p></p>"},{"location":"2024_workshops/2024_bioinformatics101/#getting-to-know-tufts-hpc-cluster-resources","title":"Getting to Know Tufts HPC Cluster Resources","text":""},{"location":"2024_workshops/2024_bioinformatics101/#account-storage-requests","title":"Account &amp; Storage Requests","text":"<p>Go to Tufts HPC website for HPC Cluster Account and Storage Requests</p> <p>For Faculties ONLY: HPC for Classes</p>"},{"location":"2024_workshops/2024_bioinformatics101/#tufts-hpc-cluster","title":"Tufts HPC Cluster","text":"<ul> <li>Moved to MGHPCC in January 2024!</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#cluster-storage","title":"Cluster Storage","text":"<ul> <li>Home Directory</li> </ul> <p>Every user has a home directory.</p> <p>Be aware! Your Home Directory (30GB, fixed) should be <code>/cluster/home/your_utln</code></p> <p>If you are not sure how much storage you have used in your home directory, feel free to contact us and we can provide you the number. </p> <p>For self-service, you can use the following commands from a shell terminal to find out your home directory usage:</p> <p><code>$ module load hpctools</code></p> <p><code>$ hpctools</code> (from any node) </p> <p>OR </p> <p><code>$ du -a -h --max-depth=1 ~ | sort -hr</code> from a compute node in your home directory to find out the detailed usage. </p> <ul> <li>Reserach Project Storage</li> </ul> <p>Request research project storage</p> <p>Created for research labs and classes. A Tufts HPC cluster research project storage share can only be owned by a Tufts faculty.</p> <p>New Storage Policies can be found on RT Announcements page. - Tiered Storage</p> <p>Your research projet storage (from 50GB) path should be <code>/cluster/tufts/yourlabname/</code>, and each member of the lab group has a dedicated directory <code>/cluster/tufts/yourlabname/your_utln</code></p> <p>To see your research project storage quota by running the following command from any node on the new cluster Pax:</p> <p><code>$ df -h /cluster/tufts/yourlabname</code> </p> <p>OR </p> <p><code>$ module load hpctools</code></p> <p><code>$ hpctools</code></p> <p>NOTE: Accessing your research project storage space for the first time in your current session, please make sure you type out the FULL PATH to the directory <code>/cluster/tufts/yourlabname/</code>.</p>"},{"location":"2024_workshops/2024_bioinformatics101/#cpus","title":"CPUs","text":"<p>Primarily Intel Xeon CPUs, from Broadwell to Emerald Rapids, with hyperthreading enabled*.</p> <p>Compute nodes are grouped into partitions based on functionality and priority levels.</p> <p>Public Partitions :</p> <pre><code>PARTITION       TIMELIMIT      \nbatch*          7-00:00:00          \ngpu             7-00:00:00        \ninteractive     4:00:00        \nlargemem        7-00:00:00        \nmpi             7-00:00:00         \npreempt         7-00:00:00     \n</code></pre> <ul> <li>preempt - Be aware, the <code>preempt</code> partition consists of most of the nodes on the cluster, including public nodes and contrib nodes from different research labs. When submitting jobs to preempt partition, you acknowledge that your jobs are taking the risk of being preempted by higher priority jobs. In that case, you will simply have to resubmit your jobs. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#gpus","title":"GPUs","text":"<p>NVIDIA GPUs are available in <code>gpu</code> and <code>preempt</code> partitions</p> <ul> <li> <p>Request GPU resources with <code>--gres</code>. See details below.</p> </li> <li> <p>If no specific architecture is required, GPU resources can be request with<code>--gres=gpu:1</code> (one GPU)</p> </li> <li> <p>You can request more than one type of GPUs with <code>constraint</code>, e.g.  </p> </li> </ul> <p><code>--gres=gpu:1 --constraint=\"t4|p100|v100\"</code></p> <ul> <li> <p>Please DO NOT manually set <code>CUDA_VISIBLE_DEVICES</code>. </p> </li> <li> <p>Users can ONLY see GPU devices that are assigned to them with <code>$ nvidia-smi</code>.</p> </li> </ul> <p>If you submit batch jobs, it's recommended adding <code>nvidia-smi</code> in your slurm job submission script.</p> <ul> <li> <p><code>gpu</code> partition<code>-p gpu</code>:</p> </li> <li> <p>NVIDIA A100</p> <ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"a100-80G\"</code></li> <li>Each GPU comes with 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA P100s</p> <ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"p100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p><code>preempt</code> partition <code>-p preempt</code>:</p> </li> <li> <p><code>a100</code>, <code>v100</code>, <code>p100</code>, <code>rtx_6000</code>, <code>rtx_a6000</code>, <code>rtx_6000ada</code>, <code>rtx_a5000</code>, <code>h100</code>, <code>l40s</code>, <code>t4</code></p> </li> <li> <p>NVIDIA T4</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:t4:1</code>(one T4 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"t4\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 10.2</li> </ul> </li> <li> <p>NVIDIA P100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 6 on one node)</li> <li><code>--constraint=\"p100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_6000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000:1</code>(one RTX_6000 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"rtx_6000\"</code></li> <li>Each GPU comes with 24GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_a6000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_a6000:1</code>(one RTX_A6000 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"rtx_a6000\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_6000ada</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000ada:1</code>(one RTX_6000Ada GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"rtx_6000ada\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA V100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:v100:1</code>(one V100 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"v100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA A100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"a100-80G\"</code></li> <li><code>--constraint=\"a100-40G\"</code></li> <li><code>--constraint=\"a100\"</code></li> <li>Each GPU comes with 40GB of DRAM or 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA H100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:h100:1</code>(one V100 GPU, can request up to 3 on one node)</li> <li><code>--constraint=\"h100\"</code></li> <li>Each GPU comes with 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA L40s</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:l40s:1</code>(one L40s GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"l40s\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA RTX A5000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_a5000:1</code>(one RTX A5000 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"l40s\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA L40</p> <ul> <li> <p>In \"preempt\" partition</p> </li> <li> <p>Request with: <code>--gres=gpu:l40:1</code>(one L40 GPU, can request up to 4 on one node)</p> </li> <li> <p><code>--constraint=\"l40\"</code></p> </li> <li> <p>Each GPU comes with 48GB of DRAM</p> </li> <li> <p>Driver supports upto CUDA 12.2</p> </li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#cluster-resource-limit","title":"Cluster Resource Limit","text":"<ul> <li> <p>Public Partitions (batch+mpi+largemem+gpu) </p> </li> <li> <p>CPU: 500 cores</p> <p>RAM: 2000 GB</p> <p>GPU: 5</p> </li> <li> <p>Preempt Partition (preempt) </p> </li> <li> <p>CPU: 1000 cores</p> <p>RAM: 4000 GB</p> <p>GPU: 10</p> </li> <li> <p>Maximun Number of Jobs submitted: 1000</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#cluster-computing-resource-availability","title":"Cluster Computing Resource Availability","text":"<p><code>$ module load hpctools</code></p> <p><code>$ hpctools</code> </p> <p>Then follow the on-screen instructions to extract the information you need. </p>"},{"location":"2024_workshops/2024_bioinformatics101/#tufts-hpc-cluster-access","title":"Tufts HPC Cluster Access","text":"<p>VPN</p> <ul> <li>Off-campus Non-Tufts Network please connect to Tufts VPN</li> </ul> <p>2FA</p> <ul> <li>DUO is needed on Tufts Network (not needed for OnDemand, https://ondemand.pax.tufts.edu)</li> <li>DUO is needed when using FastX11 from  OnDemand</li> </ul> <p>SSH</p> <ul> <li>The SSH protocol aka Secure Shell is a method for secure remote login from one computer to another. </li> </ul> <p>X Window System (X11)</p> <ul> <li>The X Window System (X11) is an open source, cross platform,  client-server computer software system that provides a GUI in a  distributed network environment.</li> </ul> <p>Login Hostname</p> <ul> <li>login.cluster.tufts.edu = login.pax.tufts.edu</li> </ul> <p>Cluster New User Guides</p> <ul> <li>Table of Contents</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#ondemand","title":"OnDemand","text":"<p>Preferred browser: Chrome or FireFox</p> <p>Go to OnDemand, https://ondemand.pax.tufts.edu</p> <p></p> <p>Use your Tufts UTLN (Tufts username, lower case!) and password to login. </p> <p></p> <p></p> <p><code>Clusters</code>, you can start a shell access to the HPC cluster. </p> <p><code>Tufts HPC Shell Access</code> = <code>$ ssh your_utln@login.cluster.tufts.edu</code>= <code>$ ssh your_utln@login.pax.tufts.edu</code></p> <p>OR</p> <p>Use the <code>&gt;_Open in Terminal</code> button in <code>Files</code> to open a terminal in whichever directory you navigated to.</p> <p>If you need X11 access through OnDemand to display any GUI applications, please use our OnDemand <code>Clusters</code> for this option:</p> <p><code>Tufts HPC FastX11 Shell Access</code> = <code>$ ssh -XYC your_utln@login.cluster.tufts.edu</code> (with X11 for GUI applications)</p> <p>FastX Web/Desktop Client Setup Instructions</p> <p>OR </p> <p>You also have the option to use the <code>Xfce Terminal</code> under new  OnDemand <code>Interactive Apps</code> with limited computing resources.</p>"},{"location":"2024_workshops/2024_bioinformatics101/#shell-access","title":"Shell Access","text":"<p>Hostname: <code>login.cluster.tufts.edu</code> or <code>login.pax.tufts.edu</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/#mac-osx-linux","title":"Mac OSX &amp; Linux","text":"<ul> <li> <p>Terminal </p> </li> <li> <p>Shell environment (default: bash):</p> <p><code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p><code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p>With GUI (Graphical User Interface):</p> <p><code>$ ssh -XC your_utln@login.cluster.tufts.edu</code></p> <p>or</p> <p><code>$ ssh -YC your_utln@login.cluster.tufts.edu</code></p> <p>X Window System need to be locally installed.</p> <p>Now you are on a Login Node of the cluster (login-prod-[01-03]) and in your Home Directory (~). </p> <p><code>$ [your_utln@login-prod-03 ~]</code></p> </li> <li> <p>Setting up SSH keyless access </p> <ul> <li>Be sure your <code>~/.ssh</code> permission is correct! Otherwise, SSH won't work properly.</li> <li><code>. ssh</code> directory: 700 ( drwx------ )</li> <li>public key ( <code>. pub</code> file): 644 ( -rw-r--r-- )</li> <li>private key (<code>id_rsa</code> ): 600 ( -rw------- )</li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#windows","title":"Windows","text":"<ul> <li>PowerShell</li> <li>WSL - Windows Subsystem for linux</li> <li>PuTTY </li> <li>Cygwin </li> </ul> <p>Need Assistance? Contact us at tts-research@tufts.edu</p>"},{"location":"2024_workshops/2024_bioinformatics101/#file-transfers-tofrom-tufts-hpc-cluster","title":"File Transfers To/From Tufts HPC Cluster","text":"<p>File Transfer Hostname</p> <ul> <li>xfer.cluster.tufts.edu = xfer.pax.tufts.edu</li> </ul> <p>File Transfer Protocol</p> <ul> <li>SCP</li> <li>SFTP - Use this for NCBI uploads</li> <li>rsync over SSH</li> </ul> <p>Globus * - Coming Soon!</p> <ul> <li>Tufts HPC Cluster</li> <li>Tufts Box</li> <li>Tufts Sharepoint</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#file-transfer-clients","title":"File Transfer Clients","text":"<ul> <li>Windows Only - WinSCP</li> <li>FileZilla </li> <li>Cyberduck</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#ondemand_1","title":"OnDemand","text":"<ul> <li>OnDemand (Single file size up to 976MB)</li> </ul> <p>Only for transfering files size less than 976MB per file.</p> <p>Go to OnDemand:</p> <p>https://ondemand.pax.tufts.edu/ </p> <p>Under <code>Files</code>, using the <code>Upload</code> or <code>Download</code> buttons to transfer. Make sure you navigate to the destination/source directory on cluster using the <code>Go To</code> button before transfering files.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics101/#terminal","title":"Terminal","text":"<ul> <li>Hostname for file transfer: xfer.cluster.tufts.edu</li> </ul> <p>NOTE:</p> <ul> <li>Local_Path is the path to your files or directory on your local computer</li> <li>Cluster_Path is the path to your files or directory on the cluster</li> <li>Home Directory: /cluster/home/your_utln/your_folder</li> <li>Research Project Storage Space Directory: /cluster/tufts/yourlabname/your_utln/your_folder</li> </ul> <p>**Execute from your local machine terminal. **</p> <p>General Format:</p> <p><code>$ scp From_Path To_Path</code></p> <p><code>$ rsync From_Path To_Pat</code></p> <p>NOTE: If you are transfering very large files that could take hours to finish, we would suggest using <code>rsync</code> as it has ability to restart from where it left if interrupted.</p> <p>File Transfer with <code>scp</code>or <code>rsync</code>:</p> <ul> <li>Download from cluster</li> </ul> <p><code>$ scp your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <p><code>$ rsync your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <ul> <li>Upload to cluster</li> </ul> <p><code>$ scp Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p><code>$ rsync Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p>Directory Transfer with <code>scp</code> or <code>rsync</code>:</p> <ul> <li>Download from cluster</li> </ul> <p><code>$ scp -r your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <p><code>$ rsync -azP your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <ul> <li>Upload to cluster</li> </ul> <p><code>$ scp -r Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p><code>$ rsync -azP Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/#globus-coming-soon","title":"Globus - Coming Soon!","text":"<p>Globus is a research cyberinfrastructure, developed and operated as a not-for-profit service by the University of Chicago.</p> <p>Local Computer: Globus Connect Personal</p> <p>Collections: </p> <ul> <li> <p>Tufts HPC Cluster Storage (Home and Project)</p> </li> <li> <p>Tufts Box</p> </li> <li> <p>Tufts Sharepoint</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#software-modules-packages","title":"Software, Modules, Packages","text":""},{"location":"2024_workshops/2024_bioinformatics101/#what-are-modules","title":"What are modules?","text":"<ul> <li>A tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles</li> <li>Each modulefile contains the information needed to configure the shell for an application. (PATH, LD_LIBRARY_PATH, CPATH, etc.). Without modules, these environment variables need to be set manually in every new session where the application is needed. </li> <li>Modules are useful in managing different versions of applications. </li> <li>Modules can also be bundled into metamodules that will load an entire set of different applications. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#whats-new","title":"What's NEW?","text":"<ul> <li>Switched from TCL to Lmod</li> <li>All previous module commands work as they should + more</li> <li>Allows module usage tracking</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#how-to-use-modules","title":"How to use modules?","text":"<p>Cheat Sheet</p> <p><code>module av</code> - check available modules on the MODULEPATH</p> <p><code>module av &lt;software&gt;</code> - check if a specific software is available as a module</p> <p><code>module spider &lt;keyword&gt;</code> * - lists all possible modules and not just the modules that can be seen in the current MODULEPATH (such as private modules)</p> <p><code>module --raw show &lt;module_name&gt;</code> * - printing the modulefile</p> <p><code>module list</code> - check loaded modules</p> <p><code>module load &lt;software&gt;</code> - load a specific module</p> <p><code>module unload &lt;software&gt;</code> - unload a specific module</p> <p><code>module swap &lt;loaded_software&gt; &lt;new_software&gt;</code> - switch a loaded module for a new one</p> <p><code>module purge</code> - unload all loaded modules</p> <p>To check available modules installed on the cluster, this may take a few minutes as there are a lot of modules, and be sure to browse the entire list as there are several module file locations:</p> <pre><code>[tutln01@login-prod-01 ~]$ module av\n</code></pre> <p>Upon login, environment variable <code>PATH</code> is set for the system to search executables along these paths:</p> <pre><code>[tutln01@login-prod-01 ~]$ echo $PATH\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n</code></pre> <p>For example, I would like to use <code>gcc</code> compiler, to check what versions of gcc compiler is available, load the version I would like to use, and use it:</p> <pre><code>[tutln01@login-prod-01 ~]$ module av gcc\n\n----------------------------------------------------------- /opt/shared/Modules/modulefiles-rhel6 ------------------------------------------------------------\ngcc/4.7.0 gcc/4.9.2 gcc/5.3.0 gcc/7.3.0\n\n-------------------------------------------------------------- /cluster/tufts/hpc/tools/module ---------------------------------------------------------------\ngcc/8.4.0 gcc/9.3.0 gcc/11.2.0\n</code></pre> <p>Use <code>module list</code> to check loaded modules in current environment:</p> <pre><code>[tutln01@login-prod-01 ~]$ module load gcc/7.3.0\n[tutln01@login-prod-01 ~]$ module list\nCurrently Loaded Modulefiles:\n  1) use.own     2) gcc/7.3.0\n</code></pre> <pre><code>[tutln01@login-prod-01 ~]$ which gcc\n/opt/shared/gcc/7.3.0/bin/gcc\n[tutln01@login-prod-01 ~]$ echo $PATH\n/opt/shared/gcc/7.3.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n[tutln01@login-prod-01 ~]$ gcc --version\ngcc (GCC) 7.3.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre> <p>swap a module for another (doesn't have to be the same software):</p> <pre><code>[tutln01@login-prod-01 ~]$ module swap gcc/7.3.0 gcc/9.3.0 \n[tutln01@login-prod-01 ~]$ module list\nCurrently Loaded Modulefiles:\n  1) use.own     2) gcc/9.3.0\n</code></pre> <p>unload loaded modules:</p> <pre><code>[tutln01@login-prod-01 ~]$ module unload gcc\n[tutln01@login-prod-01 ~]$ echo $PATH\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n</code></pre> <p>unload ALL of the loaded modules in the current environment:</p> <pre><code>[tutln01@login-prod-01 ~]$ module purge\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics101/#install-softwarepackages","title":"Install Software/Packages","text":"<ul> <li>R (R command line recommanded)</li> <li>Packages need to be reinstalled for each version of R</li> <li>OnDemand <ul> <li>Interactive Apps - RStudio Pax</li> <li>Bioinformatics Apps - RStudio for bioinformatics, RStudio for scRNA-Seq</li> </ul> </li> <li>Python (Conda env recommanded)</li> <li>Not recommended: anaconda/3 or anaconda/2 (older versions), only use this version when it's absolutely necessary</li> <li>Other software compiled from source</li> <li>gcc</li> <li>cmake</li> <li>Autotools - automake, autoconf, autogen, .etc</li> <li>... any dependencies, load if available, install if not. Some environment variables may need to be set manually.</li> <li>Follow instructions (read it through first)</li> <li>Use \"--prefix=\" to install in non-standard locations</li> <li>Modify the environment variables !!! (such as PATH, LD_LIBRARY_PATH, CPATH, .etc)</li> <li>You can make a private module for your locally installed software. Here is HOW</li> <li>OR you can submit a request to tts-research@tufts.edu for the software to be installed globally on HPC cluster to share with the community. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#getting-work-done-on-tufts-hpc-cluster-with-slurm","title":"Getting Work Done on Tufts HPC Cluster with SLURM","text":"<p>ALL work MUST to be performed on compute nodes!</p> <p>If you see prompt like this <code>[your_utln@login-prod-01]</code>, <code>[your_utln@login-prod-02]</code>, <code>[your_utln@login-prod-03]</code>, DON'T run any programs! Get resource allocation first!</p> <p></p> <p>Things to think about before requesting resources:</p> <ul> <li>What program?</li> <li>What kind of resources? CPUs only or with GPUs?</li> <li>How much memory?</li> <li>How many CPU cores?</li> <li>How long does the job need to run?</li> <li>Command line application? GUI?</li> <li>Does the program need interactions?</li> <li>Prototyping, debugging, or production?</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#ondemand_2","title":"OnDemand","text":"<ul> <li> <p>OnDemand (https://ondemand.pax.tufts.edu)</p> </li> <li> <p><code>Interactive Apps</code> --&gt; RStudio, Matlab, JupyterLab, Jupyter Notebook, .etc</p> </li> <li> <p><code>Clusters</code> --&gt; Tufts HPC Cluster  Shell Access </p> </li> <li> <p><code>Files</code> </p> </li> <li> <p><code>Jobs</code></p> </li> <li> <p>OnDemand <code>Clusters</code> --&gt; Tufts HPC Cluster FastX11 Shell Access</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/#slurm-information","title":"Slurm Information","text":"<ul> <li>View information about Slurm nodes and partitions.</li> </ul> <p><code>$ sinfo</code></p> <p>With more specifc information and formated output:</p> <p><code>$ sinfo -o \"%20N %10P %10c %10m %85f %10G \"</code> - NODELIST, PARTITION, CPUS, MEMORY,AVAIL_FEATURES, GRES  </p> <p>More  sinfo options</p> <p>You can only see the partitions you have access to.</p> <p>For most users, you will see <code>batch</code>,<code>mpi</code>,<code>gpu</code>,<code>largemem</code>, and <code>preempt</code> partitions.          </p> <p>How to check GPU, Memory, CPU availability on the cluster?</p>"},{"location":"2024_workshops/2024_bioinformatics101/#utilize-new-hpctools-module","title":"Utilize New <code>hpctools</code> Module !!!","text":"<p>Users can use <code>hpctools</code> module to check: **Free CPU resources, Free GPU resources, User Past and Active jobs, and Project space quota and usage. **</p> <p>```[tutln01@login-prod-01 ~]$ module load hpctools [tutln01@login-prod-01 ~]$ module load hpctools      command: hpctools [tutln01@login-prod-01 ~]$ hpctools  Please select from the following options:</p> <ol> <li> <p>Checking Free Resources On Each Node in Given Partition(s)</p> </li> <li> <p>Checking Free GPU Resources On Each Node in Given Partition(s)</p> </li> <li> <p>Checking tutln01 Past Completed Jobs in Given Time Period</p> </li> <li> <p>Checking tutln01 Active Job informantion</p> </li> <li> <p>Checking Project Space Storage Quota Informantion</p> </li> <li> <p>Checking Any Directory Storage Usage Informantion</p> </li> </ol> <p>Press q to quit</p> <p>Your Selection:</p> <pre><code>Then follow the onscreen instructions to get desired information.\n\n\n\n## Interactive Session\n\n- Particularly good for debugging and working with software GUI. \n\n  `$ srun [options] --pty [command]`\n\n- Command \n\n  - command to run an application, given the module is already loaded.\n  - `bash` for a bash shell\n\n- Options\n\n  - Pseudo terminal `--pty`\n  - Partition `-p` or `--partition=`\n    - Default batch if not specified\n  - Time `-t` or `--time=`\n    - Default 15 minutes if not specified on non-interactive partition\n    - Format: DD-HH:MM:SS\n  - Number of CPU cores `-n` \n    - Default 1 if not specified\n  - Memory `--mem=`\n    - Default 2GB if not specified\n  - GPU `--gres=`\n    - Default none\n  - Features `--constraint=`\n    - GPU types\n    - OS version\n    - CPU architecture\n    - Instruction Set\n    - Default none\n  - X Window `--x11=first`\n    - Default none  \n\n**Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 2GB of RAM, with X11 forwarding for 1 day, 2 hours, and 30 minutes (use `exit` to end session and release resources).**\n\n```bash\n[tutln01@login-prod-01 ~]$ srun -p preempt -t 1-2:30:00 -n 2 --mem=2g --x11=first --pty bash\nsrun: job 296794 queued and waiting for resources\nsrun: job 296794 has been allocated resources\n[tutln01@cc1gpu001 ~]$ \n</code></pre> <p>Note: If you are requesting X11 forwarding with <code>srun</code>, <code>-XC</code> or<code>-YC</code> or <code>-XYC</code> must be used upon login with <code>ssh</code>.</p> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 4GB of RAM, with 1 A100 GPU for 1 day, 2 hours, and 30 minutes (use <code>exit</code> to end session and release resources).</p> <pre><code>[tutln01@login-prod-01 ~]$ srun -p preempt -t 1-2:30:00 -n 2 --mem=4g --gres=gpu:a100:1 --pty bash\n</code></pre> <p>Once your resource is allocated on a compute node, use <code>nvidia-smi</code> to check GPU info.</p>"},{"location":"2024_workshops/2024_bioinformatics101/#batch-job","title":"Batch Job","text":"<p>Write a batch submission script e.g. myjob.sh</p> <pre><code>#!/bin/bash -l\n#SBATCH -J My_Job_Name   #job name\n#SBATCH --time=00-00:20:00  #requested time (DD-HH:MM:SS)\n#SBATCH -p batch,preempt    #running on \"batch\" or \"preempt\" partition, wherever resource is available first\n#SBATCH -N 1    #1 nodes #for many shared-memory programs,please leave -N as 1.\n#SBATCH -n 2   #2 tasks total and 1 cpu per task, that gives you 2 cpu cores for this job\n#SBATCH --mem=2g  #requesting 2GB of RAM total for the number of cpus you requested\n##SBATCH --gres=gpu:a100:1  #requesting 1 A100 GPU, in this case, the \"-p\" needs to be switched to a partition has the requested GPU resources\n#SBATCH --output=MyJob.%j.%N.out  #saving standard output to file, %j=JOBID, %N=NodeName\n#SBATCH --error=MyJob.%j.%N.err   #saving standard error to file, %j=JOBID, %N=NodeName\n#SBATCH --mail-type=ALL    #email optitions\n#SBATCH --mail-user=Your_Tufts_Email@tufts.edu\n\n#[commands_you_would_like_to_exe_on_the_compute_nodes]\n# unload all modules to ensure a clean start.\nmodule purge\n# for example, running a python script \n# load the module so the correct version python is available to you\nmodule load anaconda/2021.05\n# If you have a conda env that you would like to use, activate it here using \"source activate xxx\". DO NOT USE \"conda activate\"\nsource activate [target_env]\n# run python script\npython myscript.py #make sure myscript.py exists in the current directory\n# make sure you save all plots, data, outputs generated to files in your script\n# Don't forget to deactivate your conda env if you are using one\nconda deactivate\n</code></pre> <p>Submit the job using the following command from command line interface:</p> <p><code>$ sbatch myjob.sh</code></p> <p>Sample Scripts including R, conda, matlab, gaussian</p> <p><code>/cluster/tufts/hpc/tools/slurm_scripts</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/#job-status","title":"Job Status","text":"<ul> <li>Checking your active jobs</li> </ul> <pre><code>[tutln01@cc1gpu001 ~]$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n            296794   preempt     bash tutln01  R       5:12      1 cc1gpu001 \n[tutln01@cc1gpu001 ~]$ squeue -u tutln01\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n            296794   preempt     bash tutln01  R       5:21      1 cc1gpu001 \n</code></pre> <p>To check your active jobs in the queue:</p> <p><code>$ squeue -u $USER</code> or <code>$ squeue -u your_utln</code></p> <p>To cancel a specific job:</p> <p><code>$ scancel JOBID</code></p> <p>To cancel all of your jobs:</p> <p><code>$ scancel -u $USER</code> or <code>$ scancel -u your_utln</code></p> <p>To check details of your active jobs (running or pending):</p> <p><code>$ scontrol show jobid -dd JOBID</code></p> <pre><code>[tutln01@cc1gpu001 ~]$ scontrol show jobid -dd 296794\nJobId=296794 JobName=bash\n   UserId=tutln01(31003) GroupId=tutln01(5343) MCS_label=N/A\n   Priority=10833 Nice=0 Account=(null) QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:10:33 TimeLimit=1-02:30:00 TimeMin=N/A\n   SubmitTime=2021-03-22T22:18:50 EligibleTime=2021-03-22T22:18:50\n   AccrueTime=2021-03-22T22:18:50\n   StartTime=2021-03-22T22:18:55 EndTime=2021-03-24T00:48:55 Deadline=N/A\n   PreemptEligibleTime=2021-03-22T22:18:55 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-03-22T22:18:55\n   Partition=preempt AllocNode:Sid=login-prod-01:34458\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=cc1gpu001\n   BatchHost=cc1gpu001\n   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=2G,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   JOB_GRES=(null)\n     Nodes=cc1gpu001 CPU_IDs=30-31 Mem=2048 GRES=\n   MinCPUsNode=1 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=bash\n   WorkDir=/cluster/home/tutln01\n   Power=\n   MailUser=tutln01 MailType=NONE\n</code></pre> <ul> <li>Checking your finished jobs</li> </ul> <p>You can no longer see these jobs in <code>squeue</code> command output.</p> <p>Querying finished jobs helps users make better decisions on requesting resources for future jobs. </p> <p>Check job CPU, memory usage, and efficiency:</p> <p><code>$ seff JOBID</code></p> <pre><code>[tutln01@login-prod-01 ~]$ seff 296794\nJob ID: 296794\nCluster: pax\nUse of uninitialized value $user in concatenation (.) or string at /usr/bin/seff line 154, &lt;DATA&gt; line 602.\nUser/Group: /tutln01\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:22:12 core-walltime\nJob Wall-clock time: 00:11:06\nMemory Utilized: 1.16 MB (estimated maximum)\nMemory Efficiency: 0.06% of 2.00 GB (2.00 GB/node)\n</code></pre> <p>Check job detailed accounting data:</p> <p><code>$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j JOBID</code></p> <pre><code>[tutln01@login-prod-01 ~]$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j  296794\n Partition      State  Timelimit               Start                 End    Elapsed     MaxRSS     ReqMem  MaxVMSize   NNodes      NCPUS        NodeList \n---------- ---------- ---------- ------------------- ------------------- ---------- ---------- ---------- ---------- -------- ---------- --------------- \n   preempt  COMPLETED 1-02:30:00 2021-03-22T22:18:55 2021-03-22T22:30:01   00:11:06                   2Gn                   1          2       cc1gpu001 \n           OUT_OF_ME+            2021-03-22T22:18:55 2021-03-22T22:30:01   00:11:06         8K        2Gn    135100K        1          2       cc1gpu001 \n            COMPLETED            2021-03-22T22:18:56 2021-03-22T22:30:01   00:11:05       592K        2Gn    351672K        1          2       cc1gpu001 \n</code></pre> <p>NOTE: there are more format options, see sacct</p> <p>OR utilize <code>hpctools</code> module on the cluster to make things a little easier</p> <p>```[tutln01@login-prod-01 ~]$ module load hpctools [tutln01@login-prod-01 ~]$ module load hpctools      command: hpctools [tutln01@login-prod-01 ~]$ hpctools  Please select from the following options:</p> <ol> <li> <p>Checking Free Resources On Each Node in Given Partition(s)</p> </li> <li> <p>Checking Free GPU Resources On Each Node in Given Partition(s)</p> </li> <li> <p>Checking tutln01 Past Completed Jobs in Given Time Period</p> </li> <li> <p>Checking tutln01 Active Job informantion</p> </li> <li> <p>Checking Project Space Storage Quota Informantion</p> </li> <li> <p>Checking Any Directory Storage Usage Informantion</p> </li> </ol> <p>Press q to quit</p> <p>Your Selection:</p> <p>```</p> <p>Then follow the onscreen instructions to get desired information.</p> <p>Useful Link</p> <p>https://tufts.box.com/v/HPC-Table-of-Contents</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/","title":"HPC for Bioinformatics Agenda","text":"<p>Tufts University Bioinformatics Workshop Series - 2024 Fall</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#workshop-1-introduction-to-hpc","title":"Workshop 1: Introduction to HPC","text":"<p>Oct 9, 2024, 1-3pm</p> <ul> <li>Tufts HPC Terminologies</li> <li>Getting to Know Tufts HPC Cluster Resources</li> <li>Tufts HPC Cluster Access</li> <li>Getting Work Done on Tufts HPC Cluster with SLURM</li> </ul> <p>Recording</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#workshop-2-getting-started-with-bioinformatics-on-tufts-hpc","title":"Workshop 2: Getting started with Bioinformatics on Tufts HPC","text":"<p>Oct 10, 2024, 1-3pm</p> <ul> <li>Linux/Unix command line basics</li> <li>Available bioinformatics resources/tools on Tufts HPC</li> <li>How to run bioinformatics analysis on Tufts HPC</li> </ul> <p>Recording</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#workshop-3","title":"Workshop 3:","text":"<p>Oct 17, 2024, 1-3pm</p> <ul> <li>Application Installation from Source Codes</li> <li>R package Installation</li> <li>Conda Environment Management, Jupyer Kernel and Modules</li> <li>Simplying Conda Environment Management with conda-env-mod</li> </ul> <p>Recording</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#workshop-4","title":"Workshop 4:","text":"<p>Oct 24, 2024, 1-3pm</p> <ul> <li>Parallelizing Workflows with Slurm Job Arrays</li> <li>Nextflow and nf-core</li> <li>nf-core: Community Curated Bioinformatics Pipelines</li> </ul> <p>Recording</p>"},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#workshop-flyer","title":"Workshop Flyer","text":""},{"location":"2024_workshops/2024_bioinformatics_workshop_agenda/#presenters","title":"Presenters:","text":"<ul> <li>Delilah Maloney, Senior HPC Specialist         </li> <li>Yucheng Zhang, Bioinformatics Engineer        </li> <li>Shirley Xue Li, Bioinformatician        </li> </ul> <p>Workshop flyer</p>"},{"location":"2024_workshops/readme/","title":"2024 Workshops","text":"<p>The TTS Research Technology Bioinformatics Team has prepared the following workshops and lectures:</p>"},{"location":"2024_workshops/readme/#2024-spring","title":"2024 Spring","text":"<ul> <li>Introduction to Metagenomics (Sp24-IGDH-1001-Bioinformatics)</li> <li>Introduction to AlphaFold2 (Sp24-CHBE-0165-01-Biomolecular Eng and Design) </li> <li>nextflow and nf-core RNA-Seq </li> </ul>"},{"location":"2024_workshops/readme/#2024-fall","title":"2024 Fall","text":"<ul> <li>Introduction to HPC</li> <li>Getting Started with Bioinformatics on HPC</li> <li>Software Installation and Environment Management in HPC</li> <li>Advanced HPC Workflows and Job Management</li> </ul> <p>If you have suggestions for future workshops, please reach out to tts-research@tufts.edu.       Additionally, if you'd like to be kept up to date on current workshops consider subscribing to our e-list: best@elist.tufts.edu</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/","title":"Tufts HPC Terminologies","text":""},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/#what-is-hpc","title":"What is \"HPC\"","text":"<ul> <li> <p>High-performance computing (HPC) is the ability to process data and perform complex calculations at high speeds.</p> </li> <li> <p>Everything is relative</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/#what-is-cluster","title":"What is \"Cluster\"","text":"<ul> <li>A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Computer clusters have each node set to perform the same/similar tasks, controlled and scheduled by software. </li> </ul> <p>To parallel, or not to parallel? That is the question.</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/#cpu-vs-gpu","title":"CPU vs GPU","text":"<p>CPU -- Central Processing Unit </p> <ul> <li> <p>A CPU can never be fully replaced by a GPU</p> </li> <li> <p>Can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks</p> </li> </ul> <p>GPU -- Graphics Processing Unit</p> <ul> <li> <p>GPUs were originally designed to create images for computer graphics and video game consoles</p> </li> <li> <p>GPGPU</p> </li> <li> <p>Performing a narrower range of more specialized tasks</p> <p></p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/#cores-vs-node","title":"Cores vs Node","text":"<ul> <li>A node is a single computer in the system, which has a number of computing units, cores. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/00_Tufts_HPC_Terminologies/#memory-vs-storage","title":"Memory vs Storage","text":"<p>The central processing unit (CPU) of a computer is what manipulates data by performing computations.  In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the  CPU and slower but less expensive and larger options further away.  Generally the fast volatile technologies (which lose data when off  power) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".</p> <p>Memory</p> <ul> <li>Small, fast, expensive</li> <li>Used to store information for immediate use</li> <li>Volatile</li> </ul> <p>Storage</p> <ul> <li>Larger, slower, cheaper</li> <li>Non-volatile (retaining data when its power is shut off)</li> </ul> <p></p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/","title":"Getting to Know Tufts HPC Cluster Resources","text":""},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#account-storage-requests","title":"Account &amp; Storage Requests","text":"<p>Go to Tufts HPC website for HPC Cluster Account and Storage Requests</p> <p>For Faculties ONLY: HPC for Classes</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#tufts-hpc-cluster","title":"Tufts HPC Cluster","text":"<ul> <li>Moved to MGHPCC in January 2024!</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#cluster-storage","title":"Cluster Storage","text":""},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#home-directory","title":"Home Directory","text":"<p>Every user has a home directory.</p> <p>Be aware! Your Home Directory (30GB, fixed) should be <code>/cluster/home/your_utln</code></p> <p>If you are not sure how much storage you have used in your home directory, feel free to contact us and we can provide you with the number. </p> <p>For self-service, you can use the following commands from a shell terminal to find out your home directory usage:</p> <pre><code>$ module load hpctools\n\n$ hpctools\n</code></pre> <p>OR </p> <p><code>$ du -a -h --max-depth=1 ~ | sort -hr</code> from a compute node in your home directory to find out the detailed usage. </p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#reserach-project-storage","title":"Reserach Project Storage","text":"<p>Request research project storage</p> <p>Created for research labs and classes. A Tufts HPC cluster research project storage share can only be owned by a Tufts faculty.</p> <p>New Storage Policies can be found on RT Announcements page.</p> <p>Starting July 1, 2026: </p> <ul> <li>Tier 1 (hot) for active, high-performance compute use. - 10TB quota, $85 per TB per year beyond 10 TB quota.</li> <li>Tier 2 (cool) for inactive, infrequently accessed data, no compute on data. - 10TB quota, $35 per TB per year beyond 10 TB quota.</li> <li>Tier 3 (deep cold) for static, rarely accessed data, \u201cattic\u201d storage. - 0TB quota, 6 per TB per year.</li> </ul> <p>Tier 1 Storage:</p> <p>Your research projet storage (from 50GB) path should be <code>/cluster/tufts/yourlabname/</code>, and each member of the lab group has a dedicated directory <code>/cluster/tufts/yourlabname/your_utln</code></p> <p>To see your research project storage quota by running the following command from any node on the new cluster Pax:</p> <pre><code>$ module load hpctools\n\n$ hpctools\n</code></pre> <p>OR </p> <p><code>$ df -h /cluster/tufts/yourlabname</code> </p> <p>NOTE: Accessing your research project storage space for the first time in your current session, please make sure you type out the FULL PATH to the directory <code>/cluster/tufts/yourlabname/</code>.</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#cpus","title":"CPUs","text":"<p>Primarily Intel Xeon CPUs, from Broadwell to Emerald Rapids, with hyperthreading enabled*.</p> <p>Compute nodes are grouped into partitions based on functionality and priority levels.</p> <p>Public Partitions :</p> <pre><code>PARTITION       TIMELIMIT      \nbatch*          7-00:00:00          \ngpu             7-00:00:00        \ninteractive     4:00:00        \nlargemem        7-00:00:00        \nmpi             7-00:00:00         \npreempt         7-00:00:00     \n</code></pre> <ul> <li>preempt - Be aware, the <code>preempt</code> partition consists of most of the nodes on the cluster, including public nodes and contrib nodes from different research labs. When submitting jobs to preempt partition, you acknowledge that your jobs are taking the risk of being preempted by higher priority jobs. In that case, you will simply have to resubmit your jobs. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#gpus","title":"GPUs","text":"<p>NVIDIA GPUs are available in <code>gpu</code> and <code>preempt</code> partitions</p> <ul> <li> <p>Request GPU resources with <code>--gres</code>. See details below.</p> </li> <li> <p>If no specific architecture is required, GPU resources can be request with<code>--gres=gpu:1</code> (one GPU)</p> </li> <li> <p>You can request more than one type of GPUs with <code>constraint</code>, e.g.  </p> </li> </ul> <p><code>--gres=gpu:1 --constraint=\"t4|p100|v100\"</code></p> <ul> <li> <p>Please DO NOT manually set <code>CUDA_VISIBLE_DEVICES</code>. </p> </li> <li> <p>Users can ONLY see GPU devices that are assigned to them with <code>$ nvidia-smi</code>.</p> </li> </ul> <p>If you submit batch jobs, it's recommended adding <code>nvidia-smi</code> in your slurm job submission script.</p> <ul> <li> <p><code>gpu</code> partition<code>-p gpu</code>:</p> </li> <li> <p>NVIDIA A100</p> <ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"a100-80G\"</code></li> <li>Each GPU comes with 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA P100s</p> <ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"p100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p><code>preempt</code> partition <code>-p preempt</code>:</p> </li> <li> <p><code>a100</code>, <code>v100</code>, <code>p100</code>, <code>rtx_6000</code>, <code>rtx_a6000</code>, <code>rtx_6000ada</code>, <code>rtx_a5000</code>, <code>h100</code>, <code>l40s</code>, <code>t4</code>, <code>l40</code></p> </li> <li> <p>NVIDIA T4</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:t4:1</code>(one T4 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"t4\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 10.2</li> </ul> </li> <li> <p>NVIDIA P100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 6 on one node)</li> <li><code>--constraint=\"p100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_6000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000:1</code>(one RTX_6000 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"rtx_6000\"</code></li> <li>Each GPU comes with 24GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_a6000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_a6000:1</code>(one RTX_A6000 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"rtx_a6000\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA rtx_6000ada</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000ada:1</code>(one RTX_6000Ada GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"rtx_6000ada\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA V100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:v100:1</code>(one V100 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"v100\"</code></li> <li>Each GPU comes with 16GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA A100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> <li><code>--constraint=\"a100-80G\"</code></li> <li><code>--constraint=\"a100-40G\"</code></li> <li><code>--constraint=\"a100\"</code></li> <li>Each GPU comes with 40GB of DRAM or 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA H100</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:h100:1</code>(one V100 GPU, can request up to 3 on one node)</li> <li><code>--constraint=\"h100\"</code></li> <li>Each GPU comes with 80GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA L40s</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:l40s:1</code>(one L40s GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"l40s\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA RTX A5000</p> <ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_a5000:1</code>(one RTX A5000 GPU, can request up to 4 on one node)</li> <li><code>--constraint=\"rtx_a5000\"</code></li> <li>Each GPU comes with 48GB of DRAM</li> <li>Driver supports upto CUDA 12.2</li> </ul> </li> <li> <p>NVIDIA L40</p> <ul> <li> <p>In \"preempt\" partition</p> </li> <li> <p>Request with: <code>--gres=gpu:l40:1</code>(one L40 GPU, can request up to 4 on one node)</p> </li> <li> <p><code>--constraint=\"l40\"</code></p> </li> <li> <p>Each GPU comes with 48GB of DRAM</p> </li> <li> <p>Driver supports upto CUDA 12.2</p> </li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#cluster-resource-limit","title":"Cluster Resource Limit","text":"<ul> <li> <p>Public Non-Preempt Partitions (batch+mpi+largemem+gpu) </p> </li> <li> <p>CPU: 500 cores</p> <p>RAM: 2000 GB</p> <p>GPU: 5</p> </li> <li> <p>Preempt Partition (preempt) </p> </li> <li> <p>CPU: 1000 cores</p> <p>RAM: 4000 GB</p> <p>GPU: 10</p> </li> <li> <p>Maximun Number of Jobs submitted: 1000</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/01_Getting_to_Know_Tufts_HPC_Cluster_Resources/#cluster-computing-resource-availability","title":"Cluster Computing Resource Availability","text":"<pre><code>$ module load hpctools\n\n$ hpctools\n</code></pre> <p>Then follow the on-screen instructions to extract the information you need. </p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/","title":"Tufts HPC Cluster Access","text":"<p>VPN</p> <ul> <li>Off-campus Non-Tufts Network please connect to Tufts VPN</li> </ul> <p>2FA</p> <ul> <li>DUO is needed on Tufts Network (not needed for OnDemand, https://ondemand.pax.tufts.edu)</li> <li>DUO is needed when using FastX11 from  OnDemand</li> </ul> <p>SSH</p> <ul> <li>The SSH protocol aka Secure Shell is a method for secure remote login from one computer to another. </li> </ul> <p>X Window System (X11)</p> <ul> <li>The X Window System (X11) is an open source, cross platform,  client-server computer software system that provides a Graphical User Interface (GUI) in a  distributed network environment.</li> </ul> <p>Login Hostname</p> <ul> <li>login.cluster.tufts.edu = login.pax.tufts.edu</li> </ul> <p>Cluster New User Guides</p> <ul> <li>Table of Contents - New documentation site coming soon! Look out for our emails!</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#ondemand","title":"OnDemand","text":"<p>Preferred browser: Chrome or FireFox</p> <p>Go to OnDemand, https://ondemand.pax.tufts.edu</p> <p></p> <p>Use your Tufts UTLN (Tufts username, lower case!) and password to login. </p> <p></p> <p></p> <p><code>Clusters</code> - Start a shell access to the HPC cluster.</p> <p><code>Tufts HPC Shell Access</code> = <code>$ ssh your_utln@login.cluster.tufts.edu</code>= <code>$ ssh your_utln@login.pax.tufts.edu</code></p> <p>Note: the <code>&gt;_Open in Terminal</code> button in <code>Files</code> also opens a terminal in whichever directory you navigated to.</p> <ul> <li>If you need X11 access through OnDemand to display any GUI applications, use OnDemand</li> </ul> <p><code>Tufts HPC FastX11 Shell Access</code> = <code>$ ssh -YC your_utln@login.cluster.tufts.edu</code> (with X11 for GUI applications)</p> <p>FastX Web/Desktop Client Setup Instructions</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#shell-access","title":"Shell Access","text":"<p>Hostname: <code>login.cluster.tufts.edu</code> or <code>login.pax.tufts.edu</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#mac-osx-linux","title":"Mac OSX &amp; Linux","text":"<p>Terminal </p> <ul> <li> <p>Shell environment (default: bash):</p> <p><code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p><code>$ ssh your_utln@login.cluster.tufts.edu</code></p> <p>With GUI (Graphical User Interface):</p> <p><code>$ ssh -XC your_utln@login.cluster.tufts.edu</code></p> <p>or</p> <p><code>$ ssh -YC your_utln@login.cluster.tufts.edu</code></p> <p>X Window System need to be locally installed.</p> <p>Now you are on a Login Node of the cluster (login-prod-[01-03]) and in your Home Directory (~). </p> <p><code>$ [your_utln@login-prod-03 ~]</code></p> </li> <li> <p>Setting up SSH keyless access </p> <ul> <li>Be sure your <code>~/.ssh</code> permission is correct! Otherwise, SSH won't work properly.</li> <li><code>. ssh</code> directory: 700 ( drwx------ )</li> <li>public key ( <code>. pub</code> file): 644 ( -rw-r--r-- )</li> <li>private key (<code>id_rsa</code> ): 600 ( -rw------- )</li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#windows","title":"Windows","text":"<ul> <li>PowerShell</li> <li>WSL - Windows Subsystem for linux</li> <li>PuTTY </li> <li>Cygwin </li> </ul> <p>Need Assistance? Contact us at tts-research@tufts.edu</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#file-transfers-tofrom-tufts-hpc-cluster","title":"File Transfers To/From Tufts HPC Cluster","text":"<p>File Transfer Hostname</p> <ul> <li>xfer.cluster.tufts.edu = xfer.pax.tufts.edu</li> </ul> <p>File Transfer Protocol</p> <ul> <li>SCP</li> <li>SFTP - Use this for NCBI uploads</li> <li>rsync over SSH</li> </ul> <p>Globus * - Coming Soon!</p> <ul> <li>Tufts HPC Cluster</li> <li>Tufts Box</li> <li>Tufts Sharepoint</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#file-transfer-clients","title":"File Transfer Clients","text":"<ul> <li>Windows Only - WinSCP</li> <li>FileZilla </li> <li>Cyberduck</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#ondemand_1","title":"OnDemand","text":"<ul> <li>OnDemand (Single file size up to 976MB)</li> </ul> <p>Only for transfering files size less than 976MB per file.</p> <p>Go to OnDemand:</p> <p>https://ondemand.pax.tufts.edu/ </p> <p>Under <code>Files</code>, using the <code>Upload</code> or <code>Download</code> buttons to transfer. Make sure you navigate to the destination/source directory on cluster using the <code>Go To</code> button before transfering files.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#terminal","title":"Terminal","text":"<ul> <li>Hostname for file transfer: xfer.cluster.tufts.edu</li> </ul> <p>NOTE:</p> <ul> <li>Local_Path is the path to your files or directory on your local computer</li> <li>Cluster_Path is the path to your files or directory on the cluster</li> <li>Home Directory: /cluster/home/your_utln/your_folder</li> <li>Research Project Storage Space Directory: /cluster/tufts/yourlabname/your_utln/your_folder</li> </ul> <p>**Execute from your local machine terminal. **</p> <p>General Format:</p> <p><code>$ scp From_Path To_Path</code></p> <p><code>$ rsync From_Path To_Pat</code></p> <p>NOTE: If you are transfering very large files that could take hours to finish, we would suggest using <code>rsync</code> as it has ability to restart from where it left if interrupted.</p> <p>File Transfer with <code>scp</code>or <code>rsync</code>:</p> <ul> <li>Download from cluster</li> </ul> <p><code>$ scp your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <p><code>$ rsync your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <ul> <li>Upload to cluster</li> </ul> <p><code>$ scp Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p><code>$ rsync Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p>Directory Transfer with <code>scp</code> or <code>rsync</code>:</p> <ul> <li>Download from cluster</li> </ul> <p><code>$ scp -r your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <p><code>$ rsync -azP your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path</code></p> <ul> <li>Upload to cluster</li> </ul> <p><code>$ scp -r Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p> <p><code>$ rsync -azP Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#globus-coming-soon","title":"Globus - Coming Soon!","text":"<p>Globus is a research cyberinfrastructure, developed and operated as a not-for-profit service by the University of Chicago.</p> <p>Local Computer: Globus Connect Personal</p> <p>Tufts Collections: </p> <ul> <li> <p>Tufts HPC Cluster Storage (Home and Project)</p> </li> <li> <p>Tufts Box</p> </li> <li> <p>Tufts Sharepoint</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#software-modules-packages","title":"Software, Modules, Packages","text":""},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#what-are-modules","title":"What are modules?","text":"<ul> <li>A tool that simplify shell initialization and lets users easily modify their environment during the session with modulefiles</li> <li>Each modulefile contains the information needed to configure the shell for an application. (PATH, LD_LIBRARY_PATH, CPATH, etc.). Without modules, these environment variables need to be set manually in every new session where the application is needed. </li> <li>Modules are useful in managing different versions of applications. </li> <li>Modules can also be bundled into metamodules that will load an entire set of different applications. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#whats-new","title":"What's NEW?","text":"<ul> <li>Switched from TCL to Lmod</li> <li>All previous module commands work as they should + more</li> <li>Allows module usage tracking</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#how-to-use-modules","title":"How to use modules?","text":"<p>Cheat Sheet</p> <p><code>module av</code> - check available modules on the MODULEPATH</p> <p><code>module av &lt;software&gt;</code> - check if a specific software is available as a module</p> <p><code>module spider &lt;keyword&gt;</code> * - lists all possible modules and not just the modules that can be seen in the current MODULEPATH (such as private modules)</p> <p><code>module --raw show &lt;module_name&gt;</code> * - printing the modulefile</p> <p><code>module list</code> - check loaded modules</p> <p><code>module load &lt;software&gt;</code> - load a specific module</p> <p><code>module unload &lt;software&gt;</code> - unload a specific module</p> <p><code>module swap &lt;loaded_software&gt; &lt;new_software&gt;</code> - switch a loaded module for a new one</p> <p><code>module purge</code> - unload all loaded modules</p> <p>To check available modules installed on the cluster, this may take a few minutes as there are a lot of modules, and be sure to browse the entire list as there are several module file locations:</p> <pre><code>[tutln01@login-prod-01 ~]$ module av\n</code></pre> <p>Upon login, environment variable <code>PATH</code> is set for the system to search executables along these paths:</p> <pre><code>[tutln01@login-prod-01 ~]$ echo $PATH\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n</code></pre> <p>For example, I would like to use <code>gcc</code> compiler, to check what versions of gcc compiler is available, load the version I would like to use, and use it:</p> <pre><code>[tutln01@login-prod-01 ~]$ module av gcc\n\n----------------------------------------------------------- /opt/shared/Modules/modulefiles-rhel6 ------------------------------------------------------------\ngcc/4.7.0 gcc/4.9.2 gcc/5.3.0 gcc/7.3.0\n\n-------------------------------------------------------------- /cluster/tufts/hpc/tools/module ---------------------------------------------------------------\ngcc/8.4.0 gcc/9.3.0 gcc/11.2.0\n</code></pre> <p>Use <code>module list</code> to check loaded modules in current environment:</p> <pre><code>[tutln01@login-prod-01 ~]$ module load gcc/7.3.0\n[tutln01@login-prod-01 ~]$ module list\nCurrently Loaded Modulefiles:\n  1) use.own     2) gcc/7.3.0\n</code></pre> <pre><code>[tutln01@login-prod-01 ~]$ which gcc\n/opt/shared/gcc/7.3.0/bin/gcc\n[tutln01@login-prod-01 ~]$ echo $PATH\n/opt/shared/gcc/7.3.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n[tutln01@login-prod-01 ~]$ gcc --version\ngcc (GCC) 7.3.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre> <p>swap a module for another (doesn't have to be the same software):</p> <pre><code>[tutln01@login-prod-01 ~]$ module swap gcc/7.3.0 gcc/9.3.0 \n[tutln01@login-prod-01 ~]$ module list\nCurrently Loaded Modulefiles:\n  1) use.own     2) gcc/9.3.0\n</code></pre> <p>unload loaded modules:</p> <pre><code>[tutln01@login-prod-01 ~]$ module unload gcc\n[tutln01@login-prod-01 ~]$ echo $PATH\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/tutln01/bin:/cluster/home/tutln01/.local/bin\n</code></pre> <p>unload ALL of the loaded modules in the current environment:</p> <pre><code>[tutln01@login-prod-01 ~]$ module purge\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/02_Tufts_HPC_Cluster_Access/#install-softwarepackages","title":"Install Software/Packages","text":"<p>R (R command line recommanded)</p> <ul> <li>Packages need to be reinstalled for each version of R</li> <li> <p>OnDemand</p> <ul> <li>Interactive Apps - RStudio Pax</li> <li>Bioinformatics Apps - RStudio for bioinformatics, RStudio for scRNA-Seq</li> </ul> </li> </ul> <p>Python (Conda env recommanded)</p> <ul> <li>Not recommended: anaconda/3 or anaconda/2 (older versions), only use this version when it's absolutely necessary</li> </ul> <p>Other software compiled from source</p> <ul> <li>gcc</li> <li>cmake</li> <li>Autotools - automake, autoconf, autogen, .etc</li> <li>... any dependencies, load if available, install if not. Some environment variables may need to be set manually.</li> <li>Follow instructions (read it through first)</li> <li>Use \"--prefix=\" to install in non-standard locations</li> <li>Modify the environment variables !!! (such as PATH, LD_LIBRARY_PATH, CPATH, .etc)</li> <li>You can make a private module for your locally installed software. Here is HOW</li> <li>OR you can submit a request to tts-research@tufts.edu for the software to be installed globally on HPC cluster to share with the community. </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/","title":"Getting Work Done on Tufts HPC Cluster with SLURM","text":"<p>ALL work MUST to be performed on compute nodes!</p> <p>If you see prompt like this  <code>[your_utln@login-prod-01]</code> <code>[your_utln@login-prod-02]</code> <code>[your_utln@login-prod-03]</code> DON'T run any programs, YET!  Get resource allocation first!</p> <p></p> <p>Things to think about before requesting resources:</p> <ul> <li>What program?</li> <li>What kind of resources? CPUs only or with GPUs?</li> <li>How much memory?</li> <li>How many CPU cores?</li> <li>How long does the job need to run?</li> <li>Command line application? GUI?</li> <li>Do you need to interact with the application at runtime?</li> <li>Prototyping, debugging, or production?</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#ondemand","title":"OnDemand","text":"<ul> <li> <p>OnDemand (https://ondemand.pax.tufts.edu)</p> </li> <li> <p><code>Interactive Apps</code> --&gt; RStudio, Matlab, JupyterLab, Jupyter Notebook, .etc</p> </li> <li> <p><code>Bioinformatics Apps</code> --&gt; AlphaFold, CellProfiler, FastQC, Jupyter, QualiMap, RStudio, RELION, nf-core piplines, .etc</p> </li> <li> <p><code>Clusters</code> --&gt; Tufts HPC Cluster  Shell Access </p> </li> <li> <p><code>Files</code> --&gt; Access Home and Research project storage</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#slurm-information","title":"Slurm Information","text":"<p>View information about Slurm nodes and partitions.</p> <p><code>$ sinfo</code></p> <p>With more specifc information and formated output:</p> <p><code>$ sinfo -o \"%20N %10P %10c %10m %85f %10G \"</code> - NODELIST, PARTITION, CPUS, MEMORY,AVAIL_FEATURES, GRES  </p> <p>Check out more  sinfo options!</p> <p>You can only see the partitions you have access to.</p> <p>For most users, you will see <code>batch</code>,<code>mpi</code>,<code>gpu</code>,<code>largemem</code>, and <code>preempt</code> partitions.          </p> <p>How to check GPU, Memory, CPU availability on the cluster?</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#utilize-new-hpctools-module","title":"Utilize New <code>hpctools</code> Module !!!","text":"<p>Users can use <code>hpctools</code> module to check:  Free CPU resources, Free GPU resources, User Past and Active jobs, and Project space quota and usage.</p> <pre><code>[tutln01@login-prod-01 ~]$ module load hpctools\n     command: hpctools\n[tutln01@login-prod-01 ~]$ hpctools\n Please select from the following options:\n\n  1. Checking Free Resources On Each Node in Given Partition(s)\n\n  2. Checking Free GPU Resources On Each Node in Given Partition(s)\n\n  3. Checking tutln01 Past Completed Jobs in Given Time Period\n\n  4. Checking tutln01 Active Job informantion\n\n  5. Checking Project Space Storage Quota Informantion\n\n  6. Checking Any Directory Storage Usage Informantion\n\n  Press q to quit\n\nYour Selection:\n</code></pre> <p>Then follow the on-screen instructions to get desired information.</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#interactive-session","title":"Interactive Session","text":"<p>Particularly good for debugging and working with software GUI. </p> <p><code>$ srun [options] [command (executable + arguments)]</code></p> <p>Command</p> <ul> <li>command to run an application, given the module is already loaded.</li> <li><code>bash</code> for a bash shell</li> </ul> <p>Options</p> <ul> <li> <p>Pseudo terminal <code>--pty</code></p> </li> <li> <p>Partition <code>-p</code> or <code>--partition=</code></p> <ul> <li>Default batch if not specified</li> </ul> </li> <li> <p>Time <code>-t</code> or <code>--time=</code></p> <ul> <li>Default 15 minutes if not specified on non-interactive partition</li> <li>Format: DD-HH:MM:SS</li> </ul> </li> <li> <p>Number of CPU cores <code>-n</code></p> <ul> <li>Default 1 if not specified</li> </ul> </li> <li> <p>Memory <code>--mem=</code></p> <ul> <li>Default 2GB if not specified</li> </ul> </li> <li> <p>GPU <code>--gres=</code></p> <ul> <li>Default none</li> </ul> </li> <li> <p>Features <code>--constraint=</code></p> <ul> <li>GPU types, OS version, CPU architecture, Instruction Set, .etc</li> <li>Default none</li> </ul> </li> <li> <p>X Window <code>--x11=first</code></p> <ul> <li>Default none</li> </ul> </li> <li> <p>More to srun</p> </li> </ul> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 2GB of RAM, with X11 forwarding for 1 day, 2 hours, and 30 minutes (use <code>exit</code> to end session and release resources).</p> <pre><code>[tutln01@login-prod-01 ~]$ srun -p preempt -t 1-2:30:00 -n 2 --mem=2g --x11=first --pty bash\nsrun: job 296794 queued and waiting for resources\nsrun: job 296794 has been allocated resources\n[tutln01@cc1gpu001 ~]$ \n</code></pre> <p>Note: If you are requesting X11 forwarding with <code>srun</code>, <code>-XC</code> or<code>-YC</code> or <code>-XYC</code> must be used upon login with <code>ssh</code>.</p> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 4GB of RAM, with 1 A100 GPU for 1 day, 2 hours, and 30 minutes (use <code>exit</code> to end session and release resources).</p> <pre><code>[tutln01@login-prod-01 ~]$ srun -p preempt -t 1-2:30:00 -n 2 --mem=4g --gres=gpu:a100:1 --pty bash\n</code></pre> <p>Once your resource is allocated on a compute node, use <code>nvidia-smi</code> to check GPU info.</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#batch-job","title":"Batch Job","text":"<p>Write a batch submission script e.g. myjob.sh</p> <pre><code>#!/bin/bash -l\n#SBATCH -J My_Job_Name   #job name\n#SBATCH --time=00-00:20:00  #requested time (DD-HH:MM:SS)\n#SBATCH -p batch,preempt    #running on \"batch\" or \"preempt\" partition, wherever resource is available first\n#SBATCH -N 1    #1 nodes #for many shared-memory programs,please leave -N as 1.\n#SBATCH -n 2   #2 tasks total and 1 cpu per task, that gives you 2 cpu cores for this job\n#SBATCH --mem=2g  #requesting 2GB of RAM total for the number of cpus you requested\n##SBATCH --gres=gpu:a100:1  #requesting 1 A100 GPU, in this case, the \"-p\" needs to be switched to a partition has the requested GPU resources\n#SBATCH --output=MyJob.%j.%N.out  #saving standard output to file, %j=JOBID, %N=NodeName\n#SBATCH --error=MyJob.%j.%N.err   #saving standard error to file, %j=JOBID, %N=NodeName\n#SBATCH --mail-type=ALL    #email optitions\n#SBATCH --mail-user=Your_Tufts_Email@tufts.edu\n\n#[commands_you_would_like_to_exe_on_the_compute_nodes]\n# unload all mo\nmodule purge\n# for example, running a python script \n# load the module so the correct version python is available to you\nmodule load anaconda/2021.05\n# If you have a conda env that you would like to use, activate it here using \"source activate xxx\". DO NOT USE \"conda activate\"\nsource activate [target_env]\n# run python script\npython myscript.py #make sure myscript.py exists in the current directory\n# make sure you save all plots, data, outputs generated to files in your script\n# Don't forget to deactivate your conda env if you are using one\nconda deactivate\n</code></pre> <p>Submit the job using the following command from command line interface:</p> <p><code>$ sbatch myjob.sh</code></p> <p>Sample Scripts including R, conda, matlab, gaussian</p> <p><code>/cluster/tufts/hpc/tools/slurm_scripts</code></p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/03_Getting_Work_Done_on_Tufts_HPC_Cluster_with_SLURM/#job-status","title":"Job Status","text":"<ul> <li>Checking your active jobs</li> </ul> <pre><code>[tutln01@cc1gpu001 ~]$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n            296794   preempt     bash tutln01  R       5:12      1 cc1gpu001 \n[tutln01@cc1gpu001 ~]$ squeue -u tutln01\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n            296794   preempt     bash tutln01  R       5:21      1 cc1gpu001 \n</code></pre> <p>To check your active jobs in the queue:</p> <p><code>$ squeue -u $USER</code> or <code>$ squeue -u your_utln</code></p> <p>To cancel a specific job:</p> <p><code>$ scancel JOBID</code></p> <p>To cancel all of your jobs:</p> <p><code>$ scancel -u $USER</code> or <code>$ scancel -u your_utln</code></p> <p>To check details of your active jobs (running or pending):</p> <p><code>$ scontrol show jobid -dd JOBID</code></p> <pre><code>[tutln01@cc1gpu001 ~]$ scontrol show jobid -dd 296794\nJobId=296794 JobName=bash\n   UserId=tutln01(31003) GroupId=tutln01(5343) MCS_label=N/A\n   Priority=10833 Nice=0 Account=(null) QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:10:33 TimeLimit=1-02:30:00 TimeMin=N/A\n   SubmitTime=2021-03-22T22:18:50 EligibleTime=2021-03-22T22:18:50\n   AccrueTime=2021-03-22T22:18:50\n   StartTime=2021-03-22T22:18:55 EndTime=2021-03-24T00:48:55 Deadline=N/A\n   PreemptEligibleTime=2021-03-22T22:18:55 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2021-03-22T22:18:55\n   Partition=preempt AllocNode:Sid=login-prod-01:34458\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=cc1gpu001\n   BatchHost=cc1gpu001\n   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=2G,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   JOB_GRES=(null)\n     Nodes=cc1gpu001 CPU_IDs=30-31 Mem=2048 GRES=\n   MinCPUsNode=1 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=bash\n   WorkDir=/cluster/home/tutln01\n   Power=\n   MailUser=tutln01 MailType=NONE\n</code></pre> <ul> <li>Checking your finished jobs</li> </ul> <p>You can no longer see these jobs in <code>squeue</code> command output.</p> <p>Querying finished jobs helps users make better decisions on requesting resources for future jobs. </p> <p>Check job CPU, memory usage, and efficiency:</p> <p><code>$ seff JOBID</code></p> <pre><code>[tutln01@login-prod-01 ~]$ seff 296794\nJob ID: 296794\nCluster: pax\nUse of uninitialized value $user in concatenation (.) or string at /usr/bin/seff line 154, &lt;DATA&gt; line 602.\nUser/Group: /tutln01\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:00:00\nCPU Efficiency: 0.00% of 00:22:12 core-walltime\nJob Wall-clock time: 00:11:06\nMemory Utilized: 1.16 MB (estimated maximum)\nMemory Efficiency: 0.06% of 2.00 GB (2.00 GB/node)\n</code></pre> <p>Check job detailed accounting data:</p> <p><code>$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j JOBID</code></p> <pre><code>[tutln01@login-prod-01 ~]$ sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j  296794\n Partition      State  Timelimit               Start                 End    Elapsed     MaxRSS     ReqMem  MaxVMSize   NNodes      NCPUS        NodeList \n---------- ---------- ---------- ------------------- ------------------- ---------- ---------- ---------- ---------- -------- ---------- --------------- \n   preempt  COMPLETED 1-02:30:00 2021-03-22T22:18:55 2021-03-22T22:30:01   00:11:06                   2Gn                   1          2       cc1gpu001 \n           OUT_OF_ME+            2021-03-22T22:18:55 2021-03-22T22:30:01   00:11:06         8K        2Gn    135100K        1          2       cc1gpu001 \n            COMPLETED            2021-03-22T22:18:56 2021-03-22T22:30:01   00:11:05       592K        2Gn    351672K        1          2       cc1gpu001 \n</code></pre> <p>NOTE: there are more format options, see sacct</p> <p>OR utilize <code>hpctools</code> module on the cluster to make things a little easier</p> <pre><code>[tutln01@login-prod-01 ~]$ module load hpctools\n     command: hpctools\n[tutln01@login-prod-01 ~]$ hpctools\n Please select from the following options:\n\n  1. Checking Free Resources On Each Node in Given Partition(s)\n\n  2. Checking Free GPU Resources On Each Node in Given Partition(s)\n\n  3. Checking tutln01 Past Completed Jobs in Given Time Period\n\n  4. Checking tutln01 Active Job informantion\n\n  5. Checking Project Space Storage Quota Informantion\n\n  6. Checking Any Directory Storage Usage Informantion\n\n  Press q to quit\n\nYour Selection:\n</code></pre> <p>Then follow the onscreen instructions to get desired information.</p> <p>Useful Link</p> <p>https://tufts.box.com/v/HPC-Table-of-Contents</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/readme/","title":"Introduction to Tufts HPC Cluster","text":"<p>Date: 10/09/2024</p> <p>Delilah Maloney</p> <p>Sr. HPC Specialist</p> <p>Questions, Issues, Requests? </p> <p>Contact us at: tts-research@tufts.edu</p> <p>Research Technology Website</p>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/readme/#tts-research-technology","title":"TTS - Research Technology","text":""},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/readme/#todays-topics","title":"Today's Topics","text":"<ul> <li>Tufts HPC Terminologies</li> <li>Getting to Know Tufts HPC Cluster Resources</li> <li>Tufts HPC Cluster Access</li> <li>Getting Work Done on Tufts HPC Cluster with SLURM</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics101/Intro2HPC/readme/#recording","title":"Recording","text":""},{"location":"2024_workshops/2024_bioinformatics201/00_introduction/","title":"Beginner's guide for conducting bioinformatics analysis on Tufts HPC","text":""},{"location":"2024_workshops/2024_bioinformatics201/00_introduction/#welcome","title":"Welcome","text":"<p>The goals of this workshop are to:</p> <ul> <li>Help users get started with bioinformatics analysis, especially command-line bioinformatics, on Tufts HPC.</li> <li>Familiarise with bioinformatics resources/tools available on Tufts HPC.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/00_introduction/#agenda","title":"Agenda","text":"<ul> <li>Linux/Unix command line basics</li> <li>Available bioinformatics resources/tools on Tufts HPC</li> <li>How to run bioinformatics analysis on Tufts HPC</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/00_introduction/#recording","title":"Recording","text":""},{"location":"2024_workshops/2024_bioinformatics201/00_introduction/#presenters","title":"Presenters","text":"<sub>Shirley Li</sub> <sub>Yucheng Zhang</sub> <p>Next: Intro to Linux/Unix</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/","title":"Available bioinformatics resources/tools on Tufts HPC","text":""},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#mailing-list-e-list","title":"Mailing List (E-list)","text":"<p>To stay updated on bioinformatics education, software tools, and workshop notifications, subscribe to our e-list: best@elist.tufts.edu.</p> <p>In the future, we will post workshop notifications and resources through this email list.</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#workshops-and-resources","title":"Workshops and resources","text":""},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#tufts-research-technology-guides-website-in-development","title":"Tufts Research Technology Guides website (In development)","text":"<p>Coming in November 2024. Stay tuned for updates! </p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#current-workshops-stop-updating-once-our-new-website-is-online","title":"Current Workshops, stop updating once our new website is online.","text":"<p>Bioinformatics workshops provided by TTS Research Technology in 2022, 2023, 2024](https://tuftsdatalab.github.io/tuftsWorkshops/)</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#archived-workshops","title":"Archived Workshops","text":"<p>Archived TTS Datalab bioinformatics workshops</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#other-workshops","title":"Other Workshops","text":"<p>Bioinformatics workshops provided by Bioinformatics Education and Services at Tufts</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#website","title":"Website","text":"<p>Bioinformatics website (Last updated July 2023) </p> <p>Note: Some materials may be outdated, so always verify the relevance of content before applying it to your projects.</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#tools-on-the-cluster","title":"Tools on the Cluster","text":"<p>Use <code>module avail</code> to check the full list of tools available on the cluster. Below are some commonly used tools:</p> <pre><code>   abcreg/0.1.0                         kallisto/0.48.0                     (D)    orthofinder/2.5.5          \n   abyss/2.3.7                          kneaddata/0.12.0                           pandaseq/2.11\n   alphafold/2.3.0                      kraken2/2.1.3                              parabricks/4.0.0-1\n   alphafold/2.3.1                      krakentools/1.2                            parabricks/4.2.1-1         \n   alphafold/2.3.2                      macs2/2.2.7.1                        \n   amplify/2.0.0                        macs3/3.0.0a6                              pepper_deepvariant/r0.8    \n   angsd/0.939                          masurca/4.0.9                              petitefinder/cpu\n   angsd/0.940                   (D)    masurca/4.1.0                       (D)    picard/2.25.1\n   bakta/1.9.3                          medaka/1.11.1                              picard/2.26.10              \n   bbmap/38.93                          megahit/1.2.9                              plink/1.90b6.21            \n   bbmap/38.96                   (D)    meme/5.5.5                                 plink2/2.00a2.3\n   bbtools/39.00                        metaphlan/4.0.2                            polypolish/0.5.0\n   bcftools/1.13                        metaphlan/4.0.6                     (D)    preseq/3.2.0\n   bcftools/1.14                        miniasm/0.3_r179                           prokka/1.14.6\n   bcftools/1.17                        minimap2/2.26                       (D)    qiime2/2023.2\n   bcftools/1.20                 (D)    minipolish/0.1.3                           qiime2/2023.5\n   beast2/2.6.3                         mirdeep2/2.0.1.3                           qiime2/2023.7\n   beast2/2.6.4                         mirge3/0.1.4                               qiime2/2023.9\n   beast2/2.6.6                  (D)    mothur/1.46.0                              qiime2/2024.2             \n   ... ...\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#a-few-tips","title":"A few tips","text":"<ol> <li> <p>Before installing your own tools, check if they are already available on the cluster using the <code>module avail</code> command.</p> </li> <li> <p>Always be aware of the software versions, especially when using scripts from colleagues.</p> </li> <li> <p>For less common tools, consider installing them yourself to ensure you have full control over the version and availability.</p> </li> </ol> <p>We will cover how to install tools from source code in our upcoming workshop. If you need to install a tool not commonly used, it's best to do it yourself to avoid issues with maintenance. Stay tuned for a detailed guide!</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#open-ondemand-apps","title":"Open OnDemand APPs","text":"<p>You can access Open OnDemand through this link</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#bioinformatics-apps","title":"Bioinformatics Apps","text":"<p>We offer a wide range of bioinformatics tools as apps, including <code>AlphaFold</code> and <code>CellProfiler</code>. Additionally, 31 nf-core pipelines are available as apps for ease of use, with the most popular being nf-core/rnaseq, which we will demonstrate in our final workshop.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#rstudio-and-shinyngs-apps","title":"RStudio and Shinyngs Apps","text":"<p>RStudio Pax, use R/4.4.1 which has the most comprehensive packages installed (1300+).  </p> <p></p> <p>We will demenstrate how to install packages in our next workshop</p> <p>RStudio is available on Open OnDemand, with different versions tailored for specific tasks. For example, in <code>RStudio for scRNA-Seq</code>, the most commonly used packages for scRNA-Seq analysis are pre-installed to help streamline your workflow.</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#other-apps","title":"Other Apps","text":"<p>We also provide other applications like <code>Jupyter Bioinfo</code>, <code>JupyterLab</code>, <code>Jupyter Notebook</code>, <code>IGV</code>, and <code>Galaxy</code> to support your daily research activities.</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#nf-core-pipelines","title":"nf-core pipelines","text":"<p>Use <code>module avail nf-core</code> to get the list of nf-core pipelines deployed on cluster</p>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#modules","title":"modules","text":"<pre><code>   nf-core-ampliseq/2.8.0                     nf-core-nanostring/1.3.0      (D)\n   nf-core-ampliseq/2.9.0                     nf-core-pairgenomealign/1.0.0\n   nf-core-ampliseq/2.10.0                    nf-core-pangenome/1.1.0\n   nf-core-ampliseq/2.11.0             (D)    nf-core-pangenome/1.1.1\n   nf-core-atacseq/2.1.2                      nf-core-pangenome/1.1.2       (D)\n   nf-core-bacass/2.2.0                       nf-core-proteinfold/1.1.0\n   nf-core-bacass/2.3.1                (D)    nf-core-raredisease/2.0.1\n   nf-core-bamtofastq/2.1.1                   nf-core-rnafusion/3.0.1\n   nf-core-chipseq/2.0.0                      nf-core-rnafusion/3.0.2       (D)\n   nf-core-denovotranscript/1.0.0             nf-core-rnaseq/3.14.0\n   nf-core-detaxizer/1.0.0                    nf-core-rnasplice/1.0.2\n   nf-core-differentialabundance/1.4.0        nf-core-rnasplice/1.0.3\n   nf-core-differentialabundance/1.5.0 (D)    nf-core-rnasplice/1.0.4       (D)\n   nf-core-eager/2.5.1                        nf-core-sarek/3.4.0\n   nf-core-fetchngs/1.11.0                    nf-core-sarek/3.4.1\n   nf-core-fetchngs/1.12.0             (D)    nf-core-sarek/3.4.3\n   nf-core-funcscan/1.1.4                     nf-core-sarek/3.4.4           (D)\n   nf-core-funcscan/1.1.5              (D)    nf-core-scrnaseq/2.5.1\n   nf-core-hic/2.1.0                          nf-core-scrnaseq/2.7.0\n   nf-core-mag/2.5.2                          nf-core-scrnaseq/2.7.1        (D)\n   nf-core-mag/2.5.4                          nf-core-smrnaseq/2.3.0\n   nf-core-mag/3.0.0                          nf-core-smrnaseq/2.3.1        (D)\n   nf-core-mag/3.0.2                   (D)    nf-core-taxprofiler/1.1.5\n   nf-core-metatdenovo/1.0.0                  nf-core-taxprofiler/1.1.6\n   nf-core-metatdenovo/1.0.1           (D)    nf-core-taxprofiler/1.1.7\n   nf-core-methylseq/2.6.0                    nf-core-taxprofiler/1.1.8     (D)\n   nf-core-nanoseq/3.1.0                      nf-core-viralrecon/2.6.0\n   nf-core-nanostring/1.2.1\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#open-ondemand-apps_1","title":"Open OnDemand apps","text":""},{"location":"2024_workshops/2024_bioinformatics201/02_tufts_bioinformatics_resources/#contact-us","title":"Contact Us","text":"<p>We are part of the TTS Research Technology team. Our services are not limited to bioinformatics; we also support data science, statistics, and research data management.</p> <p>For consultations, please submit a ticket to tts-research@tufts.edu.</p> <p>For more details, visit our page: Research Technology Consulting Services.</p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/","title":"How to run bioinformatics analysis on Tufts HPC","text":"<p>Author: Shirley Li, xue.li37@tufts.edu                 </p> <p>Date: 2024-10-07</p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#objectives","title":"Objectives","text":"<ul> <li>The primary goal of this tutorial is to introduce participants to bioinformatics on a high-performance computing (HPC) cluster.</li> <li>By the end of this tutorial, participants will: Understand basic command-line operations.  </li> <li>Be familiar with common bioinformatics file formats (e.g., FASTA, FASTQ, SAM, BAM). - Gain hands-on experience with bioinformatics tools and workflows.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#getting-started","title":"Getting Started","text":""},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of biology and bioinformatics </li> <li>Familiarity with the command line </li> <li>Access to an HPC cluster (e.g., login credentials, necessary software installations)                    </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#setup","title":"Setup","text":"<ol> <li> <p>Connecting to the Cluster through terminal or Open OnDemand.  </p> </li> <li> <p>Start an interactive session, go from log in mode to compute mode. </p> </li> </ol> <pre><code>srun -p interactive -n 1 --time=4:00:00 --mem=32g --cpus-per-task=8 --pty bash\n</code></pre> <ol> <li>Copying Sample Data</li> </ol> <p>You can use your lab storage or your home directory (only for this workshop). </p> <pre><code>cd /cluster/tufts/XXlab/utln/\n</code></pre> <p>Copy example data to your directory. </p> <pre><code>cp -r /cluster/tufts/workshop/demo/bio_2024 ./\ncd bio_2024\n</code></pre> <p>Let's take a look at the files using <code>ls</code></p> <pre><code>genomics_data  other  raw_fastq  README.txt  reference_data\n</code></pre> <p>\u200b   For this workshop, we will store the data in our home directory since it\u2019s small in size. However, in the future, avoid using your home directory for data storage. Always utilize your lab's designated storage space instead. Otherwise, your home directory will fill up quickly, limiting your ability to perform various tasks.</p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#overview-of-the-input-files","title":"Overview of the input files","text":""},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#fasta-file","title":"FASTA file","text":"<p>FASTA is a text-based format for representing nucleotide sequences or protein sequences. It is widely used in bioinformatics for sequence data storage and analysis. Each sequence in a FASTA file is represented by a header line starting with a '&gt;', followed by lines of sequence data.</p> <ul> <li>A typical FASTA file has the following structure: </li> <li>Header Line: This line starts with a '&gt;' character, followed by a description or identifier of the sequence. </li> <li>Sequence Data: The nucleotide or amino acid sequence, which can span multiple lines.</li> <li>Example:</li> </ul> <pre><code>cat other/rcsb_pdb_5XUS.fasta\n</code></pre> <p>What does this file contain?</p> <pre><code>&gt;5XUS_1|Chain A|LbCpf1|Lachnospiraceae bacterium ND2006 (1410628)\nGSHMSKLEKFTNCYSLSKTLRFKAIPVGKTQENIDNKRLLVEDEKRAEDYKGVKKLLDRYYLSFINDVLHSIKLKNLNNYISLFRKKTRTEKENKELENLEINLRKEIAKAFKGNEGYKSLFKKDIIETILPEFLDDKDEIALVNSFNGFTTAFTGFFDNRENMFSEEAKSTSIAFRCINENLTRYISNMDIFEKVDAIFDKHEVQEIKEKILNSDYDVEDFFEGEFFNFVLTQEGIDVYNAIIGGFVTESGEKIKGLNEYINLYNQKTKQKLPKFKPLYKQVLSDRESLSFYGEGYTSDEEVLEVFRNTLNKNSEIFSSIKKLEKLFKNFDEYSSAGIFVKNGPAISTISKDIFGEWNVIRDKWNAEYDDIHLKKKAVVTEKYEDDRRKSFKKIGSFSLEQLQEYADADLSVVEKLKEIIIQKVDEIYKVYGSSEKLFDADFVLEKSLKKNDAVVAIMKDLLDSVKSFENYIKAFFGEGKETNRDESFYGDFVLAYDILLKVDHIYDAIRNYVTQKPYSKDKFKLYFQNPQFMGGWDKDKETDYRATILRYGSKYYLAIMDKKYAKCLQKIDKDDVNGNYEKINYKLLPGPNKMLPKVFFSKKWMAYYNPSEDIQKIYKNGTFKKGDMFNLNDCHKLIDFFKDSISRYPKWSNAYDFNFSETEKYKDIAGFYREVEEQGYKVSFESASKKEVDKLVEEGKLYMFQIYNKDFSDKSHGTPNLHTMYFKLLFDENNHGQIRLSGGAELFMRRASLKKEELVVHPANSPIANKNPDNPKKTTTLSYDVYKDKRFSEDQYELHIPIAINKCPKNIFKINTEVRVLLKHDDNPYVIGIDRGERNLLYIVVVDGKGNIVEQYSLNEIINNFNGIRIKTDYHSLLDKKEKERFEARQNWTSIENIKELKAGYISQVVHKICELVEKYDAVIALEDLNSGFKNSRVKVEKQVYQKFEKMLIDKLNYMVDKKSNPCATGGALKGYQITNKFESFKSMSTQNGFIFYIPAWLTSKIDPSTGFVNLLKTKYTSIADSKKFISSFDRIMYVPEEDLFEFALDYKNFSRTDADYIKKWKLYSYGNRIRIFRNPKKNNVFDWEEVCLTSAYKELFNKYGINYQQGDIRALLCEQSDKAFYSSFMALMSLMLQMRNSITGRTDVDFLISPVKNSDGIFYDSRNYEAQENAILPKNADANGAYNIARKVLWAIGQFKKAEDEKLDKVKIAISNKEWLEYAQTSVKH\n\n&gt;5XUS_2|Chain B|crRNA|synthetic construct (32630)\nAAUUUCUACUAAGUGUAGAUGGAAAUUAGGUGCGCUUGGC\n\n&gt;5XUS_3|Chain C|DNA (29-MER)|synthetic construct (32630)\nGCCAAGCGCACCTAATTTCCTAAAGGACG\n\n&gt;5XUS_4|Chain D|DNA (5'-D(*CP*GP*TP*CP*CP*TP*TP*TP*A)-3')|synthetic construct (32630)\nCGTCCTTTA\n</code></pre> <p>Counting the number of sequencies in a FASTA file:</p> <pre><code>grep -c \"^&gt;\" other/rcsb_pdb_5XUS.fasta\n</code></pre> <ul> <li>Common Use: The FASTA format is typically used to store reference genome sequences, which can be for a whole genome, specific chromosomes, or individual genes. This format is essential for tasks like sequence alignment, variant calling, and genome assembly.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#fastq-file","title":"FASTQ file","text":"<p>FASTQ is a text-based format used to store both nucleotide sequences and their corresponding quality scores. It is widely used in bioinformatics, particularly for storing data from high-throughput sequencing technologies.           - Structure of a FASTQ File A FASTQ file consists of a series of entries, each representing a single read. Each entry has four lines:             - Header Line: Starts with '@' followed by a sequence identifier and an optional description.             - Sequence Line: The raw sequence of nucleotides (A, T, C, G).             - Separator Line: Starts with a '+' character and can optionally be followed by the same sequence identifier and description as in the header line.             - Quality Line: Encodes the quality scores for the sequence in the sequence line, using ASCII characters.         </p> <ul> <li>Example:</li> </ul> <pre><code>head raw_fastq/Irrel_kd_1.subset.fq\n</code></pre> <p>Warning: Do not use <code>cat</code> to view fq files, as they are often very large. </p> <pre><code>@HWI-ST330:304:H045HADXX:1:1214:19169:32660\nCTTTTTTGCTGGAACTTTAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGCAGTACCCTTAGTACCAGGTGCTTTTTTGGGAGAAGCT\n+\nCCCFFFFFGGHGHJJJJJJJIEHIIIIIGIIIJJHIJIIJJJJJGGGIJJJJJJJJJJGHGHHFBCEFFEEEEE;@@CDDD5&gt;ACDDDDDDBB?BDDDCA\n@HWI-ST330:304:H045HADXX:2:1212:14280:80867\nCCCTAATGATGATATATGGATCAAAAGTCTTCTTTGTAGTACAAACAGTCATGCTGCCTTCGATCAGGTCCAGGGTTGCATTAACATGATGTTCATTTAA\n+\n:?@DDDDFDHHGHJIGBFGIGIIIGDE:CFIHIIJE1?&gt;CEFGCEGFG?FGI?F@FHCCGIHIIJGHC@FHHGEB@9AEEHFFFEDFCCACEEEEDDDDD\n@HWI-ST330:304:H045HADXX:2:2111:14933:89958\nGGCATCCATGTTCTTGCCCAAAACCTTGGTTACAGCAATCTGATACTTCTTTTGTGTGGGCTGGCATAGGTCAATGAGGCAGATCGGAAGAGCACACGTC\n</code></pre> <p>You can also use <code>less</code> to view the file and press <code>Q</code> key to exit.  </p> <p>FASTQC</p> <p>A quality control tool used in bioinformatics to assess the quality of high-throughput sequencing data, typically for next-generation sequencing (NGS) reads, like those in FASTQ files. It provides a series of quality checks on raw sequence data, helping researchers identify issues before proceeding with downstream analysis like alignment, variant calling, or transcript quantification.</p> <p>Example usage:</p> <pre><code>module load fastqc/0.12.1\nfastqc raw_fastq/Irrel_kd_1.subset.fq  # It will generate a html report \n</code></pre> <p>Fastqc output</p> <p>Check this article for how to interpret fastqc result.</p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#gtf-file","title":"GTF file","text":"<p>GTF (Gene Transfer Format) is a file format used to hold information about gene structure. It is widely used in genomics to store annotations of genomic features such as genes, exons, and regulatory elements. GTF is similar to GFF (General Feature Format) but includes additional standardized attributes. </p> <p>Structure of a GTF File:</p> <p>A GTF file is a tab-delimited text file with one line per feature. Each line consists of nine fields: </p> <ol> <li>seqname: Name of the sequence (e.g., chromosome). </li> <li>source: Name of the program that generated the feature. </li> <li>feature: Type of feature (e.g., gene, exon, CDS). </li> <li>start: Start position of the feature. </li> <li>end: End position of the feature. </li> <li>score: Score value (can be a dot if not used). </li> <li>strand: Strand of the feature (+ or -). </li> <li>frame: Frame for coding sequences (0, 1, or 2). </li> <li>attribute: A semicolon-separated list of key-value pairs describing the feature.</li> </ol> <pre><code>head reference_data/chr1-hg19_genes.gtf \n</code></pre> <pre><code>chr1    unknown exon    14362   14829   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    14970   15038   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    15796   15947   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    16607   16765   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    16858   17055   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    17233   17368   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    17606   17742   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    17915   18061   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    18268   18366   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    24738   24891   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    29321   29370   .       -       .       gene_id \"WASH7P\"; gene_name \"WASH7P\"; transcript_id \"NR_024540\"; tss_id \"TSS7245\";\nchr1    unknown exon    34611   35174   .       -       .       gene_id \"FAM138F\"; gene_name \"FAM138F\"; transcript_id \"NR_026820\"; tss_id \"TSS8099\";\nchr1    unknown exon    34611   35174   .       -       .       gene_id \"FAM138A\"; gene_name \"FAM138A\"; transcript_id \"NR_026818\"; tss_id \"TSS8099\";\nchr1    unknown exon    35277   35481   .       -       .       gene_id \"FAM138F\"; gene_name \"FAM138F\"; transcript_id \"NR_026820\"; tss_id \"TSS8099\";\nchr1    unknown exon    35277   35481   .       -       .       gene_id \"FAM138A\"; gene_name \"FAM138A\"; transcript_id \"NR_026818\"; tss_id \"TSS8099\";\nchr1    unknown exon    35721   36081   .       -       .       gene_id \"FAM138F\"; gene_name \"FAM138F\"; transcript_id \"NR_026820\"; tss_id \"TSS8099\";\nchr1    unknown exon    35721   36081   .       -       .       gene_id \"FAM138A\"; gene_name \"FAM138A\"; transcript_id \"NR_026818\"; tss_id \"TSS8099\";\nchr1    unknown CDS     69091   70005   .       +       0       gene_id \"OR4F5\"; gene_name \"OR4F5\"; p_id \"P9488\"; transcript_id \"NM_001005484\"; tss_id \"TSS13903\";\nchr1    unknown exon    69091   70008   .       +       .       gene_id \"OR4F5\"; gene_name \"OR4F5\"; p_id \"P9488\"; transcript_id \"NM_001005484\"; tss_id \"TSS13903\";\nchr1    unknown start_codon     69091   69093   .       +       .       gene_id \"OR4F5\"; gene_name \"OR4F5\"; p_id \"P9488\"; transcript_id \"NM_001005484\"; tss_id \"TSS13903\";\nchr1    unknown stop_codon      70006   70008   .       +       .       gene_id \"OR4F5\"; gene_name \"OR4F5\"; p_id \"P9488\"; transcript_id \"NM_001005484\"; tss_id \"TSS13903\";\nch\n</code></pre> <p>GTF files are essential for common bioinformatics analyses such as RNA-Seq analysis, genome annotation, and differential expression analysis. They provide the necessary annotation information to map reads to genomic features and to understand the functional elements of the genome.</p> <p>You can download human gtf file from UCSC Genome Browser  <code>https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/genes/</code> or Ensembl Genome Browser <code>https://ftp.ensembl.org/pub/release-112/gtf/homo_sapiens/</code></p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#how-to-donwload-your-own-reference-genome-file","title":"How to donwload your own reference genome file","text":"<ul> <li>Navigate to ensembl website:  <code>https://useast.ensembl.org/index.html</code></li> <li>Select the organisms you are working on. Ex: <code>Human</code></li> </ul> <ul> <li> <p>Choose the file you would like to download: FASTA, GTF or GFF3, ...</p> </li> <li> <p>Always keep an eye on the genome build. <code>GRCh38</code> is the latest for human genome. </p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#toy-analysis-with-interactive-jobs","title":"Toy analysis with interactive jobs","text":""},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#alignment-with-star","title":"Alignment with STAR","text":"<p>Make sure you are in compute node</p> <pre><code>utln@login-prod-02    # log in node\nutln@p1cmp017         # compute node\n</code></pre> <p>If not, do the following</p> <pre><code>srun -p interactive -n 1 --time=4:00:00 --mem=32g --cpus-per-task=8 --pty bash\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#load-necessary-modules","title":"Load necessary modules","text":"<pre><code>module load star/2.7.11b\n</code></pre> <p>Before aligning your FASTQ files, you need to generate an index using the reference genome (<code>.fa</code> file) and the annotation file (<code>.gtf</code> file).</p> <pre><code>STAR --runMode genomeGenerate \\\n     --genomeDir ./reference_data/ \\\n     --genomeFastaFiles ./reference_data/chr1.fa \\\n     --sjdbGTFfile ./reference_data/chr1-hg19_genes.gtf \\\n     --runThreadN 8\n</code></pre> <p>You will see output log like this </p> <pre><code>Sep 20 11:25:15 ..... started STAR run\nSep 20 11:25:15 ... starting to generate Genome files\nSep 20 11:25:20 ..... processing annotations GTF\n!!!!! WARNING: --genomeSAindexNbases 14 is too large for the genome size=249250621, which may cause seg-fault at the mapping step. Re-run geno\nme generation with recommended --genomeSAindexNbases 12\nSep 20 11:25:22 ... starting to sort Suffix Array. This may take a long time...\nSep 20 11:25:23 ... sorting Suffix Array chunks and saving them to disk...\nSep 20 11:26:40 ... loading chunks from disk, packing SA...\nSep 20 11:26:47 ... finished generating suffix array\nSep 20 11:26:47 ... generating Suffix Array index\nSep 20 11:27:52 ... completed Suffix Array index\nSep 20 11:27:52 ..... inserting junctions into the genome indices\nSep 20 11:28:17 ... writing Genome to disk ...\nSep 20 11:28:17 ... writing Suffix Array to disk ...\nSep 20 11:28:19 ... writing SAindex to disk\nSep 20 11:28:22 ..... finished successfully\n</code></pre> <p>If you do <code>ls -lhtr reference_data</code> , you will see the genome index you just generated. </p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#align-fastq-reads-to-the-reference-genome","title":"Align FASTQ Reads to the Reference Genome","text":"<p>First, let's create an folder to store output from STAR:</p> <pre><code>mkdir star_output\n</code></pre> <p>For single-end reads: </p> <pre><code>STAR --genomeDir ./reference_data/ \\\n     --readFilesIn ./raw_fastq/Irrel_kd_1.subset.fq \\\n     --outFileNamePrefix ./star_output/ \\\n     --runThreadN 8\n</code></pre> <p>In our example, we use single-end reads.</p> <p>For paired-end reads:</p> <pre><code>STAR --genomeDir ./reference_data/ \\\n     --readFilesIn /path/to/read1.fastq /path/to/read2.fastq \\\n     --outFileNamePrefix ./star_output/ \\\n     --runThreadN 8\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#output","title":"Output","text":"<p>Check the outputs:</p> <pre><code>cd star_output\nls\n</code></pre> <p>You will see the following:</p> <pre><code>Aligned.out.sam  Log.final.out  Log.out  Log.progress.out  SJ.out.tab\n</code></pre> <p><code>Aligned.out.sam</code> contains the aligned information. </p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#sam-file","title":"SAM file","text":"<p>SAM (Sequence Alignment/Map) is a text-based format for storing biological sequences aligned to a reference sequence. It is widely used in bioinformatics for storing large-scale sequencing data, such as that from next-generation sequencing technologies.</p> <p>A SAM file consists of a header section and an alignment section. </p> <ol> <li>Header Section: Optional. Contains metadata about the alignments and reference sequences. Header lines start with the '@' character. </li> <li>Alignment Section: Contains the aligned sequences and their corresponding information. Each line represents a single alignment and consists of 11 mandatory fields and optional fields.</li> </ol> <pre><code>@HD     VN:1.4\n@SQ     SN:chr1 LN:249250621\n@PG     ID:STAR PN:STAR VN:2.7.11b      CL:/usr/local/bin/STAR-avx2   --runThreadN 8   --genomeDir ./reference_data/   --readFilesIn ./raw_fastq/Irrel_kd_1.subset.fq      --outFileNamePrefix ./star_output/\n@CO     user command line: /usr/local/bin/STAR-avx2 --genomeDir ./reference_data/ --readFilesIn ./raw_fastq/Irrel_kd_1.subset.fq --outFileNamePrefix ./star_output/ --runThreadN 8\nHWI-ST330:304:H045HADXX:2:1102:9621:48866       0       chr1    115266503       255     100M    *       0       0       CCCTCCTCCACAATCTCAATCATTCCTTGGTACTCAGTCTGTGTTGGATCAACACTCCTCAGGGGGCGAATTACTTTGCCAGAGTAAATGGTGGGATCAG    CCCFFFFFHHHHHJJJJJJJJJJJJJJJJJHIIIJJJGIJJIJJJJJJJJJGJJJJJJJJIIIJJJFDDDDDDDDDEDDDDDBB&gt;CDDEEDCDDDDDDDD    NH:i:1  HI:i:1  AS:i:98 nM:i:0\nHWI-ST330:304:H045HADXX:2:1104:16192:61086      0       chr1    115266503       255     100M    *       0       0       CCCTCCTCCACAATCTCAATCATTCCTTGGTACTCAGTCTGTGTTGGATCAACACTCCTCAGGGGGCGAATTACTTTGCCAGAGTAAATGGTGGCATCAG    @BCFFFFFHHHHHJJJJJJJJJJJJJJJJJIJJIJJJIIJJJIIJJJJJJIJJHJJJJJJJJJJJJFDDDDDDDDDEDDDDDDD@CCDEED:ABDDDDDD    NH:i:1  HI:i:1  AS:i:96 nM:i:1\nHWI-ST330:304:H045HADXX:2:1107:5712:34356       0       chr1    115266503       255     100M    *       0       0       CCCTCCTCCACAATCTCAATCATTCCTTGGTACTCAGTCTGTGTTGGATCAACACTCCTCAGGGGGCGAATTACTTTGCCAGAGTAAATGGTGGGATCAG    @B@FFDFFHFFHHIGIIIGIHJJIJJJJJJCEHIJJJFGHIIJIJJJJJIIJJJJJJJIJJJJIIJFDBBBDDDDDEDDDDDDD&gt;CCDEED&gt;ADDDDDDD    NH:i:1  HI:i:1  AS:i:98 nM:i:0\nHWI-ST330:304:H045HADXX:2:1113:8407:74596       0       chr1    115266503       255     100M    *       0       0 \n</code></pre> <p>Check this article about how to interpret sam files. </p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#bam-file","title":"BAM file","text":"<p>The BAM (Binary Alignment/Map) file format is a binary version of the SAM (Sequence Alignment/Map) format. It is used to store aligned sequence data in a more efficient and compressed form.</p> <p>Convert sam file to bam file:</p> <pre><code>module load samtools/1.17\nsamtools view -S -b Aligned.out.sam &gt; Aligned.out.bam\n</code></pre> <p>Bam file cannot be read directly, we have to use samtools to view it:</p> <pre><code>module load samtools/1.17\nsamtools view Aligned.out.bam | head -n 20\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#toy-analysis-with-job-scripts","title":"Toy analysis with job scripts","text":"<p>Let's write a slurm script call <code>run_star.sh</code></p> <pre><code>#!/bin/bash\n#SBATCH -J STAR_JOB             # Job name\n#SBATCH --time=12:00:00         # Job will run for a maximum of 12 hours (D-HH:MM:SS format)\n#SBATCH -p batch                # Partition (queue) to submit the job to\n#SBATCH -n 1                    # Number of tasks (here we only need 1 task)\n#SBATCH --mem=32g               # Allocate 32 GB of memory for this job\n#SBATCH --cpus-per-task=8       # Number of CPU cores allocated for this task\n#SBATCH --output=MyJob.%j.%N.out  # Output will be saved to a file with job ID (%j) and node name (%N)\n#SBATCH --error=MyJob.%j.%N.err   # Error will be saved to a file with job ID (%j) and node name (%N)\n#SBATCH --mail-type=ALL         # Receive email notifications for job status (ALL = start, end, fail)\n#SBATCH --mail-user=utln@tufts.edu  # Your email to receive notifications\n\n# Load the STAR module\nmodule load star/2.7.11b         # Load the specific version of STAR needed for the alignment\n\nmkdir star_output\n\n# Run STAR alignment\nSTAR --genomeDir ./reference_data/ \\       # Specify the directory containing the genome index files\n     --readFilesIn ./raw_fastq/Irrel_kd_1.subset.fq \\  # Input FASTQ file(s) for alignment\n     --outFileNamePrefix ./star_output/ \\  # Output files will be saved in this directory \n     --runThreadN 8                        # Use 8 threads for faster processing\n</code></pre> <p>Here is the command to submit job</p> <pre><code>chmod +x run_star.sh    # Makes the script executable\nsbatch run_star.sh      # Submits the script to the SLURM queue\n</code></pre> <p>Use <code>squeue -u yourusername</code> to check job status. </p>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#run-job-with-gpu-node","title":"Run job with GPU node","text":""},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#interactive-session","title":"Interactive session","text":"<pre><code>srun -p preempt -n 1 --time=04:00:00 --mem=20G --gres=gpu:1 --pty /bin/bash\n</code></pre> <p>You can also specify which gpu node you would like to run jobs on </p> <pre><code>srun -p preempt -n 1 --time=04:00:00 --mem=20G --gres=gpu:a100:1 --pty /bin/bash\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/03_run_bioinformatics_on_HPC/#submit-jobs-to-queue","title":"Submit jobs to queue","text":"<p>Example script: <code>align.sh</code> using parabricks to do the alignment.</p> <pre><code>#!/bin/bash\n#SBATCH -J fq2bam_alignment          # Job name\n#SBATCH -p preempt                   # Submit to the 'preempt' partition (modify based on your cluster setup)\n#SBATCH --gres=gpu:1                 # Request 1 GPU for accelerated processing\n#SBATCH -n 2                         # Number of tasks (2 in this case)\n#SBATCH --mem=60g                    # Memory allocation (60GB)\n#SBATCH --time=02:00:00              # Maximum job run time (2 hours)\n#SBATCH --cpus-per-task=20           # Number of CPU cores allocated per task\n#SBATCH --output=alignment.%j.out    # Standard output file (with job ID %j)\n#SBATCH --error=alignment.%j.err     # Standard error file (with job ID %j)\n#SBATCH --mail-type=ALL              # Email notifications for all job states (begin, end, fail)\n#SBATCH --mail-user=utln@tufts.edu   # Email address for notifications\n\n# Load necessary modules\nnvidia-smi                              # Show GPU information (optional for logging)\nmodule load parabricks/4.0.0-1          # Load Parabricks module for GPU-accelerated alignment\n\n# Define variables\ngenome_reference=\"/path/to/reference_genome\"      # Path to the reference genome (.fasta)\ninput_fastq1=\"/path/to/input_read1.fastq\"         # Path to the first paired-end FASTQ file\ninput_fastq2=\"/path/to/input_read2.fastq\"         # Path to the second paired-end FASTQ file\nsample_name=\"sample_identifier\"                  # Sample identifier\nknown_sites_vcf=\"/path/to/known_sites.vcf\"        # Known sites VCF file for BQSR (optional, if available)\noutput_directory=\"/path/to/output_directory\"      # Directory for the output BAM file and reports\noutput_bam=\"${output_directory}/${sample_name}.bam\"            # Output BAM file path\noutput_bqsr_report=\"${output_directory}/${sample_name}.BQSR-report.txt\"  # Output BQSR report path\n\n# Run the Parabricks fq2bam alignment pipeline\npbrun fq2bam \\\n    --ref ${genome_reference} \\                # Reference genome (.fasta)\n    --in-fq ${input_fastq1} ${input_fastq2} \\  # Input paired-end FASTQ files\n    --read-group-sm ${sample_name} \\           # Sample name for read group\n    --knownSites ${known_sites_vcf} \\          # Known sites for BQSR \n    --out-bam ${output_bam} \\                  # Output BAM file\n    --out-recal-file ${output_bqsr_report}     # Output Base Quality Score Recalibration (BQSR) report\n</code></pre> <p>Here is the command to submit job</p> <pre><code>chmod +x align.sh    # Makes the script executable\nsbatch align.sh      # Submits the script to the SLURM queue\n</code></pre> <p>Use <code>squeue -u yourusername</code> to check job status. </p> <p>Useful links:</p> <p>https://hbctraining.github.io/Intro-to-shell-flipped/lessons/03_working_with_files.html</p> <p>https://hbctraining.github.io/Intro-to-rnaseq-hpc-O2/lessons/03_alignment.html </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/00_overview/","title":"Linux/Unix command line basics","text":"<p>Learning Linux is crucial for bioinformatics because the majority of bioinformatics tools, pipelines, and databases are designed to run in a Linux environment. </p> <p>Linux provides powerful command-line utilities that are essential for handling large datasets, automating repetitive tasks, and managing high-throughput sequencing data. </p> <p>Additionally, Linux offers robust support for scripting languages like Python, Perl, and R, which are widely used in bioinformatics for data analysis and visualization. </p> <p>The open-source nature of Linux ensures that researchers have access to cutting-edge tools and can customize their computational environment to meet the specific needs of their projects. </p> <p>Mastering Linux not only enhances efficiency and productivity but also enables bioinformaticians to fully leverage the computational resources available in modern research environments.</p> <p>In this section, we will briefly introduce the history and features of Linux/Unix, file systems, and useful tools for bioinformatics analysis. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/00_overview/#overview","title":"Overview","text":"<ul> <li>Intro to Linux/Unix</li> <li>Files and File system</li> <li>Must-know Linux/Unix tools</li> <li>Conditional Statements and Loops</li> <li>Advanced Linux/Unix tools</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/01_what_is_linux/","title":"Introduction to Linux/Unix","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/01_what_is_linux/#what-is-unix","title":"What is Unix?","text":"<p>Unix is a powerful, multiuser, multitasking operating system that was originally developed in the 1960s and 1970s at Bell Labs by Ken Thompson, Dennis Ritchie, and others. It has since become one of the most influential operating systems in the history of computing, serving as the foundation for many modern operating systems, including Linux, macOS, and various BSDs. Dennis Ritchie is also the creator of C. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/01_what_is_linux/#key-features","title":"Key features","text":"<ul> <li>Multiuser: Unix allows multiple users to access the system simultaneously, each with their own environment, files, and processes.</li> <li>Multitasking:  Unix is capable of running multiple processes at the same time. This means that it can handle several tasks concurrently, like running applications, performing background tasks, and processing commands.</li> <li>Hierarchical File System: Unix uses a hierarchical file system structure, where files are organized in directories (folders), starting from a root directory (<code>/</code>). </li> <li>Simple and Consistent Interface: Unix provides a command-line interface (CLI) where users interact with the system by typing commands. The Unix philosophy emphasizes small, simple, and modular commands that can be combined in scripts to perform complex tasks.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/01_what_is_linux/#what-is-linux","title":"What is Linux?","text":"<p>Linux is a free, open-source, and Unix-like operating system kernel that was originally created by Linus Torvalds in 1991. Over time, Linux has grown into a full-fledged operating system used worldwide across various types of devices, from servers and desktop computers to smartphones and embedded systems.</p> <p></p> <p>Popular Linux Distributions:</p> <p>\u200b   \u2022   Ubuntu: A user-friendly distribution popular for desktop and server use, based on Debian.</p> <p>\u200b   \u2022   Fedora: A cutting-edge distribution often used by developers and those who want the latest features.</p> <p>\u200b   \u2022   Debian: Known for its stability and extensive software repositories, often used in server environments.</p> <p>\u200b   \u2022   CentOS/AlmaLinux/Rocky Linux: Enterprise-grade distributions derived from Red Hat Enterprise Linux (RHEL).</p> <p>\u200b   \u2022   Arch Linux: A rolling release distribution known for its simplicity and customization, aimed at advanced users.</p> <p>\u200b   \u2022   Kali Linux: A distribution designed for penetration testing and security research.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/01_what_is_linux/#shell","title":"Shell","text":"<p>Shell is like a translator and bridge between you and the operating system's core, the kernel. It takes the commands you type and interprets them, telling the kernel what actions to perform. </p> <p>Tufts HPC only supports <code>bash</code>, short for Bourne-Again SHell.</p> <p></p> <p>Previous: Overview  </p> <p>Next: Files and File system</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/","title":"Files and File System","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/#everything-is-a-file","title":"Everything is a file","text":"<p>A file is an addressable location that contains some data which can take many forms.</p> <ul> <li> <p>Text data </p> </li> <li> <p>Binary/Image data</p> </li> </ul> <p>Files have associated meta-data </p> <ul> <li> <p>Owner</p> </li> <li> <p>Group</p> </li> <li> <p>Timestamps</p> </li> <li> <p>Permission:</p> <ol> <li>Read(r)</li> <li>Write(w)</li> <li>Execute(x)</li> <li>no permission(-)</li> </ol> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/#file-permissions","title":"File permissions","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/#file-names","title":"File names","text":"<ul> <li> <p>Case-sensitive: myfile.txt is different from MyFile.txt.</p> </li> <li> <p>Hidden files: Filenames starting with a dot (.) are hidden by default.</p> </li> <li> <p>File extensions: Not mandatory, but using them helps identify file types.</p> </li> <li> <p>Spaces: Allowed but not recommended, use _ or \u2013 instead.</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/#best-practices-of-file-names","title":"Best practices of file names","text":"<ul> <li> <p>Keep names descriptive and concise.</p> </li> <li> <p>Use lowercase for consistency.</p> </li> <li> <p>Avoid special characters unless necessary.</p> </li> <li> <p>Stick to alphanumeric characters, underscores, and hyphens.</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/02_files/#file-organization","title":"File organization","text":"<p>Everything is mounted to the root directory </p> <p>Files are referred to by their location called path </p> <ul> <li> <p>Absolute Path (From the root): /cluster/tufts/mylab/user01</p> </li> <li> <p>Relative Path (From my current location)\uff1auser01</p> </li> </ul> <p></p> <p>Previous: Linux/Unix  </p> <p>Next: Commands</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/","title":"Must-known Linux/Unix Tools","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#file-and-directory-management","title":"File and Directory Management","text":"<p>Linux provides powerful tools for managing files and file systems. Here we will introduce a few essential commands. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#pwd-print-the-current-working-directory","title":"pwd: print the current working directory","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage","title":"Usage","text":"<pre><code>$ pwd\n/cluster/home/yzhang85\n$ cd /cluster/tufts/rt/yzhang85/\n$ pwd\n/cluster/tufts/rt/yzhang85\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#cd-change-directory","title":"cd: change directory","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_1","title":"Usage","text":"<p><pre><code>cd [directory]\n</code></pre> If a directory is not supplied as an argument, it will default to your home directory.  <pre><code>$ pwd\n/cluster/tufts/rt/yzhang85\n$ cd ..\n$ pwd\n/cluster/tufts/rt\n$ cd  \n$ pwd\n/cluster/home/yzhang85\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#shortcuts","title":"Shortcuts","text":"<ul> <li>..:   cd to the parent directory. cd ..</li> <li>~:    cd to the home directory.   cd ~</li> <li>-:    cd to the previous directory.   cd -</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#ls-list-all-the-files-in-the-given-directory","title":"ls: list all the files in the given directory","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_2","title":"Usage","text":"<pre><code>ls [options] [directory]\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-options","title":"Common options:","text":"<ul> <li>-1: list each file/directory on a separate line</li> <li>-l: lists files/directories with their most common metadata </li> <li>-a: include hidden files /directories (files\u2019 name begins with a dot .)</li> <li>-h: print size of files/directories in human readable format </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#chmod-manage-file-permissions","title":"chmod: manage file permissions","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#symbolic-notation","title":"Symbolic Notation","text":"<ul> <li>u: user (owner)</li> <li>g: group</li> <li>o: others</li> <li>a: all (user, group, others)</li> <li>+: add permission</li> <li>-: remove permission</li> <li>=: set exact permission</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#examples","title":"Examples","text":"<pre><code>$ chmod g+w filename ## Give the group write permission \n$ chmod u+x filename ## Give user execute permission\n$ chmod a+r filename ## Give all users read access\n$ chmod u=rw,g=r,o=r filename ## Give user read and write permission, group and other only read permission.\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#recursive-updating-permissions-with-r","title":"Recursive updating permissions with -R","text":"<p>To apply permissions recursively to all files and subdirectories within a directory, use the -R option: <pre><code>$ chmod -R g+rx /path/to/directory\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#touch-create-new-files-or-update-timestamps","title":"touch: create new files or update timestamps","text":"<p>touch is used to create new files or to update the timestamps (access and modification times) of existing files. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#create-new-file","title":"Create new file","text":"<pre><code>touch newfile.txt\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#update-timestamps-of-existing-files","title":"Update timestamps of existing files","text":"<pre><code>touch existingfile.txt\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#mkdir-create-new-directory","title":"mkdir: create new directory","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_3","title":"Usage","text":"<pre><code>mkdir [options] dir_name\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option","title":"Common option","text":"<ul> <li>-p: Creates parent directories if they don't exist.</li> </ul> <p><pre><code>$ mkdir -p rnaseq/output \n</code></pre> This will create <code>output</code> folder as well as its parent folder <code>rnaseq</code> if it doesn't exist.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#mv-move-a-filedirectory-to-a-new-location-or-rename-it","title":"mv: move a file/directory to a new location or rename it","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_4","title":"Usage","text":"<pre><code>mv [options] source destination\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_1","title":"Common option","text":"<ul> <li>-i: Prompts for confirmation before overwriting an existing file. Useful to avoid accidental data loss.</li> <li>-f: Forces the operation without prompting, even if an existing file would be overwritten. Use with caution!</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#cp-copy-a-filedirectory","title":"cp: copy a file/directory","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_5","title":"Usage","text":"<pre><code>cp [options] source destination\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_2","title":"Common option","text":"<ul> <li>-r:  To copy directory</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#rm-remove-filesdirectories","title":"rm: remove files/directories","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_6","title":"Usage","text":"<pre><code>rm [options] file/directory\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_3","title":"Common option","text":"<ul> <li>-r: Deletes recursively any file and subdirectories contained within the given directory.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#text-processing","title":"Text processing","text":"<p>Linux command-line tools are invaluable for bioinformatics text processing due to their efficiency and flexibility. They allow for rapid manipulation and analysis of large biological datasets, such as DNA sequences, protein structures, and gene expression data. Commands like <code>grep</code>, <code>sed</code>, <code>awk</code>, and <code>cut</code> are essential for filtering, extracting, and reformatting text-based biological information.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#cat-catenate-filesjoins-their-contents","title":"cat: catenate files(joins their contents)","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_7","title":"Usage","text":"<pre><code>cat [options] file1 file2 \u2026\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_4","title":"Common option","text":"<ul> <li>-n: tells cat to number each line of the output. This is helpful for debugging scripts.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#headtail-display-the-beginningend-of-a-file","title":"head/tail: display the beginning/end of a file","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_8","title":"Usage","text":"<pre><code>head/tail [options] file\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_5","title":"Common option","text":"<ul> <li>-n [number]: Specifies the number of lines to display (default: 10).</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#lessmore-view-the-content-of-a-file-page-by-page","title":"less/more: view the content of a file page by page","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_9","title":"Usage","text":"<pre><code>less largefile.txt\nmore largefile.txt\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#navigation-with-less","title":"Navigation with less","text":"<ul> <li>Arrow Keys: Use the up and down arrow keys to scroll line by line.</li> <li>Spacebar: Move forward one page.</li> <li>b: Move backward one page.</li> <li>q: Quit and exit less.</li> <li>/search_term: Search for search_term within the file. Press n to go to the next occurrence.</li> <li>g: Go to the beginning of the file.</li> <li>G: Go to the end of the file.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#navigation-with-more","title":"Navigation with more","text":"<ul> <li>Spacebar: Move forward one page.</li> <li>Enter: Move forward one line.</li> <li>q: Quit and exit more.</li> <li>/search_term: Search for search_term within the file (forward only).</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#cut-extract-sections-from-each-line-of-files","title":"cut: Extract sections from each line of files","text":"<p>cut is a text-processing utility used to extract specific sections of lines from a file or standard input. It is commonly used to split lines of text based on delimiters, extract columns of data, and work with fixed-width fields. The cut command is helpful for manipulating text files like TSVs and CSVs.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_10","title":"Usage","text":"<pre><code>cut [OPTIONS] [FILE...]\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-options_1","title":"Common options","text":"<ul> <li>-d: Specifies the delimiter for field extraction. Default is TAB. Example: -d, (use a comma as the delimiter).</li> <li>-f: Selects specific fields, used with a delimiter (default is TAB). Example: -f 1,3 (extract fields 1 and 3).</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#example","title":"Example","text":"<pre><code>cut -f1,3 -d, file.csv ##(Extract columns 1 and 3 from a comma-separated file)\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sort-sort-lines-of-text-files","title":"sort: Sort lines of text files","text":"<p><code>sort</code> is designed to sort plain-text data with columns. Running sort without any arguments simply sorts a file alphabetically.  By default, sort treats blank characters (like tab or spaces) as field delimiters. If your file uses another delimiter (such as a comma for CSV files), you can specify the field separator with -t (e.g., -t \",\"). </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_11","title":"Usage","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sort-lines-alphabetically","title":"Sort lines alphabetically","text":"<pre><code>sort file.txt\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sort-lines-numerically","title":"Sort lines numerically","text":"<pre><code>sort -n file.txt\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sort-by-a-specific-column","title":"Sort by a specific column","text":"<pre><code>sort -k 2 file.txt  # Sort by the second column\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sort-by-multiple-columns","title":"Sort by multiple columns","text":"<pre><code>sort -k 1,2 file.txt  # Sort by the first column, then the second\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#uniq-report-or-filter-out-repeated-lines-in-a-file","title":"uniq: Report or filter out repeated lines in a file.","text":"<p>uniq is used to filter out or identify duplicate lines in a sorted file. It removes adjacent duplicate lines, so it\u2019s often combined with the <code>sort</code> command to find unique lines in a file effectively.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_12","title":"Usage","text":"<pre><code>sort file.txt | uniq\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#grepextracting-lines-matching-not-matching-a-pattern","title":"grep:Extracting lines matching (not matching) a pattern","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_13","title":"Usage","text":"<p><code>grep [options] PATTERN file</code></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#common-option_6","title":"Common option","text":"<ul> <li>-i: ignore cases</li> <li>-v: select non-matching lines.</li> <li>-A NUM: Print NUM lines of trailing context after matching lines.</li> <li>-B NUM: Print NUM lines of leading context before matching lines.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#sed-stream-editor-for-modifying-file-content","title":"sed: Stream editor for modifying file content","text":"<p>sed (short for stream editor) is a powerful text-processing tool in Bash that allows you to parse and transform text in files or streams. It is commonly used to perform basic text manipulations like search and replace, insert and delete lines, and apply regular expressions on text data.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#substitution-search-and-replace","title":"Substitution (Search and Replace)","text":"<p>Replace the first occurrence of old with new in each line: <pre><code>sed 's/old/new/' filename.txt\n</code></pre> Replace all occurrences of old with new in each line: <pre><code>sed 's/old/new/g' filename.txt\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#in-place-substitution","title":"In-place substitution","text":"<p><pre><code>sed -i 's/old/new/g' filename.txt\n</code></pre> Warning: Use this command with caution as it directly modifies the original file. To create a backup, use <code>-i.bak</code>: <pre><code>sed -i.bak 's/old/new/g' filename.txt\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#delete-lines","title":"Delete lines","text":"<pre><code>sed '/pattern/d' filename.txt\n</code></pre> <p><code>sed</code> and <code>awk</code> that we will introduce later are very powerful bash commands. If you have interest in learning more about their usage, below is a very good book. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#data-compression-and-archiving","title":"Data Compression and Archiving","text":"<p>When working with files on Linux, compressing them to save space and bundling multiple files into a single archive is a common practice. The commands gzip, gunzip, and tar are essential tools for file compression and archiving in Bash.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#gzip-compress-files","title":"gzip: Compress files","text":"<p>gzip is used to compress files in Linux. It reduces file sizes using the DEFLATE algorithm, resulting in files with the <code>.gz</code> extension.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_14","title":"Usage","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#compress-filefiles","title":"Compress file/files","text":"<pre><code>gzip file.txt ## Compress file.txt and replace it with file.txt.gz.\ngzip file1.txt file2.txt file3.txt ## Each file will be compressed and replaced with a .gz version.\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#keep-the-original-file","title":"Keep the Original File","text":"<pre><code>gzip -k file.txt ## Create file.txt.gz while preserving the original file.txt.\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#compress-the-folder","title":"Compress the folder","text":"<pre><code>gzip -r directory_name\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#gunzip-decompress-gz-files","title":"gunzip: decompress .gz files","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#decompress-files","title":"Decompress files","text":"<pre><code>gunzip file.txt.gz\ngunzip file1.txt.gz file2.txt.gz\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#keep-the-compressed-file","title":"Keep the Compressed File","text":"<pre><code>gunzip -k file.txt.gz\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#tar-archive-multiple-files-into-one-or-extract-them","title":"tar: Archive multiple files into one or extract them.","text":"<p>tar is used to create, extract, and manipulate archive files. However, tar itself does not compress files; it only archives them by combining multiple files and directories into a single file. This file usually has a <code>.tar</code> extension. However, tar can be used in combination with other compression utilities (like <code>gzip</code> or <code>bzip2</code>) to compress the archive.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#create-a-tar-archive-without-compression","title":"Create a .tar archive without compression","text":"<pre><code>tar -cvf archive.tar my_folder\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#extract-a-tar-file","title":"Extract a tar file","text":"<pre><code>tar -xvf archive.tar\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#creating-a-compressed-archivetargz","title":"Creating a compressed archive(.tar.gz)","text":"<pre><code>tar -cvzf archive.tar.gz my_folder\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#extracting-a-compressed-archivetargz","title":"Extracting a compressed archive(.tar.gz)","text":"<pre><code>tar -xvzf archive.tar.gz\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#other-useful-tools","title":"Other useful tools","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#environment-variables","title":"Environment variables","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#define-variables","title":"Define variables:","text":"<pre><code>VARIABLE=value    ## No space around =\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#variable-reference","title":"Variable reference","text":"<pre><code>$VARIABLE     ## echo $VARIABLE\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#commonly-used-environment-variables","title":"Commonly used environment variables","text":"<ul> <li>$USER: the login user</li> <li>$HOME: the home directory</li> <li>$PWD: the current directory</li> <li>$PATH: A list of directories which will be checked for executable files </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#redirection","title":"Redirection: &gt;, &gt;&gt;, &lt;","text":"<ul> <li> <p><code>&gt;</code>: Overwrites the contents of a file with the command's output</p> <p><code>cat file1 file2 &gt; files</code></p> </li> <li> <p><code>&gt;&gt;</code>: Appends the output to the end of an existing file</p> </li> </ul> <p>\u200b     <code>cat file3 &gt;&gt; files</code></p> <ul> <li><code>&lt;</code>: Uses the contents of a file as input to a command</li> </ul> <p>\u200b     <code>sort &lt; names.txt</code></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#pipe","title":"Pipe: |","text":"<p>Pipes in Linux are a powerful feature that allows you to connect the output of one command directly as the input to another command. This is a key concept in Unix/Linux philosophy, which promotes the use of small, modular tools that can be combined to perform complex tasks.</p> <p>A pipe is represented by the <code>|</code> symbol. When you place a pipe between two commands, the standard output (<code>stdout</code>) of the command on the left of the pipe becomes the standard input (<code>stdin</code>) for the command on the right.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_15","title":"Usage","text":"<pre><code>command1 | command2\n</code></pre> <pre><code>sort file.txt | uniq\n</code></pre> <ul> <li> <p>sort file.txt: Sorts the lines in file.txt.   </p> </li> <li> <p>uniq: Removes duplicate lines from the sorted output.</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#wildcards-selecting-multiple-filesdirectories-based-on-patterns","title":"Wildcards: selecting multiple files/directories based on patterns","text":"<ul> <li> <p>*: Represents zero or more characters:  </p> <ul> <li>*.fastq.gz  matches all fastq.gz files</li> </ul> </li> <li> <p>?: Represents a single character:</p> <ul> <li>file?.txt matches \"file1.txt\", \"fileA.txt\", but not \"file12.txt\".</li> </ul> </li> <li> <p>[]: Represents a single character within a specified range or set:</p> <ul> <li>[abc]at matches \"bat\", \"cat\", or \"aat\".</li> <li>[0-9] matches any single digit.</li> </ul> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#alias","title":"Alias","text":"<p>An alias in Linux is a custom shortcut or abbreviation for a command or a series of commands. Once defined, you can use the alias in place of the original command.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#creating-an-alias","title":"Creating an alias","text":"<p>To create an alias, use the alias command followed by the name you want to give the alias and the command it should execute. <pre><code>alias alias_name='command'\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#example_1","title":"Example","text":"<pre><code>alias ll='ls -l'\nalias la='ls -a'\nalias mav='module avail'\nalias ml='module load'\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#using-alias","title":"Using alias","text":"<pre><code>[yzhang85@login-prod-03 MPI]$ ll\ntotal 31\n-rwxrwx--- 1 yzhang85 yzhang85 16256 May 16 07:57 hello_mpi*\n-rw-rw---- 1 yzhang85 yzhang85   731 May 16 07:56 hello_mpi.c\n-rwxrwxr-x 1 yzhang85 yzhang85  8392 Dec 23  2023 hello_world*\n-rwxrwxr-x 1 yzhang85 yzhang85   731 Oct 31  2023 hello_world.c*\n-rwxrwxr-x 1 yzhang85 yzhang85  4053 Oct 31  2023 mpi_hello_world.c*\n\n[yzhang85@login-prod-03 MPI]$ mav blast\n-------------------------------- /opt/shared/Modules/modulefiles-rhel6 --------------------------\n   blast/2.2.24    blast/2.2.31    blast/2.3.0    blast/2.8.1\n\n------------------------------- /cluster/tufts/hpc/tools/module ---------------------------------\n   blast-plus/2.11.0    ncbi-magicblast/1.5.0\n\n------------------------------- /cluster/tufts/biocontainers/modules -----------------------------\n   blast/2.15.0 (D)\n\n  Where:\n   D:  Default Module\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#ln-s-softlink","title":"ln -s: softlink","text":"<p>A soft link (also known as a symbolic link or symlink) is a type of file in Linux that points to another file or directory. It\u2019s essentially a shortcut that references the location of another file, allowing you to access it from a different location in the filesystem.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#usage_16","title":"Usage","text":"<p>To create a soft link, you use the <code>ln</code> command with the <code>-s</code> option: <pre><code>ln -s target_file link_name\n</code></pre></p> <ul> <li>target_file: The file or directory you want to link to.</li> <li>link_name: The name of the symlink that will point to the target.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/03_basictools/#text-editor","title":"Text Editor","text":"<p>Previous: Files and File system </p> <p>Next: Loops</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/","title":"Conditional Statements and Loops","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#for","title":"for","text":"<p>Using <code>for</code> loop, we can execute a set of commands for a finite number of times for every item in a list.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#syntax","title":"Syntax","text":"<pre><code>for VARIABLE in LIST\ndo\n  command1\n  command2\ndone\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#example","title":"Example","text":"<pre><code>for query in /cluster/tufts/Yourlab/input/*.fasta\ndo\n    echo \"Running BLAST for $query\"\n    blastn -query $query -db /path/to/database -out ${query%.fasta}_blast_results.txt -outfmt 6\ndone\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#while","title":"while","text":"<p>The command next to <code>while</code> is evaluated. If it is evaluated to be true, then the commands between <code>do</code> and <code>done</code> are executed.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#syntax_1","title":"Syntax","text":"<pre><code>while [ condition ]\ndo\n   command1\n   command2\n   command3\ndone\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#example_1","title":"Example","text":"<pre><code>#!/bin/bash\n\n# File containing the list of sequence files\nfile_list=\"sequence_files.txt\"\n# Path to the BLAST database\nblast_db=\"/path/to/blast_database\"\n\n# Read the list of files line by line\nwhile IFS= read -r sequence_file\ndo\n    # Check if the file exists\n    if [ -f \"$sequence_file\" ]; then\n        echo \"Processing $sequence_file\"\n        # Run BLAST on the current sequence file\n        blastn -query \"$sequence_file\" -db \"$blast_db\" -out \"${sequence_file%.fasta}_blast_results.txt\" -outfmt 6\n        echo \"Finished processing $sequence_file\"\n    else\n        echo \"File $sequence_file not found\"\n    fi\ndone &lt; \"$file_list\"\n\necho \"All files have been processed.\"\n</code></pre> <p>The content of <code>sequence_files.txt</code> is the name of all of the query fasta files:  <pre><code>sequence1.fasta\nsequence2.fasta\nsequence3.fasta\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#if-conditionals-in-a-bash-script","title":"if: conditionals in a bash script","text":"<p>In bioinformatics, if statements are used to make conditional decisions in scripts, such as checking whether a file or result already exists before performing data analysis or processing tasks, thus optimizing workflows and avoiding redundant computations.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#syntax_2","title":"Syntax","text":"<pre><code>if [ commands ]\nthen\n  [ if-statements ]\nelif [commands]\n  [ elif-statements ]\nelse\n  [ else-statements ]\nfi\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/04_loops/#example_2","title":"Example","text":"<pre><code>#!/bin/bash\n\n# Define the paths for input files and output file\ninput_bam=\"/path/to/genomic_data.bam\"\noutput_vcf=\"/path/to/output_directory/variants.vcf\"\n\n# Check if the output VCF file already exists\nif [ -f \"$output_vcf\" ]; then\n    echo \"Output file $output_vcf already exists. Skipping the variant calling step.\"\nelse\n    echo \"Output file $output_vcf does not exist. Running variant calling.\"\n\n    # Run the variant calling tool (e.g., bcftools)\n    bcftools mpileup -Ou -f /path/to/reference_genome.fasta \"$input_bam\" | \\\n    bcftools call -mv -Ov -o \"$output_vcf\"\n\n    echo \"Variant calling complete. Results saved to $output_vcf\"\nfi\n</code></pre> <p>This if statement ensures that the variant calling process is skipped if the output file already exists, preventing unnecessary computations and saving processing time. This approach is useful in bioinformatics workflows where redundant analyses can be avoided by checking for existing results.</p> <p>Previous: Commands </p> <p>Next: Advanced tools</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/","title":"Advanced Linux/Unix Tools","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#awk","title":"Awk","text":"<p>Awk is a powerful text-processing tool in Unix/Linux that allows you to manipulate and analyze text files and streams. It\u2019s named after its creators (Aho, Weinberger, and Kernighan) and is commonly used for pattern scanning, processing, and reporting.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#syntax","title":"Syntax","text":"<pre><code>awk 'pattern { action }' file\n</code></pre> <ul> <li> <p>pattern: A condition or regular expression that decides which lines are selected for processing. Patterns are  similar to  <code>if</code> statements in other languages: if the pattern\u2019s expression evaluates to true or the regular expression matches, the statements inside action will run. If we omit the pattern, Awk will run the action on all records.</p> </li> <li> <p>action: Commands to be executed on the selected lines. If we omit the action but specify a pattern, Awk will print all records that match the pattern. </p> </li> <li> <p>file: The text file to be processed.</p> </li> </ul> <p>Awk processes input data a record at a time. Each record is composed of fields, separate chunks that awk automatically separates. Because awk was designed to work with <code>tabular data</code> each record is a line, and each field is a column\u2019s entry for that record. The clever part about awk is that it automatically assigns the entire record to the variable <code>$0</code>, and field one\u2019s value is assigned to <code>$1</code>, field two\u2019s value is assigned to <code>$2</code>, field three\u2019s value is assigned to <code>$3</code>, and so forth. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#example","title":"Example","text":"<pre><code>$ head -n 6 deseq2.results.tsv \ngene_id baseMean    log2FoldChange  lfcSE   pvalue  padj\nENSG00000000003 782.8404    -0.06662793 0.05691688  0.2130615   0.3800412\nENSG00000000419 746.4319    -0.1265914  0.06250695  0.02873058  0.07811519\nENSG00000000457 100.9565    0.1006396   0.14143475  0.3202419   0.5071726\nENSG00000000460 307.3198    0.1057537   0.08803984  0.1649367   0.3151837\nENSG00000000971 263.8175    -1.865821   0.11722751  6.524737e-59    4.470018e-57\n\n$ awk 'BEGIN { FS=\"\\t\"; OFS=\"\\t\" } $6 &lt; 0.05 &amp;&amp; $3 &gt; 1 { print $1, $3, $6 }' deseq2.results.tsv \n</code></pre> <ul> <li>BEGIN { FS=\"\\t\"; OFS=\"\\t\" }: Sets the input (<code>FS</code>) and output (<code>OFS</code>) field separators to tab (<code>\\t</code>) since your file is a TSV (tab-separated values).</li> <li>$6 &lt; 0.05 &amp;&amp; $3 &gt; 1: Filters rows where the padj (6th column) is less than 0.05 and the log2FoldChange (3rd column) is greater than 1.</li> <li>{ print $1, $3, $6 }: Prints the gene_id (1st column), log2FoldChange (3rd column), and padj (6th column).</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#gnu-parallel","title":"GNU Parallel","text":"<p>GNU Parallel is a command-line tool designed to execute shell commands or scripts in parallel on a local or remote system. It is especially useful for bioinformatics, data processing, and other fields that involve repetitive command execution, as it can significantly speed up tasks by utilizing multiple CPU cores.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#load-module","title":"Load module","text":"<pre><code>$ module load parallel/20240822\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#basic-syntax","title":"Basic syntax","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#triple-colon","title":"Triple colon:::","text":"<pre><code>$ parallel [options] command ::: arguments\n</code></pre> <ul> <li>::: arguments: The list of arguments passed to the command. Each argument is passed to the command in parallel.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#quad-colon","title":"Quad colon::::","text":"<pre><code>$ parallel [options] command :::: input_file\n</code></pre> <p><code>-a</code> is alternative syntax to quadruple colon. <pre><code>$ parallel [options] command -a input_file\n</code></pre></p> <p>Run <code>command</code> in parallel for each line in input_file</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#pipe","title":"Pipe","text":"<p><pre><code>$ command1 | parallel [options] command2\n</code></pre> standard output from command1 as argument</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/05_advanced_tools/#example_1","title":"Example","text":"<pre><code>parallel -j N \"fastqc {}\" ::: *.fastq.gz\n</code></pre> <ul> <li>-j N:\u00a0Specifies the number of parallel jobs to run (replace N with the desired number,\u00a0considering available CPU cores).</li> <li>\"fastqc {}\":\u00a0The FastQC command to execute in parallel,\u00a0with\u00a0{}\u00a0representing each input file.</li> <li>::::\u00a0Separates the command from the list of files.</li> <li>*.fastq.gz:\u00a0Wildcard pattern to match all FASTQ files with the *.fastq.gz extension in the current directory.\u00a0Modify as needed for different file extensions or locations.</li> </ul> <p>Highly recommeded to read this article written by the developer Ole Tange in Biostars. </p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/","title":"Overview","text":"<p>This tutorial provides step-by-step instructions on setting up SSH keys for secure and password-free access to Tufts HPC, enhancing both convenience and security.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#ssh-keys","title":"SSH Keys","text":"<p>SSH keys provide a secure, password-free method to access remote systems, such as Tufts HPC. This tutorial will guide you through setting up an SSH key for simpler and safer connections.</p>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#steps-to-connect-to-tufts-hpc-using-ssh-keys","title":"Steps to connect to Tufts HPC using SSH keys:","text":""},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#1-generate-ssh-key-pair","title":"1. Generate SSH Key Pair","text":"<p>Start by creating a private and a public key on your local machine:</p> <p>Mac, Linux, and Windows Subsystem for Linux (WSL):</p> <ul> <li> <p>Open the terminal. Do not log in to Tufts server </p> </li> <li> <p>Run: <code>ssh-keygen</code></p> </li> <li> <p>You will be prompted to enter a file name and a passphrase:</p> </li> <li> <p>Filename: Hit Enter to use the default location (<code>~/.ssh/id_rsa</code>).</p> <ul> <li>Your default location may be different. Here is an example:</li> </ul> <pre><code>Generating public/private ed25519 key pair.\nEnter file in which to save the key (/Users/james/.ssh/id_ed25519): \n</code></pre> <p>If this is the case, then <code>.ssh/id_ed25519</code> is where your key file is. </p> </li> <li> <p>Passphrase: Set a passphrase for added security. Important: Do not skip the passphrase. It protects your private key from unauthorized use if your local machine is compromised. </p> <pre><code>Enter passphrase (empty for no passphrase): \n</code></pre> <p>Ex: You can try something like <code>tuftshpc</code> </p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#2-copy-public-key-to-cluster","title":"2. Copy Public Key to Cluster","text":"<p>Transfer your public key to the Tufts HPC to allow secure access:</p> <ul> <li>Run the following command:</li> </ul> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub myusername@login.pax.tufts.edu\n</code></pre> <p>Remember: This is still on your local computer, do not log in to the server </p> <ul> <li> <p>Replace <code>myusername</code> with your Tufts username.</p> </li> <li> <p>Alternative: If <code>ssh-copy-id</code> isn't available:</p> </li> </ul> <pre><code>cat ~/.ssh/id_rsa.pub | ssh myusername@login.pax.tufts.edu \"mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh &amp;&amp; cat &gt;&gt; ~/.ssh/authorized_keys\"\n</code></pre> <ul> <li>When prompted, enter your Tufts password and approve the Duo login notification.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#3-test-ssh-connection","title":"3. Test SSH Connection","text":"<p>Verify that you can SSH from your local computer to the cluster without a Tufts password:</p> <p>Execute: <code>ssh myusername@login.pax.tufts.edu</code></p> <p>You will see prompt something like this:</p> <pre><code>Enter passphrase for key '/Users/username/.ssh/id_ed25519': \n</code></pre> <p>Or</p> <pre><code>Enter passphrase for key '/Users/username/.ssh/id_rsa': \n</code></pre> <p>You should now be able to log in using just your passphrase, bypassing the need for your Tufts password and Duo approval. </p> <p>Ex: </p> <p>If your passphrase is set to <code>tuftshpc</code>, then just enter</p> <pre><code>Enter passphrase for key '/Users/username/.ssh/id_rsa': tuftshpc\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics201/linux/ssh_key/#4-additional-steps-for-custom-key-names-or-locations","title":"4. Additional Steps for Custom Key Names or Locations","text":"<p>If your private key is not named <code>id_rsa</code> or located in the default directory, specify its location when connecting:</p> <pre><code>ssh -i path/to/my_private_key myusername@login.pax.tufts.edu\n</code></pre> <p>In our example:</p> <p><code>path/to/my_private_key</code> = <code>/Users/username/.ssh/id_rsa</code></p>"},{"location":"2024_workshops/2024_bioinformatics301/00_introduction/","title":"Software Installation and Environment Management in HPC","text":"<p>For bioinformatics users, learning how to install software on HPC is very useful. </p> <p>Bioinformatics workflows often require specialized software tools, many of which have complex dependencies or need to be optimized for the HPC environment. Installing and managing these tools on an HPC system allows bioinformatics users to access the latest versions of key software. </p> <p>Understanding software installation on HPC ensures that users can customize their environments, streamline their analyses, and maintain reproducibility in their research, which is critical in bioinformatics given the rapid pace of data generation and evolving computational tools.</p> <p>In this workshop, we will introduce how to install bioinformatics softwares on HPC using different ways. </p>"},{"location":"2024_workshops/2024_bioinformatics301/00_introduction/#agenda","title":"Agenda","text":"<ul> <li>Application Installation from Source Codes</li> <li>R package Installation</li> <li>Conda Environment Management, Jupyer Kernel and Modules</li> <li>Simplying Conda Environment Management with conda-env-mod</li> </ul> <p>Containerization is an easier and recommended way</p> <p>In this workshop, we will focus on R package installation and package installation from source codes or using a Conda environment. Actually, there is an easier way to run applications on clusters without any installation steps; this method involves containerization using Singularity/Apptainer. We will not introduce containerization in this workshop but plan to conduct a container workshop in Spring 2025. If you are interested in containers, you can check out our Spring, 2024 container workshop.</p>"},{"location":"2024_workshops/2024_bioinformatics301/00_introduction/#presenters","title":"Presenters","text":"<sub>Shirley Li</sub> <sub>Yucheng Zhang</sub> <p>Next: source</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/","title":"Installation from source codes","text":""},{"location":"2024_workshops/2024_bioinformatics301/01_source/#understanding-make-cmake-and-software-installation-on-hpc","title":"Understanding Make, CMake, and Software Installation on HPC","text":"<p>This guide will help you understand how GNU Make and CMake are used in software installation, particularly for bioinformatics applications. These tools are essential for managing the building and compiling of programs from source code. Additionally, you will learn about specific software installation instructions for bioinformatics tools like BWA, HMMER, and RegTools on a High-Performance Computing (HPC) environment.</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#what-is-make","title":"What is Make?","text":"<p>GNU Make is a program often used for compiling software. It uses a plain text file named makefile or Makefile, which lists each of the non-source files and how to compute it from other files.</p> <p><code>make</code> and <code>Makefile</code> are also widely used in building reproducible workflows. This ariticle is a good introduction.</p> <p>What is complier? </p> <p>A compiler is a program that translates source code written in a high-level programming language (such as C, C++, or Java) into machine code or bytecode that a computer's processor can understand and execute. This process involves several steps, including lexical analysis, syntax analysis, optimization, and code generation.</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#steps-for-software-installation-using-make","title":"Steps for software installation using make","text":"<ol> <li> <p>Unpack the source code archive. </p> </li> <li> <p>Configure the package.                                # Some packages do not have the configure file</p> </li> <li> <p>Run make to build the programs. </p> </li> <li> <p>Run make install to install the package.  # Optional</p> </li> </ol> <p>\u274c Do not run sudo make install</p> <p>Tip: By default, make install will install applications into <code>/usr/local</code>, but regular users do not have permission to write into <code>/usr/local</code>. </p> <p>The best way is to install applications into your home directory or your group's shared directory by passing the option <code>--prefix=TargetDirName</code> to <code>./configure</code>. </p> <p>What is configuration?</p> <p>Configuration refers to the arrangement or setup of various components and settings within a system, software, or device to achieve a specific behavior or function. It involves specifying options and parameters that control how the system or software operates.</p> <p><code>make</code> and <code>make install</code></p> <p>make: Compiles the source code and creates binaries, typically in the current directory.             make install: Installs the compiled program into system-wide directories, so it can be run from anywhere on the system. This step usually follows after make. </p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installing-bwa-using-make","title":"Installing bwa using make","text":"<p>BWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. Installation guide from the developer can be found HERE. </p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installation-on-tufts-hpc","title":"Installation on Tufts HPC","text":"<p>Step1: Go to the folder where you would like to install the tools. Ex: We create <code>apps</code> folder under $HOME to install the tools.</p> <pre><code>cd $HOME\nmkdir apps\ncd apps\n</code></pre> <p>Step2: Load the compiler. GCC (GNU Compiler Collection) is required for installing BWA. Since BWA is written in C, you need a C compiler like GCC to compile the source code. When you run the <code>make</code> command to compile BWA, it invokes the GCC compiler to build the binaries from the source code. </p> <pre><code>module av gcc          # check which version of gcc is available. \nmodule load gcc/11.2.0 # Recommend to load the newest version of gcc\n</code></pre> <p>Step3: Clone the bwa repository</p> <p><code>git clone https://github.com/lh3/bwa.git</code></p> <p>Step4: Configure and build</p> <pre><code>cd bwa\nmake\n</code></pre> <p>Step5: Add the build directory to your PATH</p> <p><code>export PATH=$PATH:$HOME/apps/bwa</code></p> <p>Step6: Now bwa is read to use</p> <pre><code>$ bwa\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.18-r1243-dirty\nContact: Heng Li &lt;hli@ds.dfci.harvard.edu&gt;\n\nUsage:   bwa &lt;command&gt; [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries (DEPRECATED)\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index'.\n      There are three alignment algorithms in BWA: `mem', `bwasw', and\n      `aln/samse/sampe'. If you are not sure which to use, try `bwa mem'\n      first. Please `man ./bwa.1' for the manual.\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installing-hmmer-using-make","title":"Installing hmmer using make","text":"<p>HMMER is used for searching sequence databases for sequence homologs, and for making sequence alignments. It implements methods using probabilistic models called profile hidden Markov models (profile HMMs). Installation guide from the developer can be found HERE.</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installation-on-tufts-hpc_1","title":"Installation on Tufts HPC","text":"<p>Make sure you are on compute mode by running the command below</p> <pre><code>srun -p interactive -n 1 --time=02:00:00 --mem 4g --pty bash\n</code></pre> <p>Step1: Go to the folder where you would like to install the tools. Ex: We create <code>apps</code> folder under $HOME to install the tools.</p> <pre><code>cd $HOME\nmkdir apps\ncd apps\n</code></pre> <p>Step2: Download and unpack the source code</p> <pre><code>wget http://eddylab.org/software/hmmer/hmmer-3.4.tar.gz\ntar -xvf hmmer-3.4.tar.gz\ncd hmmer-3.4\n</code></pre> <p>Step3: Configure and build the software</p> <pre><code>./configure --prefix=$HOME/apps   # replace /your/install/path with what you want\nmake\nmake check                        # optional: run automated tests\nmake install                      # Install HMMER programs, a bin folder will be created under $HOME/apps \n</code></pre> <p>Step4: Add HMMER to your PATH</p> <pre><code>export PATH=$PATH:$HOME/apps/bin\n</code></pre> <p>Step5: Now HMMER is read to use</p> <pre><code>$ hmmsearch -h\n# hmmsearch :: search profile(s) against a sequence database\n# HMMER 3.4 (Aug 2023); http://hmmer.org/\n# Copyright (C) 2023 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsearch [options] &lt;hmmfile&gt; &lt;seqdb&gt;\n</code></pre> <p>Tip: Check what has been created after each step to understand how the software is built and installed. </p> <pre><code># After \"tar -xvf hmmer-3.4.tar.gz\"\nls -lhtr $HOME/apps/hmmer-3.4\n# After \"make\"\nls -lhtr $HOME/apps/hmmer-3.4\nls -lhtr $HOME/apps/\n# After \"make install\"\nls -lhtr $HOME/apps/hmmer-3.4\nls -lhtr $HOME/apps/\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#what-is-cmake","title":"What is CMake?","text":"<p>CMake is an open-source, cross-platform family of tools designed to build, test, and package software. It controls the software compilation process by generating native build scripts (like Makefiles or project files) for a wide variety of platforms and compilers.</p> <p>Installation of some bioinformatics applications requires both make and cmake.</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installing-regtools-using-make-and-cmake","title":"Installing RegTools Using Make and CMake","text":"<p>RegTools integrate DNA-seq and RNA-seq data to help interpret mutations in a regulatory and splicing context. Installation guide from the developer can be found HERE</p>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#installation-on-tufts-hpc_2","title":"Installation on Tufts HPC","text":"<pre><code>module avail cmake         # Always use the latest version\n\n--------------------- /opt/shared/Modules/modulefiles-rhel6 ------------------------------\n   cmake/2.8    cmake/2.8.11.2    cmake/3.2.1    cmake/3.4.3\n\n--------------------- /cluster/tufts/hpc/tools/module ------------------------------------\n   cmake/3.18    cmake/3.23_gui (D)\n</code></pre> <pre><code># Load modules\nmodule load gcc/11.2.0\nmodule load cmake/3.23_gui ## Recommand to use the latest version of cmake\n\n# Go to the folder where you would like to install tools\ncd $HOME/apps\n\n# Clone the github repo\ngit clone https://github.com/griffithlab/regtools\n\n# Create a folder called `build`. \n# cmake is designed to work well with out-of-source builds, where the build directory contains all the generated files (e.g., Makefiles, binaries, configuration files). \n# It's a common practice to create build folder\n\ncd regtools/\nmkdir build\ncd build/\n\n# Run cmake first and then make\ncmake ..\nmake\n</code></pre> <pre><code>export PATH=$PATH:$HOME/apps/regtools/build\n\n# The tool is now successfully installed. \nregtools --help\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#dcmake_install_prefix","title":"DCMAKE_INSTALL_PREFIX","text":"<p>Some applications' installation also has install stage, which will have <code>make intall</code> as the last step. For these installations, we have to include <code>-DCMAKE_INSTALL_PREFIX</code> in the <code>cmake ..</code> step. Below are the common steps for such installations:</p> <pre><code>module load gcc/11.2.0\nmodule load cmake/3.23_gui\nmkdir build\ncd build\ncmake -DCMAKE_INSTALL_PREFIX=/path/to/install ..\nmake\nmake install\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/01_source/#summary","title":"Summary","text":"<p>In this guide, we covered the use of Make and CMake in software installation, particularly in the context of HPC environments. We also provided specific examples for installing bioinformatics tools such as BWA, HMMER, and RegTools. These examples emphasize the importance of configuring installation paths to avoid system permission issues and ensure software is installed in user-accessible directories.</p> <p>Previous: Intro</p> <p>Next: R</p>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/","title":"R package installation","text":""},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#objectives","title":"Objectives","text":"<ul> <li> <p>Learn how to run R efficiently on Tufts HPC using both graphical and command-line interfaces.</p> </li> <li> <p>Understand R package management, including installation and configuration on the HPC environment.</p> </li> <li> <p>Develop skills to run R scripts in batch mode, submit jobs to the queue, and manage multiple jobs effectively.</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#how-to-run-r-on-tufts-hpc","title":"How to run R on Tufts HPC","text":""},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#1-open-ondemand-rstudio-app","title":"1. Open OnDemand Rstudio app","text":"<p>1.1 Log in to Open OnDemand with your UTLN</p> <p>https://ondemand.pax.tufts.edu/</p> <p>1.2 You will see <code>RStudio Pax</code> under <code>Interactive Apps</code> and other topic-specific Rstudio apps under <code>Bioinformatics Apps</code></p> <p>1.3 Fill in the parameters according to your needs. You can start with default value. </p> <p>1.4 Launch the job.  </p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#2-command-line-interface","title":"2. Command line interface","text":"<p>2.1 Make sure you are on the compute mode</p> <pre><code>srun -p interactive -n 1 --time=02:00:00 --mem 4g --pty bash\n</code></pre> <p>2.2 Load R modules</p> <pre><code>module load R/4.4.1\n</code></pre> <p>2.3 Launch R interactive session</p> <pre><code>R\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#r-package-installation_1","title":"R package installation","text":""},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#pre-installed-r-packages","title":"Pre-installed R packages","text":"<p>To provide convenience to users, we pre-installed many commonly used R packages used in bioinformatics analysis. Before you try to install packages, load the module for R, start R and check if the package is already installed. </p> <pre><code>library(packageName)\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#get-the-list-of-installed-r-packages","title":"Get the list of installed R packages","text":"<pre><code>installed.packages()\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#install-r-packages","title":"Install R packages","text":"<pre><code>install.packages(\"packageName\")\n</code></pre> <p>Rstudio </p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#libpaths","title":".libPaths()","text":"<p>This shows the folders where the packages are installed. </p> <pre><code>&gt; .libPaths()\n[1] \"/home/user/R/x86_64-pc-linux-gnu-library/4.4\" # Personal library\n[2] \"/usr/local/lib/R/site-library\"                # System-wide library\n[3] \"/usr/lib/R/library\"                           # Default R library\n</code></pre> <p>You can change the libpath to folder under lab storage or research projects. </p> <pre><code>.libPaths(\"your_path_to_install_packages\")\n</code></pre> <p>For R/4.4.1, try not to change libpath, just install to your default $HOME directory.</p>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#r-modules","title":"R modules","text":""},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#r-base","title":"r base","text":"<pre><code> R/4.0.0 \n R/4.1.1 \n R/4.3.0 \n R/4.4.0\n R/4.4.1    # Recommend to use this version\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#r-bioinformatics","title":"r-bioinformatics","text":"<pre><code>$ module av r-bioinformatics\n\n------- /cluster/tufts/biocontainers/modules ---------------\n   r-bioinformatics/4.3.2    r-bioinformatics/4.4.0 (D)\n  Where:\n   D:  Default Module\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#r-scrnaseq","title":"r-scrnaseq","text":"<pre><code>$ module av r-scrnaseq\n\n------- /cluster/tufts/biocontainers/modules -----------------------------\n   r-scrnaseq/4.2.3    r-scrnaseq/4.3.1    r-scrnaseq/4.3.2    r-scrnaseq/4.4.0 (D)\n\n  Where:\n   D:  Default Module\n</code></pre> <p>TIPS</p> <p>For new projects, it's recommended to use the latest version, R/4.4.1, as it comes with the most comprehensive set of installed packages (1300+).</p> <p>For older scripts, either those you created some time ago or inherited from a colleague, be mindful of the R version and <code>libPaths()</code> used in those scripts. Do not run it without paying attention to the version. </p>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#bioconductor","title":"Bioconductor","text":"<p>Bioconductor is both an open source project and repository for R packages related to the analysis of biological data, primarily bioinformatics and computational biology, and as such it is a great place to search for -omics packages and pipelines. </p> <p>Do not run the belew code to install bioconductor core packages</p> <p>We have pre-installed bioconductor core packages for users. </p> <pre><code>if (!require(\"BiocManager\", quietly = TRUE))\n  install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.xx\")\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#installation-with-biocmanager","title":"Installation with BiocManager","text":"<p>To install specific bioconductor packages, use <code>BiocManager</code> to install. </p> <p>Ex: Let's install adverSCarial </p> <pre><code>BiocManager::install(\"adverSCarial\")\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/02_Rpackage/#devtools","title":"devtools","text":"<p>devtools is an R package designed to simplify the process of developing, testing, and installing R packages from various sources.</p> <p><code>devtools</code> has been pre-installed in our R modules. You can load it to install packages require it. </p> <pre><code>library(devtools)\n</code></pre> <p>Installing Packages from GitHub </p> <p>Ex: Install an R package STew used to jointly characterize the gene expression variation and spatial information.</p> <pre><code>.libPaths(\"/home/user/R/x86_64-pc-linux-gnu-library/4.4\")\ndevtools::install_github(\"fanzhanglab/STew\")\n</code></pre> <p>Notice that for many R package, especially those process scRNA-Seq or spatial transcriptomics data, it often require dependencies or version-specific packages. </p> <p>Previous: Souce</p> <p>Next: Conda</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/","title":"Package Installation Using Conda Environment","text":"<p>Author/Presenter: Yucheng Zhang, Bioinformatics Engineer, TTS Research Technology        </p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda","title":"Conda","text":"<p>Conda is an open-source package manager and virtual environment manager for installing packages. </p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-install-is-not-the-first-step","title":"conda install is not the first step","text":"<p><code>conda install</code> is a versatile command that simplifies the installation and management of packages. If you run <code>conda install</code> directly without creating and activating a <code>conda environment</code> first,  conda will install packages into conda's <code>root</code> environment, where you do not have write permissions. </p> <pre><code>EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n  environment location: /cluster/tufts/software/anaconda/Anaconda3-2020.02\n  uid: 33874\n  gid: 7593\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-modules","title":"Conda modules","text":"<pre><code>$ module avail anaconda\n\n--------------------- /opt/shared/Modules/modulefiles-rhel6 ------------------------------------------\n   anaconda/2  anaconda/3\n\n-------------------- /cluster/tufts/hpc/tools/module --------------------------------------------------\nanaconda/bio35  anaconda/2020.02   anaconda/2021.05   anaconda/2021.11   anaconda/2023.07.tuftsai   anaconda/2024.06-py312 (D)\n</code></pre> <p><code>Libmamba</code> became the default solver for Conda starting with version 23.10.0, released in November 2023. The new versions of anaconda modules(2024.06-py312) are significantly faster in solving dependencies thanks to the integration of <code>libmamba</code>, a highly efficient package management library that outperforms Conda\u2019s traditional solver. Not recommend to use old anaconda modules.</p> <p>Anaconda has updated its terms of service(TOS), as below: </p> <p>\"We clarified our definition of commercial usage in our Terms of Service in an update on Sept. 30, 2020. The new language states that use by individual hobbyists, students, universities, non-profit organizations, or businesses with less than 200 employees is allowed, and all other usage is considered commercial and thus requires a business relationship with Anaconda.\"</p> <p>Due to this updated TOS, it's likely we will have to uninstall anaconda from Tufts HPC and other Tufts-owned computers, and migrate to miniforge. Miniforge provides a similar repository of packages and software to enable your install and use of Python for your research workflows. Right now, we are waiting to see what Anaconda Inc. decides. In the meantime, you can still use Anaconda, but we recommend new users to use miniforge instead.</p> <p></p> Feature Anaconda Miniconda Miniforge Size ~3 GB ~400 MB ~400 MB Package Manager Conda Conda Conda Default Channels Anaconda's <code>defaults</code> Anaconda's <code>defaults</code> <code>conda-forge</code> (community-driven open-source) Included Packages Over 300 scientific packages pre-installed Minimal installation (essential tools only) Minimal installation (essential tools only) Target Audience Data scientists needing full environment Developers who want a lightweight version Developers focused on open-source solutions Ease of Use Easy with pre-installed packages Requires manual installation of most packages Similar to Miniconda, but optimized for <code>mamba</code> Solver libmamba libmamba libmamba"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#miniforge-modules","title":"miniforge modules","text":"<pre><code>$ module avail miniforge\n ----------------------/cluster/tufts/hpc/tools/module-----------------------\n   miniforge/24.3.0-py310    miniforge/24.7.1-py312 (D)\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-channels","title":"conda channels","text":"<ul> <li> <p>Conda channels are the locations where packages are stored. </p> </li> <li> <p>Conda search or download packages from channels. </p> </li> <li> <p>For anaconda and miniconda, the default set of channels is called defaults. </p> </li> <li> <p>To install a package that is not in defaults, you need to tell conda which channel contains the package. </p> </li> </ul> <p><pre><code>$ conda install -c pytorch pytorch\u00a0\n$ conda install -c conda-forge r-base\n$ conda install -c bioconda samtools\n</code></pre> Bioconda is a community-enabled conda channel of thousands of bioinformatics packages. </p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#condarc","title":".condarc","text":"<p>By default, Conda stores packages in your $HOME directory. Since each user has a storage limit of 30GB in $HOME, we recommend avoiding package installations there to prevent exceeding your quota. If you are a member of the XXXXlab group on the cluster, we suggest using the group\u2019s research storage space for this purpose.</p> <p>To change the default location to your group's project directory, use a text editor to create a hidden file called <code>.condarc</code>. The path to that file should be <code>$HOME/.condarc</code>.  <pre><code>$ cd \n$ ls -a # -a will reveal hidden files starting with .\n$ touch .condarc # If you haven't created .condarc before, you can create it using the touch command\n</code></pre> Create two directories in your group research storage space (one for storing the <code>envs</code>, one for storing the <code>pkgs</code>, for example: condaenv, condapkg)</p> <pre><code>$ mkdir /cluster/tufts/XXXXlab/$USER/condaenv/\n$ mkdir /cluster/tufts/XXXXlab/$USER/condapkg/\n</code></pre> <p>Now add the following 4 lines to the <code>.condarc</code> file in your home directory (modify according to your real path to the directories):</p> <pre><code>envs_dirs:\n  - /cluster/tufts/XXXXlab/$USER/condaenv/\npkgs_dirs:\n  - /cluster/tufts/XXXXlab/$USER/condapkg/\n</code></pre> <p>OR you can do so from command line with the following commands:</p> <pre><code>$ module load miniforge # or module load anaconda\n$ conda config --append envs_dirs /cluster/tufts/XXXXlab/$USER/condaenv/\n$ conda config --append pkgs_dirs /cluster/tufts/XXXXlab/$USER/condapkg/\n</code></pre> <p>Another common use of <code>.condarc</code> is setting the channels that will be used to search for packages to install. For example, to add the <code>bioconda</code> and <code>conda-forge</code> channels, you can run :</p> <pre><code>$ conda config --add channels bioconda\n$ conda config --add channels conda-forge\n</code></pre> <p>According to Anaconda's updated terms of service, you cannot download and install packages from Anaconda's <code>defaults</code> channel without a paid license. You'd better remove this <code>defaults</code> channel from your <code>.condarc</code> by: <pre><code>$ conda config --remove channels defaults \n</code></pre></p> <p>After this, your <code>.condarc</code>file should look like this</p> <pre><code>$ cat ~/.condarc\nenvs_dirs:\n  - /cluster/tufts/XXXXlab/$USER/condaenv/\npkgs_dirs:\n  - /cluster/tufts/XXXXlab/$USER/condapkg/\nchannels:\n  - bioconda\n  - conda-forge\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-environment","title":"Conda environment","text":"<p>With Conda, you can create and use environments that have different versions of python and other packages. Switching between conda environments is called activating the environment. </p> <p>Conda environments are a useful way to install packages with specific versions and dependencies. </p> <p>It is recommended to always use conda environments. By default, conda environments are located in <code>./conda/envs</code> of $HOME. If you have created <code>.condarc</code> following the above steps, conda environment will be stored in other directories.</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-create-n","title":"conda create -n","text":"<p>To create a new Conda environment named <code>myenv</code>, you can use the following command: <pre><code>$ conda create -n myenv\n</code></pre> You can also use this way: <pre><code>$ conda create --name myenv\n</code></pre> This will create a new environment called <code>myenv</code> without installing any specific packages. If you want to create the environment with specific packages (e.g., Python, NumPy, etc.), you can specify them as follows: <pre><code>$ conda create -n myenv python=3.11 numpy\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-create-p","title":"conda create -p","text":"<p>You can control where a conda environment lives by providing a path to a target directory when creating the environment. For example, you want to create a conda environment in your lab's project folder instead of $HOME.</p> <pre><code>$ conda create --prefix /cluster/tufts/zhanglab/condaenv/myenv2\nor \n$ conda create -p /cluster/tufts/zhanglab/condaenv/myenv2\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#source-activate-and-deactivate","title":"source activate and deactivate","text":"<pre><code>$ source activate myenv1\n$ source deactivate\n$ source activate /cluster/tufts/zhanglab/condaenv/myenv2\n$ source deactivate\n</code></pre> <p>conda activate is supported</p> <p>Since <code>anaconda/2024.06-py312</code>, our anaconda and miniforge modules will support both <code>conda activate</code> and <code>source activate</code>: </p> <p><pre><code>$ conda activate myenv1\n$ conda deactivate\n</code></pre>   You still need to use source activate in old anaconda modules. </p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#do-not-run-conda-init","title":"Do not run <code>conda init</code>","text":"<pre><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n</code></pre> <p>You can run the below command to avoid <code>conda init</code> <pre><code>$ eval \"$(conda shell.bash hook)\"\n</code></pre></p> <p>We do not recommend using <code>conda init</code>, it permanently alters your <code>.bashrc</code> so that the <code>base</code> environment is always activated in new shells. </p> <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/cluster/tufts/software/anaconda/Anaconda3-2020.02/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"/cluster/tufts/software/anaconda/Anaconda3-2020.02/etc/profile.d/conda.sh\" ]; then\n        . \"/cluster/tufts/software/anaconda/Anaconda3-2020.02/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"/cluster/tufts/software/anaconda/Anaconda3-2020.02/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre> <pre><code>(base) $ which conda\n/cluster/tufts/software/anaconda/Anaconda3-2020.02/bin/conda\n</code></pre> <p>If you ran <code>conda init</code> previously, it is recommended to delete the <code>conda init</code> section from your <code>.bashrc</code>.</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#pip","title":"Pip","text":"<p>pip stands for \"pip installs packages\". It is a package manager for Python packages. pip installs packages that are hosted on the Python Package Index or PyPI.</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-install-vs-pip-install","title":"conda install vs pip install","text":"Feature <code>conda install</code> <code>pip install</code> Package Management Manages both python packages and non-python libraries, dependencies, and environments. Manages only python packages and dependencies. Environment Creation Can create and manage isolated environments with both python and non-python dependencies. Requires <code>venv</code> or similar tools for environment creation. Dependency Handling Handles complex dependencies more robustly by using curated, pre-built binaries. Resolves dependencies but might lead to version conflicts (dependency hell)."},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-install-or-pip-install","title":"conda install or pip install?","text":"<p>If your package or versions exist supports both <code>conda install</code> and <code>pip install</code>.  You should almost always favor conda over pip. Because conda uses a more robust dependency resolution mechanism. The packages available in the Conda repositories are pre-built and tested to work together, reducing the likelihood of version conflicts (sometimes called dependency hell).</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#pip-venv","title":"pip venv","text":"<p>Similar to conda environment, it is also recommended to create isolated python environments to avoid/reduce conflicts. Python3 provides a <code>venv</code> module that can be used to lightweight virtual environment. </p> <p>A virtual environment can be created by running \"<code>python3 -m venv &lt;DIR&gt;</code>\", where <code>&lt;DIR&gt;</code> is the directory of the created environment. </p> <p><code>venv</code> is very useful to manage difffernt python projects. However, we would like to reduce users' confusion, and hope users to stick to conda environment for both python and non-python applications.</p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#pip-install-user-is-not-recommended","title":"pip install --user is not recommended","text":"<p>In user mode installation, packages are installed under the user\u2019s home directory (in <code>$HOME/.local/</code> and carry the risk of being contaminated over time. For example, during minor upgrade of the python version older packages may no longer work. Similarly, if users install multiple packages with conflicting dependencies over time, some packages would break. This can be avoided by creating a virtual environment (such as an conda environment), which provides an isolated location for installing a group of packages. </p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-create-name-myenv-python3xx","title":"conda create --name myenv python=3.xx","text":"<ul> <li>Creates a new conda environment named myenv with python version 3.xx pre-installed.</li> <li>Python modules installed using pip within this environment will be installed in the myenv environment's specific directory. This ensures isolation and prevents conflicts with other environments or the base conda environment.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#conda-create-name-myenv-this-is-not-recommended","title":"conda create --name myenv: This is not recommended","text":"<p>Creates a new conda environment named myenv without specifying a python version.</p> <p>By default, this environment will likely use the base Python installation on your system. If you install python modules using pip, their location will depend on the default python installation's configuration (usually it is user\u2019s home directory: <code>$HOME/.local/</code>), which can lead to potential conflicts and inconsistencies. </p>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#using-conda-environments-in-jupyter","title":"Using Conda environments in Jupyter","text":"<p>If you would like to use JupyterNotebook or JupyterLab from OnDemand, you can follow the instructions below and run your conda env as a kernel in Jupyter.</p> <ul> <li>Make sure with python 3.7+ and make sure you load cluster\u2019s anaconda module (this only works with py3.7+)</li> <li>Create conda environment with its own python</li> </ul> <pre><code>module load anaconda/2024.06-py312\nconda create --name myenv python=3.xx\n</code></pre> <ul> <li>Activate the conda environment</li> </ul> <pre><code>$ conda activate myenv\n</code></pre> <ul> <li>Install the ipykernel python module </li> </ul> <pre><code>$ pip install ipykernel\n</code></pre> <ul> <li>Create the kernel file</li> </ul> <pre><code>$ python -m ipykernel install --user --name=mykernelname ## give a meaningful kernel name\n</code></pre> <ul> <li> <p>Restart Jupyter from OnDemand</p> </li> <li> <p>:)</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#recommended-workflow","title":"Recommended workflow","text":"<pre><code>$ module load miniforge/24.7.1-py312  \n$ conda config --append envs_dirs /cluster/tufts/XXXXlab/$USER/condaenv/ # Once\n$ conda config --append pkgs_dirs /cluster/tufts/XXXXlab/$USER/condapkg/ $ Once\n$ conda config --remove channels defaults  # Once\n$ conda config --add channels bioconda   # Once\n$ conda config --add channels conda-forge # Once\n$ conda create -n myenv python=3.12\n$ conda activate myenv\n$ conda install xxx\n$ pip install xxx\n$ conda deactivate # After your job/analysis is complete\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/03_conda/#caveats","title":"Caveats","text":"<ul> <li>Do not run <code>conda init</code></li> <li>Do not install python modules with <code>pip install --user</code></li> <li>Watch for disk usage in your $HOME   <pre><code> $ module load hpctools\n $ hpctools\n Please select from the following options:\n1. Checking Free Resources On Each Node in Given Partition(s)\n2. Checking Free GPU Resources On Each Node in Given Partition(s)\n3. Checking Past Completed Jobs in Given Time Period\n4. Checking Active Job informantion\n5. Checking Project Space Storage Quota Informantion\n6. Checking Any Directory Storage Usage Informantion\nPress q to quit\n## You can select 6 to check your $HOME disk usage\n</code></pre></li> </ul> <p>Previous: R</p> <p>Next: conda-env-mod</p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/","title":"Simplying Conda Environment Management with conda-env-mod","text":"<p>Author/Presenter: Yucheng Zhang, Bioinformatics Engineer, TTS Research Technology</p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#introduction","title":"Introduction","text":"<p><code>conda-env-mod</code> is a tool for simplifying Python package installation on HPC clusters. It can be used in the installation and management of not only Python packages but also packages that support Conda install.</p> <p>Due to limited user-level permissions on managed clusters, installing and maintaining Python packages becomes a challenging task for novice users. </p> <p><code>conda-env-mod</code> was developed by Amiya Maji from Rosen Center for Advanced Computing (RCAC) at Purdue University to use Anaconda environments and environment modules (Lmod) to simplify this process. Souce code of <code>conda-env-mod</code> is available at its guide repo.</p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#general-steps","title":"General steps","text":"<p>There are three simple steps to install and use packages using <code>conda-env-mod</code>:</p> <ol> <li>Create an conda environment using <code>conda-env-mod</code>. During the process, the script automatically creates module file for using the environment.</li> <li>Load the module generated by <code>conda-env-mod</code>. By default modules are generated in <code>$HOME/privatemodules</code>. </li> <li>Now use <code>conda</code> or <code>pip</code> to install your packages. Once the installation is complete, you can directly use them in your script.  No need to run <code>conda init</code> or <code>conda activate</code>.</li> </ol>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#benefits","title":"Benefits","text":"<ul> <li>Automating environment creation and configuration reduces mistakes</li> <li>Module files enable easy sharing of conda environments</li> <li>Automatic kernel creation allows environments to be used in Jupyter notebook/lab.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#usage","title":"Usage","text":"<pre><code>conda-env-mod &lt;subcommand&gt; &lt;required argument&gt; [optional argument(s)]\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#subcommands","title":"Subcommands","text":"<ul> <li>create: Create an environment. Must specify env_name or env_path.</li> <li>delete: Delete an existing environment.  Must specify env_name or env_path.</li> <li>module: Create a module file for an existing environment.  Must specify env_name or env_path.</li> <li>kernel: Create a Jupyter kernel for an existing environment.  Must specify env_name or env_path.</li> <li>help:   Display brief usage information.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#workflow","title":"Workflow","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#step1-load-anaconda-or-miniforge","title":"Step1: Load anaconda or miniforge","text":"<p>To use <code>conda-env-mod</code> , users must load one of the anaconda or miniforge modules first.</p> <pre><code>$ module avail anaconda\n\n--------------------- /opt/shared/Modules/modulefiles-rhel6 ------------------------------------------\n   anaconda/2  anaconda/3\n\n-------------------- /cluster/tufts/hpc/tools/module --------------------------------------------------\nanaconda/bio35  anaconda/2020.02   anaconda/2021.05   anaconda/2021.11   anaconda/2023.07.tuftsai   anaconda/2024.06-py312 (D)\n\n\n$ module load anaconda/2024.06-py312\n</code></pre> <p>Anaconda updated its terms of service(TOS)</p> <p>We clarified our definition of commercial usage in our Terms of Service in an update on Sept. 30, 2020. The new       language states that use by individual hobbyists, students, universities, non-profit organizations, or businesses with less than 200 employees is allowed, and all other usage is considered commercial and thus requires a business relationship with Anaconda.</p> <p>Due to this updated TOS, it's likely we will have to uninstall anaconda from Tufts HPC and other Tufts-owned computers, and migrate to miniforge. Right now, we are sitting tight to see whether Anaconda Inc. will make some updates. In the meantime, you can still use anaconda, butI do recommend users to use miniforge instead. </p> <pre><code>$ module avail miniforge\n ----------------------/cluster/tufts/hpc/tools/module-----------------------\n   miniforge/24.3.0-py310    miniforge/24.7.1-py312 (D)\n\n$ module load miniforge/24.7.1-py312\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#step2-load-conda-env-mod","title":"Step2: Load conda-env-mod","text":"<pre><code>$ module load conda-env-mod\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#step-3-create-a-conda-environment","title":"Step 3: Create a conda environment","text":"<p>Users can use the <code>conda-env-mod</code>  to create an empty conda environment. It needs either a name or a path for the desired environment. After the environment is created, it generates a module file for using it in future. Detailed instructions for using <code>conda-env-mod</code> can be found with the command <code>conda-env-mod --help</code>.</p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-a-conda-environment-with-n","title":"Create a conda environment with -n","text":"<p>To create a new Conda environment named <code>myenv</code>, you can use the following command: <pre><code>$ conda-env-mod create -n myenv\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-a-conda-environment-with-p","title":"Create a conda environment with -p","text":"<p>This will create a conda environment to a custom directory. </p> <pre><code>$ conda-env-mod create -p /cluster/tufts/mylab/$USER/condaenv/myenv\n</code></pre> <p>Once the conda-env-mod finishes creating the environment, you will see information similar to below: <pre><code>Please make sure you initialize conda separately.\n+---------------------------------------------------------------+\n| To use this environment, load the following modules:          |\n|     module load use.own                                       |\n|     module load conda-env/myenv-py3.12.5                      |\n| (then standard 'conda install' / 'pip install' / run scripts) |\n+---------------------------------------------------------------+\n</code></pre></p> <p>By default, modulefiles are stored in <code>$HOME/privatemodules/conda-env</code>. You can load the module by: <pre><code>$ module load use.own\n$ module load conda-env/myenv-py3.12.5\n$ conda install XXX\n$ pip install XXX\n$ module unload conda-env/myenv-py3.12.5\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-a-conda-environment-as-well-as-jupyter-kernel","title":"Create a conda environment as well as jupyter kernel","text":"<p>You just need to add <code>--jupyter</code> when you run <code>conda-env-mod create -n</code> or <code>conda-env-mod create -p</code>. <pre><code>$ conda-env-mod create -n myenv --jupyter\n$ conda-env-mod create -p /cluster/tufts/mylab/$USER/condaenv/myenv --jupyter\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-and-manage-shared-conda-environments-for-the-group","title":"Create and manage shared conda environments for the group","text":"<p>Users can create conda environments in lab's project folder, and share them with the whole group.</p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#the-pi-or-lab-manager","title":"The PI or lab manager","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-conda-environmentonce","title":"Create conda environment(once)","text":"<p><pre><code>$ module purge\n$ module load anaconda # module load miniforge\n$ module load conda-env-mod\n$ conda-env-mod create -p /cluster/tufts/mylab/apps/mypackage -m /cluster/tufts/mylab/modules --jupyter\n</code></pre> In the example code, <code>mylab</code> is my group name. I created two folders <code>apps</code> and <code>modules</code> to store packages installations, and module files, respectively.</p> <p><code>--jupyter</code> is only needed if you plan to use the conda environment on jupyter notebook/lab. </p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#install-packages-into-the-environment","title":"Install packages into the environment","text":"<pre><code>$ module use /cluster/tufts/mylab/modules\n$ module load conda-env/mypackage-py3.12.5\n$ conda install  .......                     \n$ pip install ...\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#confirm-the-access-permissions","title":"Confirm the access permissions","text":"<p>To allow all group members to use the newly built environment, PI or lab managers need to confirm the group has read and execute permissions to the conda environments and modulefile folder.  If permissions are not correctly set up, they can be modified by:</p> <pre><code>$ chmod -R g=rx /cluster/tufts/mylab/apps\n$ chmod -R g=rx /cluster/tufts/mylab/modules \n</code></pre> <ul> <li><code>-R</code> stands for recursive. It allows you to apply the permission change to directories and all of their contents, including subdirectories and files.</li> <li><code>g=rx</code> means the group members can read and execute files and directories, but not modify them (no write permission).</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#group-members","title":"Group members","text":"<p>Once the conda environment has been created, all members can start using the environment simply by loading the corresponding module: <pre><code>$ module use /cluster/tufts/mylab/modules\n$ module load conda-env/mypackage-py3.12.5\n$ python my_script.py .....\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#jupyter-notebooklab","title":"Jupyter notebook/lab","text":"<p>To use the conda environment in Jupyter notebook/lab, each lab member will need to create his/her own Jupyter kernel (once). This is because Jupyter kernels are private to individuals. By default, Jupyter will only search for kernels in users' <code>$HOME/.local/share/jupyter/kernels</code>.</p> <pre><code>$ module load miniforge # or module load anaconda\n$ module load conda-env-mod\n$ conda-env-mod kernel -p /cluster/tufts/mylab/apps/mypackage\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#conda-enivronment-for-the-class","title":"Conda enivronment for the class","text":"<p>The instructor can follow a similar process to create conda environment and install packages that can be used by all students. </p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#examples","title":"Examples","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#example-1-create-a-biopython-conda-environment-for-yourself","title":"Example 1: Create a Biopython conda environment for yourself","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-conda-environment-modulefile-and-jupyter-kernel","title":"Create conda environment, modulefile, and jupyter kernel","text":"<pre><code>$ module load miniforge/24.7.1-py312\n$ module load conda-env-mod\n$ conda-env-mod create -n biopython --jupyter\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#load-module-and-install-packages","title":"Load module and install packages","text":"<pre><code>$ module load use.own\n$ module load conda-env/biopython-py3.12.5\n$ conda install -c bioconda biopython\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#using-biopython-in-command-line-or-scripts","title":"Using biopython in command line or scripts","text":"<pre><code>$ which python\n/cluster/tufts/yzhang85/conda/condaenv/biopython/bin/python\n$ python\nPython 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:36:51) [GCC 12.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from Bio import SeqIO\n&gt;&gt;&gt; fasta_file = \"genome.fa\"\n&gt;&gt;&gt; for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n...     print(f\"ID: {seq_record.id}\")\n...     print(f\"Sequence length: {len(seq_record)}\")\n... \nID: NZ_LFXA01000001.1\nSequence length: 199691\nID: NZ_LFXA01000002.1\nSequence length: 1310906\nID: NZ_LFXA01000003.1\nSequence length: 232631\nID: NZ_LFXA01000004.1\nSequence length: 719325\nID: NZ_LFXA01000005.1\nSequence length: 109134\nID: NZ_LFXA01000006.1\nSequence length: 17068\nID: NZ_LFXA01000007.1\nSequence length: 340777\nID: NZ_LFXA01000008.1\nSequence length: 126449\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#using-biopython-as-a-jupyter-kernel-in-jupyter-notebooklab-on-open-ondemand","title":"Using biopython as a jupyter kernel in jupyter notebook/lab on Open OnDemand","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#example-2-rnaseq-environment-for-the-lab","title":"Example 2: RNAseq environment for the lab","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#load-required-modules","title":"Load required modules","text":"<p>Users can choose either anaconda or miniforge.  <pre><code>$ module load miniforge/24.7.1-py312 \n$ module load conda-env-mod\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-conda-environment","title":"Create conda environment","text":"<pre><code>$ conda-env-mod create -p /cluster/tufts/rt/shared/apps/rnaseq -m /cluster/tufts/rt/shared/modules\n</code></pre> <p>After the installation completes, you will see the below instructions about how to use the environment and module:</p> <pre><code>+---------------------------------------------------------------+\n| To use this environment, load the following modules:          |\n|     module use /cluster/tufts/rt/shared/modules               |\n|     module load conda-env/rnaseq-py3.12.5                     |\n| (then standard 'conda install' / 'pip install' / run scripts) |\n+---------------------------------------------------------------+\nYour environment \"rnaseq\" was created successfully.\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#load-the-module-and-install-applications-for-rnaseq-analysis","title":"Load the module and install applications for RNAseq analysis","text":"<pre><code>$ module use /cluster/tufts/rt/shared/modules\n$ module load conda-env/rnaseq-py3.12.5 \n$ conda install -c bioconda star samtools fastqc trim-galore salmon \n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#example-3-pytorch-for-the-lab","title":"Example 3: PyTorch for the lab","text":""},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#load-required-modules_1","title":"Load required modules","text":"<pre><code>$ module load miniforge/24.7.1-py312 \n$ module load conda-env-mod\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#create-conda-environment_1","title":"Create conda environment","text":"<p><pre><code>$ conda-env-mod create -p /cluster/tufts/rt/shared/apps/pytorch_2.4.1 -m /cluster/tufts/rt/shared/modules --jupyter\n</code></pre> Since PyTorch is a python package, running it using Jupyter notebook/lab is convenient. That's why <code>--jupyter</code> is used. </p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#install-pytorch","title":"Install PyTorch","text":"<pre><code>$ module use /cluster/tufts/rt/shared/modules \n$ module load conda-env/pytorch_2.4.1-py3.12.5  \n$ pip3 install torch torchvision torchaudio\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#running-the-kernel-in-jupyter-notebook-on-open-ondemand","title":"Running the kernel in Jupyter Notebook on Open OnDemand","text":"<p>When you open Jupyter notebook/lab, you can see the PyTorch kernel <code>My pytorch_2.4.1 Kernel</code>.  </p> <p>Let's try to use the PyTorch package to run a simple analysis. You can see that it successfully run on Jupyter notebook with Nvidia A100 GPU.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics301/04_conda-env-mod/#sharing-with-other-members-in-the-group","title":"Sharing with other members in the group","text":"<p>If other members from the group also want to run pytorch inside Jupyter, they only need to create their own kernel file: <pre><code>$ module load miniforge # or module load anaconda\n$ module load conda-env-mod\n$ conda-env-mod kernel -p /cluster/tufts/rt/shared/apps/pytorch_2.4.1\n</code></pre></p> <p>Previous: Conda</p>"},{"location":"2024_workshops/2024_bioinformatics401/00_introduction/","title":"Advanced HPC Workflows and Job Management","text":"<p>This repository stores the slides and hands-on sessions for bioinformatics training workshops provided by Tufts Research Technology in October 2024.</p>"},{"location":"2024_workshops/2024_bioinformatics401/00_introduction/#agenda","title":"Agenda","text":"<ul> <li>Parallelizing Workflows with Slurm Job Arrays</li> <li>Nextflow and nf-core</li> <li>nf-core: Community Curated Bioinformatics Pipelines</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics401/00_introduction/#presenters","title":"Presenters","text":"<sub>Shirley Li</sub> <sub>Yucheng Zhang</sub> <p>Next: fetchngs</p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/","title":"Slurm Job Arrays","text":"<p>Job arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily, saving both time and computational resources.</p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#use-cases","title":"Use cases","text":"<ul> <li>I have 1000 samples and they all need to run the same workflow.</li> <li>I need to run a simulation 1000 times with a different set of parameters.</li> </ul> <p>Why not use serial jobs? </p> <p>A common approach is to use bash loops to submit jobs one by one, but this is not efficient for large numbers of tasks. For example:</p> <p><pre><code>for fq in *.fastq.gz; do \n  fastqc -t 4 $fq\ndone\n</code></pre> </p> <p>Using bash loops works but often results in jobs taking much longer. Instead, using SLURM job arrays can streamline this process.</p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#slurm-arrays","title":"Slurm arrays","text":""},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#basic-syntax","title":"Basic Syntax","text":"<p>Job arrays are only supported for batch jobs, and the array index values are specified using the <code>--array</code> or <code>-a</code> option of the <code>sbatch</code> command or <code>#SBATCH</code> inisde job script. </p> <pre><code>--array=&lt;indices&gt;\n</code></pre> <ul> <li> <p>You can specify the array indices in different ways:</p> </li> <li> <p><code>--array=0-100</code>: Runs jobs with indices from 0 to 100.</p> </li> <li> <p><code>--array=2,4,6,8,10</code>: Runs jobs with specific indices (2, 4, 6, 8, and 10).</p> </li> <li> <p><code>--array=2-1000:2</code>: Runs jobs with a step size, in this case, every 2nd job from 2 to 1000.</p> </li> <li> <p>You can limit the number of array jobs which are allowed to run at once by using the <code>%</code> character when specifying indices.</p> </li> <li> <p><code>1-16%2</code> Create 16 jobs, but only allow two to run at a time</p> </li> </ul>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#job-id-and-environment-variables","title":"Job ID and Environment Variables","text":""},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#slurm_array_job_id","title":"SLURM_ARRAY_JOB_ID","text":"<ul> <li> <p>This environment variable represents the job ID of the entire job array.</p> </li> <li> <p>It is the same for all tasks within that job array.</p> </li> <li> <p>If you submit a job array with 10 tasks, each of those tasks will have the same <code>SLURM_ARRAY_JOB_ID</code>.</p> </li> </ul> <p>Example:</p> <p>If you submit a job array with <code>sbatch --array=1-10 script.sh</code>, and the job array is assigned the job ID 12345, then</p> <ul> <li><code>SLURM_ARRAY_JOB_ID</code> for all tasks will be 12345.</li> </ul>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#slurm_array_task_id","title":"SLURM_ARRAY_TASK_ID","text":"<ul> <li> <p>This environment variable represents the unique identifier of each task within the job array.</p> </li> <li> <p>It differentiates each task in the array and usually corresponds to the index you specified when submitting the job array.</p> </li> <li> <p>This is the variable you use to handle task-specific operations within the script.</p> </li> </ul> <p>Example:</p> <p>If you submit a job array with <code>sbatch --array=1-10 script.sh</code>, and the job array is assigned the job ID 12345, then:</p> <ul> <li> <p>Task 1 will have SLURM_ARRAY_TASK_ID=1.</p> </li> <li> <p>Task 2 will have SLURM_ARRAY_TASK_ID=2.</p> </li> <li> <p>And so on, up to SLURM_ARRAY_TASK_ID=10 for the last task.</p> </li> </ul> <p>In a simple case, you can directly use the <code>$SLURM_ARRAY_TASK_ID</code> variable in your script to set up your job array. </p> <p>For instance, if you have a fasta file for each sample like: sample1.fa, sample2.fa, sample3.fa ... sample10.fa, and you want each of the 10 Slurm array tasks to handle a separate sample file, you can replace the line specifying the sample filename with <code>sample${SLURM_ARRAY_TASK_ID}.fa</code>. </p> <p>This means that for array task 1, the script will run sample1.fa, for array task 2 it will run sample2.fa, and so on.</p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#monitor-and-cancel-jobs","title":"Monitor and cancel jobs","text":"<p>You can cancel a particular array task using the respective JOBID in the first column, e.g. <code>scancel 7456478_2</code>, or you can cancel all array tasks in the array job by just specifying the main job ID, e.g. <code>scancel 7456478</code>.</p> <pre><code>[yzhang85@login-prod-01 array]$ sbatch fastqc_array.sh \nSubmitted batch job 7456347\n[yzhang85@login-prod-01 array]$ squeue --me\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n         7456478_1   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_2   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_3   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_4   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_5   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n         7456478_6   preempt   fastqc yzhang85  R       1:30      1 s1cmp004\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#limiting-the-number-of-tasks-to-run-simultaneously","title":"Limiting the number of tasks to run simultaneously","text":"<p>By default, if sufficient resources are available, all tasks in a job array will run simultaneously. However, if you wish to limit the number of tasks running at once, you can use the <code>%N</code> parameter with the <code>--array</code> option (where N specifies the maximum number of tasks to execute concurrently). </p> <p>In the following example, I used <code>--array=0-999%10</code>, which creates a total of 1000 tasks. By appending <code>%10</code>, I limited the number of tasks that can run concurrently to 10, meaning that instead of all 1000 tasks running simultaneously, only 10 tasks will be executed at any given time. This helps manage resource usage on the cluster.</p> <pre><code>         JOBID       PARTITION  NAME     USER  ST       TIME  NODES NODELIST(REASON)\n7689847_[10-999%10   preempt array_te yzhang85 PD       0:00      1 (JobArrayTaskLimit)\n         7689847_0   preempt array_te yzhang85  R       0:03      1 p1cmp029\n         7689847_1   preempt array_te yzhang85  R       0:03      1 p1cmp029\n         7689847_2   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_3   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_4   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_5   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_6   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_7   preempt array_te yzhang85  R       0:03      1 p1cmp043\n         7689847_8   preempt array_te yzhang85  R       0:03      1 p1cmp045\n         7689847_9   preempt array_te yzhang85  R       0:03      1 p1cmp045\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#example-job-scripts","title":"Example job scripts","text":"<p>In the following example, I have many <code>fastq.gz</code> files in the folder <code>fastq</code>. I need to run <code>fastqc</code>  to check the quality of each of these <code>fastq.gz</code> files. </p> <pre><code>$ ls -1 fastq/*.gz\nfastq/SRX1693951_1.fastq.gz\nfastq/SRX1693951_2.fastq.gz\nfastq/SRX1693952_1.fastq.gz\nfastq/SRX1693952_2.fastq.gz\nfastq/SRX1693953_1.fastq.gz\nfastq/SRX1693953_2.fastq.gz\nfastq/SRX1693954_1.fastq.gz\nfastq/SRX1693954_2.fastq.gz\nfastq/SRX1693955_1.fastq.gz\nfastq/SRX1693955_2.fastq.gz\nfastq/SRX1693956_1.fastq.gz\nfastq/SRX1693956_2.fastq.gz\n</code></pre> <p>For each <code>fastq.gz</code> file, I want to submit a separate slurm job to our cluster. This can be achieved with slurm job array. </p> <p>We can use <code>fastq/SRX169395${SLURM_ARRAY_TASK_ID}_1.fastq.gz</code> and <code>fastq/SRX169395${SLURM_ARRAY_TASK_ID}_2.fastq.gz</code> to represent pairs of <code>fastq.gz</code> files.</p> <pre><code>#!/bin/bash\n#SBATCH -p preempt  # batch, gpu, preempt, mpi or your group's own partition\n#SBATCH -t 1:00:00  # Runtime limit (D-HH:MM:SS)\n#SBATCH -N 1   # Number of nodes\n#SBATCH -n 1   # Number of tasks per node\n#SBATCH -c 4   # Number of CPU cores per task\n#SBATCH --mem=8G       # Memory required per node\n#SBATCH --array=1-6     # An array of 6 jobs\n#SBATCH --job-name=fastqc      # Job name\n#SBATCH --mail-type=FAIL,BEGIN,END     # Send an email when job fails, begins, and finishes\n#SBATCH --mail-user=yzhang85@tufts.edu       # Email address for notifications\n#SBATCH --error=%x-%A_%a.err   # Standard error file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.err\n#SBATCH --output=%x-%A_%a.out  # Standard output file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.out\n\necho \"SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n\nmodule load fastqc/0.12.1\nfastqc -t 4 fastq/SRX169395${SLURM_ARRAY_TASK_ID}_1.fastq.gz fastq/SRX169395${SLURM_ARRAY_TASK_ID}_2.fastq.gz -o fastqcOut\n</code></pre> <p>Output logs</p> <pre><code>[yzhang85@login-prod-01 array]$ ls -hl\ntotal 13K\ndrwxrws--- 2 yzhang85 workshop 4.0K Aug 30 11:51 fastq/\ndrwxrws--- 2 yzhang85 workshop 4.0K Aug 30 11:39 fastqcOut/\n-rw-rw---- 1 yzhang85 workshop 1.2K Aug 30 11:54 fastqc-7456478_1.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_1.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_2.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_2.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_3.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_3.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_4.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_4.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_5.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_5.out\n-rw-rw---- 1 yzhang85 workshop 1.1K Aug 30 11:54 fastqc-7456478_6.err\n-rw-rw---- 1 yzhang85 workshop  110 Aug 30 11:52 fastqc-7456478_6.out\n-rw-rw---- 1 yzhang85 workshop  918 Aug 30 11:48 fastqc_array.sh\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#limits","title":"Limits","text":"<p>HPC is a valuable shared resource that allows many users to perform complex calculations simultaneously. To ensure a productive and fair environment for everyone, we have implemented policies and practices that promote equitable access to our computational resources.</p> <p>There are several limits for array jobs.  If you submit too many array jobs and exceed the limits, you will get the below error message: <pre><code>$ sbatch array.sub \nsbatch: error: AssocMaxSubmitJobLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre></p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#maxarraysize","title":"MaxArraySize","text":"<p>The array index should be smaller than <code>MaxArraySize</code>. </p> <pre><code>scontrol show conf | grep MaxArraySize\n$ scontrol show config | grep -i array\nMaxArraySize            = 2000\n</code></pre> <p>Since <code>MaxArraySize</code> is set as 2000, the maximum array index you can use is 1999. So \"1000-1999\" is valid, but \"1001-2000\" is invalid. </p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#maxsubmit","title":"MaxSubmit","text":"<p>Our cluster does not allow users to submit &gt; 1000 jobs. As a result, the maximum array size is 1000. So \"0-999\" and \"1-1000\" is valid, but \"1-1001\" or \"0-1000\" is invalid. </p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#cpus-ram-and-gpus","title":"CPUs, RAM and GPUs","text":"<pre><code>Public Partitions (batch+mpi+largemem+gpu)\nCPU: 1000 cores\nRAM: 4000 GB\nGPU: 10\n\nPreempt Partition (preempt)\nCPU: 2000 cores\nRAM: 8000 GB\nGPU: 20\n</code></pre> <p>Please note that the above limits are subject to change in the future. To ensure optimal resource allocation, the limit value is dynamic and may change as we evaluate system demands.</p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#using-parameter-file-to-manage-array-tasks","title":"Using parameter file to manage array tasks","text":"<p>In most cases, your script will loop through different input parameters, which are usually not number 1-10 or 1-100. </p> <p>In this situation, we would like to use a parameter file with input parameters for each job.  </p>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#required-files","title":"Required files","text":"<ol> <li>Parameter File: A file containing the parameters that your array job will iterate through. This file could include different variables or data that each array task will process individually.</li> <li>Slurm Script: A simple shell script that sends your jobs to the SLURM scheduler. This script makes it easy to run multiple tasks automatically, with each task using different parameters from the parameter file.</li> </ol>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#fastqc-command","title":"Fastqc Command","text":"<p>Here is an example of fastqc command that generates html report for each pair of fastq files</p> <pre><code>fastqc ${fastq_R1} ${fastq_R2} -o ${output_folder}\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#parameter-file","title":"Parameter File","text":"<p>Here\u2019s an example of the parameter file <code>id_sample.tsv</code> used in the job array. Each row includes a sample ID, along with the corresponding forward and reverse read FASTQ files.</p> <pre><code>1     SRX1693953_SRR3362663_1.fastq.gz SRX1693953_SRR3362663_2.fastq.gz\n2     SRX1693956_SRR3362666_1.fastq.gz SRX1693956_SRR3362666_2.fastq.gz\n3     SRX1693952_SRR3362662_1.fastq.gz SRX1693952_SRR3362662_2.fastq.gz\n4     SRX1693955_SRR3362665_1.fastq.gz SRX1693955_SRR3362665_2.fastq.gz\n5     SRX1693951_SRR3362661_1.fastq.gz SRX1693951_SRR3362661_2.fastq.gz\n6     SRX1693954_SRR3362664_1.fastq.gz SRX1693954_SRR3362664_2.fastq.gz\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#slurm-script","title":"Slurm Script","text":"<p>The following shell script submits the jobs to the SLURM scheduler as an array of tasks. Each task processes a sample. </p> <pre><code>#!/bin/bash\n#SBATCH -p preempt  # batch, gpu, preempt, mpi or your group's partition\n#SBATCH -t 1:00:00  # Runtime limit (D-HH:MM:SS)\n#SBATCH -N 1        # Number of nodes\n#SBATCH -n 1        # Number of tasks per node\n#SBATCH -c 4        # Number of CPU cores per task\n#SBATCH --mem=8G    # Memory required per node\n#SBATCH --array=1-6 # An array of 10 jobs\n#SBATCH --job-name=fastqc\n#SBATCH --mail-type=FAIL,BEGIN,END\n#SBATCH --mail-user=utln@tufts.edu\n#SBATCH --error=%x-%A_%a.err   # Standard error file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.err\n#SBATCH --output=%x-%A_%a.out  # Standard output file: &lt;job_name&gt;-&lt;job_id&gt;-&lt;taskid&gt;.out\n\necho \"SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID\n\nmodule load fastqc/0.12.1\nID=$(awk \"NR==${SLURM_ARRAY_TASK_ID} {print \\$1}\" id_sample.tsv) \nfastq_R1=$(awk \"NR==${SLURM_ARRAY_TASK_ID} {print \\$2}\" id_sample.tsv) \nfastq_R2=$(awk \"NR==${SLURM_ARRAY_TASK_ID} {print \\$3}\" id_sample.tsv)\n\necho $ID ${fastq_R1} ${fastq_R2}\nfastqc ${fastq_R1} ${fastq_R2} -o fastqcOut\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#script-details","title":"Script Details","text":"<ul> <li><code>SBATCH --array=1-6</code> tells SLURM to run jobs for rows 1 to 6 of the parameter file.</li> <li>The <code>awk</code> commands extract the <code>ID</code>, <code>fastq_R1</code> and <code>fastq_R2</code> values from the specified row and columns (1st, 2nd and 3rd).</li> <li>The script submits 6 jobs, each running the FASTQC command with different <code>fastq_R1</code> and <code>fastq_R2</code> values.</li> </ul> <p>After the array jobs are submitted, we can see that 6 separate jobs are running, with SLURM_ARRAY_TASK_ID from 1-6.</p> <pre><code>JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n         7975940_1   preempt   fastqc     utln  R       4:35      1 d1cmp024\n         7975940_2   preempt   fastqc     utln  R       4:35      1 d1cmp032\n         7975940_3   preempt   fastqc     utln  R       4:35      1 d1cmp031\n         7975940_4   preempt   fastqc     utln  R       4:35      1 d1cmp031\n         7975940_5   preempt   fastqc     utln  R       4:35      1 d1cmp022\n         7975940_6   preempt   fastqc     utln  R       4:35      1 d1cmp022\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#output-files","title":"Output files","text":"<pre><code>-rw-rw---- 1 yzhang85 workshop 386K Oct 22 14:40 SRX1693951_SRR3362661_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 590K Oct 22 14:40 SRX1693951_SRR3362661_1_fastqc.html\n-rw-rw---- 1 yzhang85 workshop 386K Oct 22 14:40 SRX1693954_SRR3362664_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 588K Oct 22 14:40 SRX1693954_SRR3362664_1_fastqc.html\n-rw-rw---- 1 yzhang85 workshop 379K Oct 22 14:40 SRX1693953_SRR3362663_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 585K Oct 22 14:40 SRX1693953_SRR3362663_1_fastqc.html\n-rw-rw---- 1 yzhang85 workshop 385K Oct 22 14:41 SRX1693952_SRR3362662_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 590K Oct 22 14:41 SRX1693952_SRR3362662_1_fastqc.html\n-rw-rw---- 1 yzhang85 workshop 383K Oct 22 14:41 SRX1693956_SRR3362666_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 588K Oct 22 14:41 SRX1693956_SRR3362666_1_fastqc.html\n-rw-rw---- 1 yzhang85 workshop 386K Oct 22 14:41 SRX1693955_SRR3362665_1_fastqc.zip\n-rw-rw---- 1 yzhang85 workshop 589K Oct 22 14:41 SRX1693955_SRR3362665_1_fastqc.html \n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/01_array_jobs/#useful-links","title":"Useful links:","text":"<p>https://slurm.schedmd.com/job_array.html          </p> <p>https://blog.ronin.cloud/slurm-job-arrays/ </p>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/","title":"Running nf-core/rnaseq pipeline on Tufts HPC","text":"<p>Author: Shirley Li</p> <p>Email: xue.li37@tufts.edu</p>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#introduction-to-nf-corernaseq","title":"Introduction to nf-core/rnaseq","text":"<p>nf-core/rnaseq is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.</p> <p></p>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#create-the-working-directory","title":"Create the working directory","text":"<pre><code>mkdir -p /cluster/tufts/workshop/demo/rnaseq  \n# Change the working directory to your own, DO NOT USE home directory\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#prepare-input-files","title":"Prepare Input files","text":"<ul> <li>fastq files: <code>/cluster/tufts/workshop/demo/rnaseq/input/fastq/</code></li> </ul> <pre><code>-rw-r--r-- 1 xli37 workshop 3.6G Oct 22 13:39 SRX1693954_SRR3362664_2.fastq.gz\n-rw-r--r-- 1 xli37 workshop 3.7G Oct 22 13:39 SRX1693954_SRR3362664_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 3.6G Oct 22 13:41 SRX1693953_SRR3362663_2.fastq.gz\n-rw-r--r-- 1 xli37 workshop 3.8G Oct 22 13:41 SRX1693953_SRR3362663_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 3.4G Oct 22 13:42 SRX1693951_SRR3362661_2.fastq.gz\n-rw-r--r-- 1 xli37 workshop 3.5G Oct 22 13:42 SRX1693951_SRR3362661_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.1G Oct 22 13:44 SRX1693952_SRR3362662_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.0G Oct 22 13:44 SRX1693952_SRR3362662_2.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.2G Oct 22 13:46 SRX1693956_SRR3362666_2.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.3G Oct 22 13:46 SRX1693956_SRR3362666_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.5G Oct 22 13:46 SRX1693955_SRR3362665_1.fastq.gz\n-rw-r--r-- 1 xli37 workshop 4.3G Oct 22 13:46 SRX1693955_SRR3362665_2.fastq.gz\n</code></pre> <p>The raw fastq files were downloaded using <code>fetchngs</code> pipeline, you can refer to the our previous workshop material to learn the details. </p> <p>For conducting RNAseq analysis, we also need the reference genome <code>fasta</code> file and <code>gtf</code> annotation file. Since these are human samples, we require the human reference genome. We can obtain the human reference genome from public databases such as Ensembl or UCSC genome browser.</p> <ul> <li>reference genome: fastq file (We do not need to download it locally)</li> </ul> <pre><code>https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n</code></pre> <ul> <li>reference genome annotation: gtf file</li> </ul> <pre><code>https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\n</code></pre> <p>No need to download reference files locally</p> <p>Many bioinformatics tools and workflows allow for cloud-based or remote server access, which can pull data directly from URLs like the Ensembl FTP site. This is especially useful when you have limited local storage or want to ensure you're always using the most updated version.</p> <p>Adjust it to other organisms or different versions as needed for your analysis.</p>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#prepare-the-input-samplesheet","title":"Prepare the input samplesheet","text":"<p>Let's create a <code>samplesheet.csv</code> to store the input sample information. </p> <p>There are two ways to create this file. </p> <ol> <li>Manually create and edit the file on Open OnDemand. </li> <li>Use VI editor to create and edit the file on command-line interface. </li> </ol> <p>Once your created this file, use <code>cat</code> to check the contents. Please remember the path where the samplesheet is stored, you will need this as input for nf-core/rnaseq pipeline. </p> <pre><code>cat /cluster/tufts/workshop/demo/rnaseq/samplesheet.csv\n</code></pre> <pre><code>sample,fastq_1,fastq_2,strandedness\nGFPkd_1,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693954_SRR3362664_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693954_SRR3362664_2.fastq.gz,auto\nGFPkd_2,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693953_SRR3362663_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693953_SRR3362663_2.fastq.gz,auto\nGFPkd_3,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693951_SRR3362661_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693951_SRR3362661_2.fastq.gz,auto\nPRMT5kd_1,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693952_SRR3362662_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693952_SRR3362662_2.fastq.gz,auto\nPRMT5kd_2,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693956_SRR3362666_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693956_SRR3362666_2.fastq.gz,auto\nPRMT5kd_3,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693955_SRR3362665_1.fastq.gz,/cluster/tufts/workshop/demo/rnaseq/input/fastq/SRX1693955_SRR3362665_2.fastq.gz,auto\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#nf-corernaseq-on-open-ondemand","title":"nf-core/rnaseq on Open OnDemand","text":""},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#open-ondemand-arguments","title":"Open OnDemand Arguments","text":"<ul> <li> <p>Number of hours: 24</p> </li> <li> <p>Select cpu partition: batch</p> </li> <li> <p>Reservation for class, training, workshop: Default</p> </li> <li> <p>Version: 3.16.0</p> </li> <li> <p>Working Directory: <code>/cluster/tufts/workshop/demo/rnaseq/</code> </p> </li> <li> <p>outdir: <code>/cluster/tufts/workshop/demo/rnaseq/out/</code> </p> </li> <li> <p>input: <code>/cluster/tufts/workshop/demo/rnaseq/samplesheet.csv</code></p> </li> <li> <p>multiqc_title: PRMT5kd vs. GFPkd</p> </li> <li> <p>iGenomes: None                                # We do not recommend to use iGenomes, they are outdated. </p> </li> <li> <p>fasta: https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz</p> </li> <li> <p>gtf: https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz</p> </li> <li> <p>trimmer: trimgalore</p> </li> <li> <p>aligner: star_salmon</p> </li> <li> <p>save_reference: true</p> </li> <li> <p>skip_umi_extract: true</p> </li> <li> <p>skip_pseudo_alignment: true</p> </li> <li> <p>skip_stringtie: true</p> </li> </ul> <p>Monitor job status      Click session ID and then view <code>output.log</code> file </p> <pre><code>Script starting...\nGenerating connection YAML file...\nThis is a fresh run\nYou are not using Nextflow Tower\n\nCurrently Loaded Modules:\n  1) java/15.0.2        3) squashfs/4.4        5) nf-core/2.14.1\n  2) nextflow/23.10.0   4) singularity/3.8.4\n\n\n\nnextflow run /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.16.0/3_16_0 -params-file nf-params.json -profile tufts --partition batch\nNextflow 24.04.4 is available - Please consider updating your version to it\nN E X T F L O W  ~  version 23.10.0\nLaunching `/cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.16.0/3_16_0/main.nf` [deadly_knuth] DSL2 - revision: f68f604b04\nWARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.16.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : deadly_knuth\n  containerEngine           : singularity\n  launchDir                 : /cluster/tufts/workshop/demo/rnaseq\n  workDir                   : /cluster/tufts/workshop/demo/rnaseq/work\n  projectDir                : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.16.0/3_16_0\n  userName                  : xli37\n  profile                   : tufts\n  configFiles               : \n\nInput/output options\n  input                     : /cluster/tufts/workshop/demo/rnaseq/samplesheet.csv\n  outdir                    : /cluster/tufts/workshop/demo/rnaseq/out/\n  multiqc_title             : PRMT5kd_vs_GFPkd\n\nReference genome options\n  fasta                     : https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n  gtf                       : https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\n  igenomes_base             : /cluster/tufts/biocontainers/datasets/igenomes/\n\nUMI options\n  umitools_grouping_method  : unique\n\nAlignment options\n  min_mapped_reads          : 5\n\nOptional outputs\n  save_reference            : true\n\nProcess skipping options\n  skip_umi_extract          : true\n  skip_pseudo_alignment     : true\n  skip_stringtie            : true\n\nInstitutional config options\n  custom_config_base        : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.16.0/3_16_0/../configs/\n  config_profile_description: The Tufts University HPC cluster profile provided by nf-core/configs.\n  config_profile_contact    : Yucheng Zhang\n  config_profile_url        : https://it.tufts.edu/high-performance-computing\n\nMax job request options\n  max_cpus                  : 72\n  max_memory                : 120 GB\n  max_time                  : 7d\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nIf you use nf-core/rnaseq for your analysis please cite:\n\n* The pipeline\n  https://doi.org/10.5281/zenodo.1400710\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/rnaseq/blob/master/CITATIONS.md\n------------------------------------------------------\nWARN: The following invalid input values have been detected:\n\n* --partition: batch\n* --config_profile_contact_github: @zhan4429\n* --config_profile_contact_email: Yucheng.Zhang@tufts.edu\n\n\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:PREPARE_GENOM... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... -\n\n\n.\n.\n.\n\n[64/9877b8] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[75/ba92e4] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[d2/6912c4] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[16/06771e] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[28/384ace] process &gt; NFCORE_RNASEQ:RNASEQ:DESEQ2... [100%] 1 of 1 \u2714\n[8f/c84523] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[d2/ed264c] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[e6/6724f5] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[d8/656f2c] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[85/b2e2de] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[d6/66075d] process &gt; NFCORE_RNASEQ:RNASEQ:SUBREA... [100%] 6 of 6 \u2714\n[40/754888] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQ... [100%] 6 of 6 \u2714\n[26/3aa9ff] process &gt; NFCORE_RNASEQ:RNASEQ:BEDTOO... [100%] 6 of 6 \u2714\n[ec/03ad32] process &gt; NFCORE_RNASEQ:RNASEQ:BEDTOO... [100%] 6 of 6 \u2714\n[cd/237c01] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[0a/60f7b2] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[83/5dc39e] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[c2/1382f2] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[79/856cc8] process &gt; NFCORE_RNASEQ:RNASEQ:QUALIM... [100%] 6 of 6 \u2714\n[a6/749287] process &gt; NFCORE_RNASEQ:RNASEQ:DUPRAD... [100%] 6 of 6 \u2714\n[21/bbeca0] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[98/6266fd] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[bc/8c3167] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[01/736952] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[a0/511c10] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[33/979a09] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[71/9d4406] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[fc/54e093] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQ... [100%] 1 of 1 \u2714\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 22-Oct-2024 17:20:09\nDuration    : 3h 5m 33s\nCPU hours   : 136.4\nSucceeded   : 214\n\n\nCleaning up...\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#nf-corernaseq-on-the-command-line-interface","title":"nf-core/rnaseq on the command line interface","text":"<p>If you prefer to run the pipelines using the command line interface, you can submit a slurm jobscript with the following code.</p>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#run-the-pipeline-using-nextflow-run","title":"Run the pipeline using <code>nextflow run</code>.","text":"<pre><code>#!/bin/bash\n#SBATCH --time=24:00:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH -c 2\n#SBATCH --output=MyJob.%j.%N.out\n#SBATCH --error=MyJob.%j.%N.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=utln@tufts.edu\n\nmodule load nf-core\nexport NXF_SINGULARITY_CACHEDIR=/cluster/tufts/biocontainers/nf-core/singularity-images\n\nnextflow run /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.16.0/3_16_0\n  -profile tufts \\\n  --input  /cluster/tufts/workshop/demo/rnaseq/samplesheet.csv \\\n  --outdir /cluster/tufts/workshop/demo/rnaseq/out/ \\\n  --gtf \"https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\" \\\n  --fasta \"https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\" \\\n  --extra_trimgalore_args \"-q 35 --paired\" \\\n  --skip_pseudo_alignment \\\n  --save_reference\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#run-the-pipeline-using-our-modules","title":"Run the pipeline using our modules.","text":"<pre><code>#!/bin/bash\n#SBATCH --time=24:00:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH -c 2\n#SBATCH --output=MyJob.%j.%N.out\n#SBATCH --error=MyJob.%j.%N.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=utln@tufts.edu\n\nmodule load nf-core-rnaseq/3.16.0\nrnaseq -profile tufts \\\n  --input  /cluster/tufts/workshop/demo/rnaseq/samplesheet.csv \\\n  --outdir /cluster/tufts/workshop/demo/rnaseq/out/ \\\n  --gtf \"https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\" \\\n  --fasta \"https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\" \\\n  --extra_trimgalore_args \"-q 35 --paired\" \\\n  --skip_pseudo_alignment \\\n  --save_reference\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#nextflow-clean","title":"Nextflow clean","text":""},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#clean-the-work","title":"Clean the work","text":"<p>You can clean the <code>work</code> directory, by mannualy run</p> <pre><code>rm -rf work\n</code></pre>"},{"location":"2024_workshops/2024_bioinformatics401/03_nfcore/#next-step","title":"Next Step","text":"<p>For differential abundance analysis, please refer to our previous tutorial on nf-core/differentialabundance pipeline. </p>"},{"location":"2024_workshops/Cluster_for_biologist/00_Introduction/","title":"Description","text":"<p>This workshop focues on teaching basic skills to use the command line interface, introduce bioinformatics resources at Tufts, and provide guidance on initiating bioinformatics analysis.</p>"},{"location":"2024_workshops/Cluster_for_biologist/00_Introduction/#learning-objectives","title":"Learning Objectives","text":"<ol> <li>Navigate around the command line interface. </li> <li>Create and manipulate text files.</li> <li>Submit jobs to a high-performance computing cluster.</li> <li>Explore and utilize Open OnDemand.</li> <li>Transfer files to and from the cluster. </li> </ol>"},{"location":"2024_workshops/Cluster_for_biologist/00_Introduction/#requirements","title":"Requirements","text":""},{"location":"2024_workshops/Cluster_for_biologist/00_Introduction/#e-list","title":"E-List","text":"<p>To find out about Bioinformatics Education, Software and Tools you can subscribe to our e-list: best@elist.tufts.edu </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/","title":"In class hands-on session","text":"<p>Class ID: Sp24-IDGH-1001-1-Bioinformatics   Canvas link: https://canvas.tufts.edu/courses/55751</p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#introduction-to-metagenomics-session-2","title":"Introduction to Metagenomics - Session 2","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#learning-objective","title":"Learning Objective","text":"<ol> <li>NCBI Database Proficiency: Develop skills to efficiently locate and interpret data on the NCBI database, including navigating to specific BioProject and SRA experiment pages.</li> <li>Data Retrieval from Published Papers: Gain the ability to identify and extract relevant raw data and metadata from published scientific papers.</li> <li>Metagenomic Sequencing Platforms: Learn about different sequencing platforms by analyzing their data characteristics, specifically focusing on Illumina and Nanopore technologies.</li> <li>Taxonomic Analysis: Acquire practical experience in assigning taxonomic labels to sequencing reads using Kraken2 on Tufts Galaxy, and in converting and visualizing these labels with Krona.</li> <li>Data Comparison and Interpretation: Enhance skills in comparing visualized data with NCBI SRA information and drawing conclusions about sample composition.</li> </ol>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#exercise-1-ncbi-database-navigation","title":"Exercise 1: NCBI Database Navigation","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#objective","title":"Objective:","text":"<p>In this section, you will learn to navigate the NCBI database. Known for its comprehensive nature, the database hosts a variety of sections, each tailored to specific types of datasets. Your focus will be on mastering the ability to identify and navigate through BioProject pages, SRA experiment pages, and SRA Runs linked to published research papers. This proficiency is crucial in the field of bioinformatics, enabling us to effectively leverage previous research as a foundation for new discoveries and advancements in the study of biological data.</p> <p> </p> <p>A screenshot of NCBI website </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#instructions","title":"Instructions:","text":"<p>Your task involves exploring the NCBI database to gather specific information:</p> <ol> <li>Review the research paper \"Evaluation of full-length nanopore 16S sequencing for detection of pathogens in microbial keratitis\" . Your goal is to identify the associated BioProject ID within the paper.<ul> <li>Hint: BioProject ID is PRJEB37709 in the \"Data Availability\" section of the paper.        </li> </ul> </li> <li>Answer the following queries regarding the BioProject, and keep a record of your findings:              a. What is the URL for this specific BioProject?           b. Total number of biosamples included in this BioProject.           c. Total number of SRA experiments associated with this BioProject.             d. Determine the sequencing platform used for SRA run ERR4836970.          </li> <li>Apply the same procedure as step 2 for a second research paper: \"Benchmarking second and third-generation sequencing platforms for microbial metagenomics\".<ul> <li>Hint: BioProject ID is in the \"Data Records\" section of the paper. </li> </ul> </li> </ol>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#exercise-2-ncbi-database-exploration","title":"Exercise 2: NCBI Database Exploration","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#objective_1","title":"Objective:","text":"<p>Engage in a hands-on exercise to explore the NCBI database using specific SRA run IDs. Your task will involve navigating various sections of the database and applying your understanding of sequencing technologies to hypothetical research scenarios.   </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#instructions_1","title":"Instructions:","text":"<p>Utilize the SRA run ID to search the NCBI website. Explore the corresponding SRA, BioSample, and BioProject sections related to this SRA run.            1. Assignment Completion: Choose one SRA run and document your findings in the provided Google Spreadsheet: Exercise Spreadsheet.</p> <p> </p> <p>A screenshot of the spreadsheet </p> <ol> <li>Questions for Analysis            a. Mars Soil Sample Analysis: If you obtained a soil sample from Mars for identifying microorganisms and assembling their genomes, which sequencing technology would be optimal? Consider factors like the detection of novel organisms and the precision required for genome assembly. Discuss your choice, focusing on read length, accuracy, and cost implications.            b. Gut Microbiome Study: In researching the impact of dietary changes on the gut microbiome, what type of sample would you collect, and which sequencing technology would be most suitable? Provide your rationale for this choice.         </li> </ol> <p>Additional Resources: Hints for these questions can be found here.        </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#exercise-3-taxonomy-assignment-and-interpretation","title":"Exercise 3 Taxonomy assignment and interpretation.","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#objective_2","title":"Objective:","text":"<p>Use Kraken2 for taxonomy assignment and visualize the results with a Krona plot. Interpretate and present the result. </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#instructions_2","title":"Instructions:","text":"<p>[!NOTE] The tools we will use for this analysis are:    - Download and Extract Reads in FASTA/Q   - Kraken2         - filter      - sort               1. Log in to your Galaxy account. 2. Name the history as \"Session 2 Metagenomics-ERR12302112\" by double clicking the \"Unnamed history\". 3. Now let's start the analysis:            1. Under tools on the far left of the page, search for Download and Extract Reads in FASTA/Q format from NCBI SRA, run the tool with the following parameters:                           - Accession: ERR12302112                           - Click Execute      2. Kraken2 assign taxonomic labels to sequencing reads with the following parameters:                     - Single or paired end: Single                 - Input Sequences: the output from last step. Ex: 1.ERR12302112 (fastq-dump)                   - Click Create Report, then set Print a report with aggregrate counts/clade to file to Yes                  - Select a Kraken2 database: Minikraken2 v2           Note this step will create two output files </p> <pre><code>3. **Filter** data on any column using simple expressions with the following parameters:       \n        - **Filter**: the *report* output from last step. Ex: Report: Kraken2 on data 1        \n        - **With following condition**: c4==\"S\"       \n         *This will keep the rows whose fourth column has a character S, S stands for species*      \n4. **Sort** data in ascending or descending order with the following parameters:         \n        - **Sort Dataset**: the output file from filter. Ex: Filter on data 2       \n        - **with flavor**: Numerical sort       \n        - **everything in**: Descending order                  \n*Take a look at the output file, the first few lines should be like this:*\n\n   &lt;img width=\"743\" alt=\"Screenshot 2024-01-25 at 16 30 26\" src=\"https://github.com/shirleyxueli41/Tufts_workshops/assets/88347911/d284b4de-d6db-43fb-b14b-5c2452823902\"&gt;\n</code></pre>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#in-class-assignment","title":"In-class assignment:","text":"<p>Divide into teams (either two or three teams). Each team should select one SRA run from the provided google spreadsheet. Then, replicate the previously outlined steps to identify the top three prevalent species. Research one or two of these species using Google, and compare your findings with the samples to check for coherence. Each team will be given five minutes to showcase their findings. An example report can be found here.             </p> <p>[!WARNING]      Warning: Ensure you generate a fresh history and assign a distinct name for the analysis.  Click the \"+\" button on the top right to create new history session.   </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#exercise-4-taxonomy-visualization","title":"Exercise 4 Taxonomy visualization.","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#objectives","title":"Objectives:","text":"<p>The exercise aims to utilize Krona for creating interactive visualizations of taxonomic data, highlighting the tool's effectiveness in representing complex hierarchical structures. It also involves a comparison with NCBI Kroa, assessing differences in visualization techniques and data representation. </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#instructions_3","title":"Instructions:","text":"<ol> <li>Switch back to the session \"Session 2 Metagenomics-ERR12302112\".</li> <li>Krakentools: Convert kraken report file to krona text file with the following parameters:                <ul> <li>Kraken report file: The report output from Kraken2. Ex: Report: Kraken2 on data1         This will generate an output called \"Krakentools: Convert kraken report file on data 2\" </li> </ul> </li> <li>Visualize with Krona Visualize any hierarchical data with the following parameters:<ul> <li>Select input file: Krakentools: Convert kraken report file on data 2.                This will generate an output called \"Krona on data 5: HTML\" </li> </ul> </li> <li>Compare the Krona plot with it on NCBI SRA. Link is here.   <ul> <li>Click Show Krona View      NCBI uses Sequence Taxonomic Analysis Tool (STAT), a scalable k-mer-based tool for fast assessment of taxonomic diversity intrinsic to submissions, independent of metadata.</li> </ul> </li> </ol>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Hands-on_session/#reference","title":"Reference","text":"<p>https://bisonnet.bucknell.edu/files/2021/05/Kraken2-Help-Sheet.pdf   https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-020-00900-2   https://jddtonline.info/index.php/jddt/article/view/5433   https://www.sciencedirect.com/science/article/pii/S094450132200194X?via%3Dihub    https://benlangmead.github.io/aws-indexes/k2          </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/","title":"Take-home exercise","text":"<p>Class ID: Sp24-IDGH-1001-1-Bioinformatics   Canvas link: https://canvas.tufts.edu/courses/55751/assignments</p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#exercise-1","title":"Exercise 1","text":""},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#11","title":"1.1","text":"<p>Visit the NCBI to search for the BioProject PRJNA891065 and anwser the following questions:           </p> <ol> <li>What is this project about? Summarize in a few sentences.     </li> <li>How many biosamples are included in the project?</li> <li>Which gene is being sequenced, the 16S or 18S rRNA gene?</li> </ol>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#12","title":"1.2","text":"<p>The paper for the bioproject is available here. Combining omics tools for the characterization of the microbiota of diverse vinegars obtained by submerged culture: 16S rRNA amplicon sequencing and MALDI-TOF MS</p> <p>Summarize the abstract of the paper in no more than three sentences.</p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#13","title":"1.3","text":"<p>For this SRA run SRR21926282, answer the following questions:     </p> <ol> <li>What sequencing instrument was used?</li> <li>What sequencing strategy was applied?</li> <li>Are the reads paired-end or single-end?</li> <li>How many reads were generated in this sequencing run?</li> <li>What is the biosample for this run? Which sample is it? Check the \"environmental medium\" on the biosample page.</li> </ol>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#exercise-2","title":"Exercise 2","text":"<p>Review the materials on \"Hands-on sesssion.md\", especially the session \"Exercise 3 Taxonomy assignment and interpretation.\" and \"Exercise 4 Taxonomy visualization.\". </p> <p>For the samples provided, please follow these steps: First, identify the Sequence Read Archive (SRA) run numbers associated with each sample. Next, utilize the Galaxy platform to process these SRA runs. Your goal is to ascertain the top three genera present in each sample. Additionally, for each sample, create a Krona chart to visually represent the taxonomic classification of the organisms found.</p> <p>SAMN31308859     SAMN31308863    SAMN31308856    SAMN31308853      </p> <p>[!WARNING]      Identify the top three genera instead of species. To do so, for the filtering step in Hands-on exercise.md , replace \"c4==S\" to \"c4==G\" S stands for species, G stands for genera. </p> <p></p> <p>Analyze and compare the results from the four SRA runs, particularly between samples from wineries and breweries. Write a report including the answers to the questions above, results from Galaxy, and screenshots, in a Microsoft Word document.</p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise/#assignment-rubric-25-points-total","title":"Assignment - Rubric (25 points total)","text":"<p>1 Exercise 1.1 (1 point for each question, 3 points in total)     2 Exercise 1.2 (2 points)    3 Exercise 1.3 (1 point for each question, 5 points in total)     4 Exercise 2:        Identify the SRA ID (1 point for each sample, 4 points in total)     Process the SRA in Galaxy and identify the top three genera for each sample. (1 point for each sample, 4 points in total) * Visualize the results with Krona plots (1 point for each sample, 4 points in total) * Summarize the results, with a comparison between winery and brewery samples. Remember to compare between winery samples and brewery samples. (3 points)</p> <p>[!NOTE]      Please don't hesitate to reach out if you have any questions. </p>"},{"location":"2024_workshops/IGDH-1001_2024Feb/Take-home_exercise_announcement/","title":"Take-home exercise announcement","text":"<p>This assignment (https://github.com/shirleyxueli41/Tufts_workshops/blob/main/IGDH-1001_2024Feb/Take-home%20exercise.md ) is structured into two main exercises focused on exploring and analyzing biological data related to the microbiota of diverse vinegars. Exercise 1 is divided into three parts: * 1.1 asks students to research a specific bioproject (PRJNA891065) on the NCBI database to summarize the project, count the biosamples, and identify the genes sequenced. * 1.2 involves reading and summarizing the abstract of a research paper associated with the bioproject, focusing on the characterization of vinegar microbiota using omics tools. * 1.3 requires detailed information about a specific SRA run (SRR21926282), including sequencing instruments, strategy, read type, total reads, and sample details from the biosample page.</p> <p>Exercise 2 instructs students to review materials on a \"Hands-on session,\" identify specific SRA runs from given samples, process them using Galaxy, and analyze the top 3 genera and create a Krona plot for each sample. The exercise emphasizes comparing samples from wineries and breweries. The assignment concludes with writing a report that includes answers to all questions, results from Galaxy analyses, screenshots, and a summary comparing winery and brewery samples. The rubric outlines the points allocation for each task, totaling 25 points.</p> <p>Assignment - Rubric (25 points total) 1 Exercise 1.1 (1 point for each question, 3 points in total)\u20282 Exercise 1.2 (2 points)\u20283 Exercise 1.3 (1 point for each question, 5 points in total)\u20284 Exercise 2: * Identify the SRA ID (1 point for each sample, 4 points in total) * Process the SRA in Galaxy and identify the top three genera for each sample. (1 point for each sample, 4 points in total) * Visualize the results with Krona plots (1 point for each sample, 4 points in total) * Summarize the results, with a comparison between winery and brewery samples. Remember to compare between winery samples and brewery samples. (3 points)</p> <p>Please don't hesitate to reach out if you have any question.  Shirley Li: xue.li37@tufts.edu</p> <p>Warning: we ask to identify the top three genera for each sample instead of species in the take-home exercise. </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/00_introduction/","title":"Cas12a Mutant Structural Predictions With AlphaFold2","text":"<p>Materials designed for class CHBE0165 (Biomolecular Engineering and Design) Spring 2024.</p> <p>Content developed by TTS Research Technology</p> <ul> <li>Shirley Li, PhD, Bioinformatician, xue.li37@tufts.edu         </li> <li>Jason Laird, MSc, former Bioinformatics Scientist</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/00_introduction/#the-research-technology-team","title":"The Research Technology Team","text":"<ul> <li>Consultation on Projects and Grants</li> <li>High Performance Compute Cluster</li> <li>Workshops</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/00_introduction/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download PyMOL</li> <li>Account on the HPC (Note: To test your account log into OnDemand. If you can log in, you are all set, if not please reach out to tts-research@tufts.edu)</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/00_introduction/#contents","title":"Contents","text":"<ol> <li>Run AlphaFold2 on Tufts HPC CLI</li> <li>Run AlphaFold2 on Tufts HPC with Open OnDemand and Accuracy Assessment</li> <li>Vizualize Predicted Structure with PYMOL</li> <li>Slides for this lecture is here.</li> </ol>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/00_introduction/#e-list","title":"E-List","text":"<p>To find out about Bioinformatics Education, Software and Tools you can subscribe to our e-list: best@elist.tufts.edu </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/","title":"Run AlphaFold2 on Tufts HPC with Command Line Interface (CLI)","text":"<p>This tutorial will guide the reader through the process of running AlphaFold2 on the Tufts High Performance Computing (HPC) system using the command line interface.</p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Tufts HPC.</li> <li>Basic familiarity with Unix/Linux command line.</li> <li>Basic familiarity with modules: Understanding how to load and unload software modules in a Unix/Linux environment. </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/#navigate-to-the-cluster-and-prepare-the-input-file","title":"Navigate To The Cluster and prepare the input file","text":"<p>Navigate to: https://ondemand.pax.tufts.edu/</p> <ul> <li>Log in with your Tufts credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code> </li> <li>Type <code>pwd</code> to check your current directory. You should be at your home directory, which is listed as <code>/cluster/home/your_utln/</code> </li> <li>Type <code>ls</code> to list the files in your current directory.      </li> <li>Copy the example folder and data to your home directory by following command          <pre><code>cp -r /cluster/tufts/bio/tools/training/cas12a_af2_sp24/ ./ \n</code></pre></li> <li>Type <code>ls</code> to list the files in your current directory after copying the files.      </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/#prepare-your-input-data","title":"Prepare your input data","text":"<p>Input data for AlphaFold2 typically includes the amino acid sequences of the proteins you wish to model.    </p> <p>Your input data should be located here: <code>/cluster/home/your_utln/cas12a_af2_sp24/5XUS_mut2cwf_modified.fasta</code> Remember to replace <code>your_utln</code> with your own tufts credentia</p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/#executing-alphafold2","title":"Executing AlphaFold2","text":"<p>Run AlphaFold2 using a command that specifies your input files, output directory, and other parameters. An example script is provided here </p> <p>Remember to edit the script before you submit it to the queue  You will need to replace the path to your own path.</p> <p>To submit your job to gpu node, type the following command    <pre><code>sbatch /cluster/home/your_utln/cas12a_af2_sp24/script/runaf.sh\n</code></pre></p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/01_Run_AlphaFold2_CLI/#check-the-results","title":"Check the results","text":"<p>ranked_0.pdb is your best predicted structure. This file contains the highest-ranked prediction based on the analysis, indicating it is considered the most accurate or reliable structure prediction from the set of models used.    <pre><code>cd /cluster/home/your_utln/cas12a_af2_sp24/out/\n</code></pre> </p> <p>[!NOTE] If you encounter any issues or this doesn't work as expected, please feel free to reach out to Shirley Li, xue.li37@tufts.edu   </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/","title":"Run AlphaFold2 on Tufts HPC with Open OnDemand App","text":"<p>This tutorial will guide the user through the process of running AlphaFold2 on Tufts High Performance Computing (HPC) system using Open Ondemand app, a graphical interface to the HPC.     </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Tufts HPC: If you don't already have an account, you can apply for a HPC account by submitting this form.</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#navigate-to-the-cluster-and-prepare-the-input-file","title":"Navigate To The Cluster and prepare the input file","text":"<p>Navigate to: https://ondemand.pax.tufts.edu/</p> <ul> <li>Log in with your Tufts credentials</li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code> </li> <li>Type <code>pwd</code> to check your current directory. You should be at your home directory, which is listed as <code>/cluster/home/your_utln/</code> </li> <li>Type <code>ls</code> to list the files in your current directory.      </li> <li>Copy the example folder and data to your home directory by following command          <pre><code>cp -r /cluster/tufts/bio/tools/training/cas12a_af2_sp24/ ./ \n</code></pre></li> <li>Type <code>ls</code> to list the files in your current directory after copying the files, you can now see the <code>cas12a_af2_sp24/</code> folder.       </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#prepare-your-input-data","title":"Prepare your input data","text":"<p>Input data for AlphaFold2 typically includes the amino acid sequences of the proteins you wish to model.    </p> <p>Your input data should be located here: <code>/cluster/home/your_utln/cas12a_af2_sp24/5XUS_mut2cwf_modified.fasta</code> Remember to replace <code>your_utln</code> with your own tufts credential</p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#run-alphafold2","title":"Run AlphaFold2","text":""},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#navigate-to-httpsondemandpaxtuftsedu","title":"Navigate to: https://ondemand.pax.tufts.edu/","text":"<ul> <li>Log in with your Tufts credentials</li> <li>On the top menu bar choose <code>Bioinformatics Apps-&gt;AlphaFold</code> </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#enter-the-following-parameters-to-run-the-job","title":"Enter the following parameters to run the job.","text":"<ul> <li><code>Number of hours: 24</code> </li> <li><code>Number of cores: 8</code> </li> <li><code>Amount of memory: 32GB</code> </li> <li><code>Select preempt or normal gpu partition: gpu</code> </li> <li><code>Select the GPU type: a100</code> </li> <li><code>Software Version: 2.3.2</code> </li> <li><code>Database: 20231031</code> </li> <li><code>Working Directory: /cluster/home/your_utln/cas12a_af2_sp24/</code></li> <li><code>Output directory Name: /cluster/home/your_utln/cas12a_af2_sp24/</code> </li> <li><code>fasta_paths: /cluster/home/your_utln/cas12a_af2_sp24/5XUS_mut2cwf_modified.fasta</code></li> <li><code>model_preset: multimer</code></li> <li><code>models_to_relax: best</code> </li> <li><code>num_multimer_predictions_per_model: 1</code></li> <li><code>max_template_date: 2020-01-01</code>  Then, hit <code>launch</code> </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#check-output-files","title":"Check output files","text":"<p>To check the output files, go back to the main page of Open Ondemand, on the top menu bar choose <code>Files -&gt; Home Directory</code>  Click <code>cas12a_af2_sp24</code>, then click <code>5XUS_mut2cwf_modified</code>, you will see the output. Given that this task requires a few hours to complete, proceed to the <code>5XUS_mut2cwf_modified_prerun</code> folder. This location contains the pre-executed results of AlphaFold2 along with the stored output files.   </p> <p>Here's the explanation of the file categories listed:</p> <ul> <li>JSON Files (confidence_model &amp; pae_model): Predictive data files from different models, indicating confidence levels and pairwise alignment errors.</li> <li>Pickle File (features.pkl): Serialized file containing features used for model prediction. Directory (msas): A folder containing multiple sequence alignments related to the analysis.</li> <li>CIF &amp; PDB Files (ranked_, relaxed_model_, unrelaxed_model_): Structural data files in two formats, showing the ranked and modelled protein structures, both relaxed and unrelaxed.</li> <li>JSON Files (ranking_debug.json, relax_metrics.json, timings.json): Debugging, metrics, and timing information for the analysis process.</li> <li>Pickle Files (result_model_): Serialized files containing comprehensive results from each model prediction.</li> </ul> <p><code>ranked_0.pdb</code> is your best predicted structure.  This file contains the highest-ranked prediction based on the analysis, indicating it is considered the most accurate or reliable structure prediction from the set of models used. You should be able to download this file with the <code>download</code> button.    </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#alphafold2-accuracy-assessment","title":"AlphaFold2 Accuracy Assessment","text":"<p>[!NOTE]   To continue with this section, you will need to know some linux basics. More information can be found here.     </p> <p>We can assess the accuracy of the AlphaFold prediction using:</p> <ul> <li>Predicted Local Distance Difference Test (pLDDT)</li> <li>Predicted Alignment Error</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#predicted-local-distance-difference-test-plddt","title":"Predicted Local Distance Difference Test (pLDDT)","text":"<ul> <li>per-residue confidence metric  ranging from 0-100 (100 being the highest confidence)</li> <li>Regions below 50 could indicate disordered regions</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#predicted-alignment-error-pae","title":"Predicted Alignment Error (PAE)","text":"<ul> <li>The Predicted Alignment Error (PAE) gives us an expected distance error based on each residue.</li> <li>If we are more confident that the distance between two residues is accurate, then the PAE is lower (darker green). If we are less confident that the distance between two residues is accurate, the PAE is higher (lighter green)</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<ul> <li> <p>Now that we have an idea of what these metrics mean, let's try generating these plots for the Cas12a-CWF mutant on the cluster. First navigate to: https://ondemand.pax.tufts.edu/</p> </li> <li> <p>Log in with your Tufts credentials</p> </li> <li>On the top menu bar choose <code>Clusters-&gt;Tufts HPC Shell Access</code></li> </ul> <p></p> <ul> <li>You'll see a welcome message and a bash prompt, for example for user <code>tutln01</code>:</li> </ul> <pre><code>[tutln01@login001 ~]$\n</code></pre> <ul> <li>This indicates you are logged in to the login node of the cluster. Please do not run any program from the login node.</li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<ul> <li>To run our analyses we will need to move from the login node to a compute node. We can do this by entering:</li> </ul> <pre><code>srun -p batch --time=3:00:00 -n 2 --mem=4g --pty bash\n</code></pre> <p>Explanation of Commands</p> <pre><code>- `srun`: SLURM command to run a parallel job\n- `-p`: asking for a partition, here we are requesting the batch partition\n- `--time`: time we need here we request 3 hours\n- `-n`:  number of CPUs needed here we requested 2\n- `--mem`:  memory we need here we request 4 Gigabytes\n- `--reservation`: the reservation of compute resources to use here we use the `chbe165` reservation\n- `--pty`: get a pseudo bash terminal\n</code></pre> <ul> <li>When you get a compute node you'll note that your prompt will no longer say login and instead say the name of the node:</li> </ul> <pre><code>[tutln01@c1cmp048 ~]$\n</code></pre>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#set-up-for-analysis","title":"Set Up For Analysis","text":"<ul> <li>Navigate to the folder that we just ran alphafold with:          Remember to replace <code>your_utln</code> with your own utln!!! </li> </ul> <pre><code>cd /cluster/home/your_utln/cas12a_af2_sp24 \n</code></pre> <ul> <li>Given that AlphaFold2 can take anywhere from a few hours to a few days to run - AlphaFold2 predictions have already been generated for the Cas12a-CWF mutants from our study. We will use a script from the VIB Bioinformatics Core to visualize the accuracy of AlphaFold2's predictions. First we will need to load the software needed to run that script: <p>[!WARNING] Do not change the module version. Stick with <code>alphafold/2.1.1</code>.        </p> </li> </ul> <pre><code>module load alphafold/2.1.1\n</code></pre>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#run-the-python-script","title":"Run the python script","text":"<ul> <li> <p>Now we will need to feed our script three arguments:</p> <ul> <li><code>--input_dir</code> input directory with model files </li> <li><code>--output_dir</code> output directory to put our plots of model information</li> <li><code>--name</code> optional prefix to add to our file names</li> </ul> </li> </ul> <p>[!NOTE] Before you run the script, make sure you are at <code>/cluster/home/your_utln/cas12a_af2_sp24</code></p> <pre><code>python script/af2_accuracy_viz.py --input_dir ./5XUS_mut2cwf_modified_prerun --output_dir ./ --name mut2cwf_modified\n</code></pre>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/02_Run_AlphaFold2_OpenOndemandApp/#check-the-output-files","title":"Check the output files","text":"<ul> <li> <p>Running this will generate two images in your current directory:</p> <ul> <li><code>mut2cwf_modified_coverage_LDDT.png</code> - plots of your msa coverage and pLDDT scores per residue per model</li> <li><code>mut2cwf_modified_PAE.png</code> - plots of your predicted alignment error </li> </ul> </li> <li> <p>The following are the pLDDT and PAE scores for the Cas12a-CWF mutant:    </p> <p><code>mut2cwf_modified_coverage_LDDT.png</code>  The left-side heat map showcases the Multiple Sequence Alignment (MSA), with each sequence aligned against the input sequences. The color scale reflects the identity score, arranging sequences from top (highest identity) to bottom (lowest identity). Uncovered areas appear white, indicative of subsequences in the database that do not fully align. A black line delineates the extent of sequence coverage relative to the total number of sequences aligned.    The right-side plot displays the predicted LDDT per residue position. </p> <p><code>mut2cwf_modified_PAE.png</code> </p> <p>The PAE (Pairwise Absolute Error) plot visually represents the absolute error in the relative positioning of residues, measured in \u00c5ngstr\u00f6ms, through pairwise comparison. Utilizing a color gradient from blue to red, dark blue indicates an error of 0 \u00c5, while larger errors shift towards dark red. Typically, along the heat map's diagonal, values are anticipated to be near 0 \u00c5, signifying minimal error. An optimal model is characterized by a predominantly dark blue heat map, denoting very low error across all comparisons.</p> </li> </ul> <p>[!NOTE] If you encounter any issues or this doesn't work as expected, please feel free to reach out to Shirley Li, xue.li37@tufts.edu      </p>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/03_Vizualize_predicted_structure_with_PYMOL/","title":"Visualize alphafold2 predicted structure with PYMOL","text":"<ul> <li>In the previous slide we plotted our MSA alignment, the pLDDT scores, and the predicted alignement error. However, it is also useful to visualize the actual predicted protein structure and compare it to the known structure if there is one. Here we use a software called PyMOL to do just that:</li> </ul> <ul> <li>Here we see that PyMOL takes either the PDB ID or a PDB file and creates a vizualization for us to examine. If you have not done so already please download PyMOL and open the app. You should see a window like the follwing:</li> </ul> <ul> <li>Here we have a:</li> <li>History Window with log of previous commands</li> <li>Command Interface to enter PyMOL commands</li> <li>List of Objects Loaded which list of objects/proteins that have been loaded into PyMOL</li> <li> <p>Visualization Window to visualize protiens loaded into PyMOL</p> </li> <li> <p>Let's try on our data!</p> </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/03_Vizualize_predicted_structure_with_PYMOL/#download-alphafold-output","title":"Download AlphaFold Output","text":"<ul> <li>First we will need to download our predicted structure pdb file for the mutant, Cas12a mut2-CWF. To this, go to <code>Files &gt; Home Directory</code>:</li> </ul> <ul> <li>Now, click on <code>cas12a_af2_sp24</code> and download the following file, then click <code>5XUS_mut2cwf_modified_prerun</code>, and download <code>ranked_0.pdb</code>: </li> </ul>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/03_Vizualize_predicted_structure_with_PYMOL/#importing-structures","title":"Importing Structures","text":"<ul> <li>To visualize this protein structure in PyMOL, open PyMOL on your computer</li> <li>Go to File &gt; Open - then choose your pdb file, <code>ranked_0.pdb</code></li> <li>The file we have loaded is the Cas12a-CWF mutant, let's fetch the structure for the wild type Cas12a protein:</li> </ul> <pre><code>fetch 5xus\n</code></pre> <ul> <li>We will now align this structure with the Cas12a-CWF mutant. So in the PyMOL command prompt enter:</li> </ul> <pre><code>align ranked_0, 5xus\n</code></pre>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/03_Vizualize_predicted_structure_with_PYMOL/#viewing-structures","title":"Viewing Structures","text":"<ul> <li>To view one structure at a time, you can use the <code>disable</code> command to hide one of the structures:</li> </ul> <pre><code>disable 5xus\n</code></pre> <ul> <li>To see this structure again we can simply use the <code>enable</code> command:</li> </ul> <pre><code>enable 5xus\n</code></pre>"},{"location":"2024_workshops/cas12aAlphaFold2_sp24/03_Vizualize_predicted_structure_with_PYMOL/#visualizing-variants","title":"Visualizing Variants","text":"<ul> <li>Now that we have aligned/colored our structures, let's select residues on the RuvC Domain on the Cas12a mut2-CWF and the Cas12a wild-type:</li> </ul> <pre><code>select resi 863+952+965+1214 and name CA\n</code></pre> <p>[!NOTE]         Note we are only selecting the alpha carbons so that when we label these residues we only have one label per residue</p> <ul> <li>To label these residues we can use the following:</li> </ul> <pre><code>label sele, \" %s%s\" % (resn,resi)\n</code></pre> <ul> <li>With these residues selected we can color them to visualize them easier:</li> </ul> <pre><code>color red, sele\n</code></pre> <ul> <li>Let's now zoom into this region:</li> </ul> <pre><code>zoom sele\n</code></pre> <ul> <li> <p>Use your mouse to drag and rotate the structure to take a look at differen angles of it. Locate V863, can you see the difference between wild type protein structure <code>5XUS</code> and our predicted structure <code>ranked_0</code>? </p> </li> <li> <p>To capture this image we can go to <code>File &gt; Export Image As &gt; PNG... &gt; Save PNG image as ...</code> and enter a name for your image!</p> </li> <li> <p>Alternatively, you can take a screen shot and save.</p> </li> <li> <p>Reference Paper:  Ma et al. 2022 </p> </li> </ul> <p>[!NOTE] If you encounter any issues or this doesn't work as expected, please feel free to reach out to Shirley Li, xue.li37@tufts.edu    </p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/00_introduction/","title":"Nextflow and nf-core at Tufts HPC","text":"<p>This repository stores the slides and hands-on sessions for nf-core and nextflow training workshops provided by Tufts Research Technology in April 2024.</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/00_introduction/#nextflow","title":"Nextflow","text":"<p>Nextflow is a software tool used to design and run scientific workflows, particularly in bioinformatics. It allows researchers to automate complex data analysis processes by chaining together smaller tasks. Here are some key features:</p> <ul> <li>Scalability: It can handle large datasets and run on various computing environments, including local machines, clusters, and clouds.</li> <li>Reproducibility: By using containers, nextflow ensures that workflows run the same way every time, regardless of the computing environment.</li> <li>Portability: Workflows written in nextflow can be easily run on different platforms without modification.</li> <li>Fast Prototyping: It allows for quick assembly of complex pipelines by reusing existing scripts and tools.</li> </ul>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/00_introduction/#nf-core","title":"nf-core","text":"<p>nf-core is a community effort to collect a curated set of analysis pipelines built using nextflow. There are currently around 100 pipelines available as of April 2024. Below are some of the most popular nf-core pipelines:</p> <ul> <li>rnaseq</li> <li>sarek</li> <li>mag</li> <li>chipseq</li> <li>scrnaseq</li> <li>atacseq</li> <li>ampliseq</li> <li>nanoseq</li> <li>methylseq</li> <li>rnafusion</li> <li>eager</li> <li>fetchngs</li> <li>differentialabundance</li> </ul>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/00_introduction/#hands-on","title":"Hands-on","text":"<ul> <li>fetchngs</li> <li>rnaseq</li> <li>differentialabundance</li> <li>Multiqc report</li> <li>differentialabundance report</li> </ul>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/00_introduction/#presenters","title":"Presenters","text":"<sub>Shirley Li</sub> <sub>Yucheng Zhang</sub> <p>Next: fetchngs</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/","title":"nf-core/fetchngs","text":"<p>In this workshop, users will be guided through the practical application of the nf-core fetchngs pipeline, focusing specifically on downloading raw sequencing reads from the NCBI database. Participants will gain insights into utilizing this powerful tool within the nf-core framework with Open OnDemand to efficiently download necessary data for bioinformatics analyses. </p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#example-dataset-and-reference-paper","title":"Example dataset and reference paper","text":"<p>In this workshop, we will analyze RNA-Seq data from the study (Asberry et al., 2022) on the human epigenetic regulator <code>PRMT5</code> and its cofactor <code>MEP50</code>.</p> <ul> <li> <p>Paper: Discovery and Biological Characterization of PRMT5:MEP50 Protein\u2013Protein Interaction Inhibitors </p> </li> <li> <p>According to the paper, the raw data can be found on Gene Expression Omnibus database at <code>GSE80182</code>. </p> </li> </ul>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#gene-exression-omnibus-geo","title":"Gene exression omnibus (GEO)","text":"<p>The Gene Expression Omnibus (GEO) is a public repository that archives and freely distributes comprehensive sets of microarray, next-generation sequencing, and other forms of high-throughput functional genomic data.</p> <p>You can find the page for the specific example dataset at this link.           The <code>fetchngs</code> pipeline is an extremely powerful tool that is capable of working with various types of IDs, such as SRA, ENA, DDBJ, and GEO IDs. You can use <code>GSE80182</code> on its own to download all available datasets. However, for this workshop, we only require 6 out of the 9 available samples. In order to obtain the necessary IDs for each of these samples, simply click on <code>SRA</code>.</p> <p>The required 6 samples are from <code>PRMT50kd</code> and <code>GTFkd</code> groups, with accession numbers ranging from <code>SRX1693951</code> to <code>SRX1693956</code>. For this demo, we will exclude the remaining 3 <code>MEP50kd</code> samples.</p> <p></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#nf-core-fetchngs","title":"nf-core fetchngs","text":"<p>nf-core/fetchngs is a bioinformatics pipeline to fetch metadata and raw FastQ files from both public databases. At present, the pipeline supports SRA / ENA / DDBJ / GEO ids.            </p> <p>In order to execute the <code>fetchngs</code> pipeline, let's start by creating a working directory where the pipeline will run.</p> <p>We created the <code>/cluster/tufts/workshop</code> folder to provide storage for users to run jobs during the workshop. Inside the <code>workshop</code> folder, each user has your own folder with your Tufts UTLN as the folder name.</p> <pre><code>ls -1 /cluster/tufts/workshop/\namarti45/\natai01/\nbcarso02/\nblin01/\nfvilch01/\ngwidme01/\nhgardn02/\nhli25/\nhyan01/\nisanog01/\njfinne04/\nkmegqu01/\nkreyno07/\nmcalde03/\nmlehr01/\nrsaid01/\nshared/\nsmosta03/\nspasch03/\nxli37/\nylee02/\nymalon01/\nyzhang85/\nyzhao12/\nzwatso01/\n</code></pre> <p>Let's create the working directory for fetchngs. </p> <pre><code>cd /cluster/tufts/workshop/UTLN ## replace UTLN with your own UTLN\nmkdir fetchngs\ncd fetchngs\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#create-a-samplesheetcsv-as-input","title":"Create a samplesheet.csv as input","text":"<p>Use the code below to create a <code>samplesheet.csv</code> file, which will be the input for fetchngs pipeline.           <pre><code>for i in {3951..3956}\ndo\n   echo \"SRX169$i\" &gt;&gt; samplesheet.csv\ndone\n</code></pre></p> <p>Now let's see what's in the file.        <pre><code>cat samplesheet.csv\n</code></pre></p> <pre><code>SRX1693951\nSRX1693952\nSRX1693953\nSRX1693954\nSRX1693955\nSRX1693956\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#open-ondemand","title":"Open OnDemand","text":"<p>In the demo, we will run the pipeline using the <code>fetchngs</code> pipeline deployed on Tufts Open OnDemand server</p> <p>Under <code>Bioinformatcis Apps</code>, you can find <code>fetchngs</code> within the <code>nf-core pipelines</code> subcategory.</p> <p>This pipeline is pretty simple. We can leave most parameters as default.</p> <p>Below are the arguments we will use:</p> <ul> <li>Number of hours: 12</li> <li>Select cpu partition: batch</li> <li>Resveration for class, training, workshop: default</li> <li>Version: 1.12.0</li> <li>Working Directory: The direcotry your created above. For me, it is <code>/cluster/tufts/workshop/yzhang85/fetchngs</code></li> <li>Output directory Name: fetchngsOut</li> <li>Input: samplesheet.csv</li> <li>nf_core_pipeline: rnaseq</li> <li>nf_core_rnaseq_strandedness: auto</li> <li>download_method: aspera</li> </ul> <p>A screenshot of the Open OnDemand fetchngs app.         </p> <p></p> <p>Once you fill in the required fields, you can launch the job.</p> <p>When the job starts, you can click the link next to <code>Session ID:</code> to view <code>output.log</code> and check the running processes of nextflow.</p> <p> <pre><code>Script starting...\nGenerating connection YAML file...\nThis is a fresh run\nYou are not using Nextflow Tower\nCurrently Loaded Modulefiles:\n  1) java/15.0.2                  4) singularity/3.8.4(default)\n  2) nextflow/23.10.0(default)    5) nf-core/2.13.1\n  3) squashfs/4.4\nnextflow run /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-fetchngs/1.12.0/1_12_0 -params-file nf-params.json -profile tufts --partition batch\nNextflow 23.10.1 is available - Please consider updating your version to it\nN E X T F L O W  ~  version 23.10.0\nLaunching `/cluster/tufts/biocontainers/nf-core/pipelines/nf-core-fetchngs/1.12.0/1_12_0/main.nf` [chaotic_wescoff] DSL2 - revision: 0f0b67958c\nWARN: Access to undefined parameter `monochromeLogs` -- Initialise it to a default value eg. `params.monochromeLogs = some_value`\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/fetchngs v1.12.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : chaotic_wescoff\n  containerEngine           : singularity\n  launchDir                 : /cluster/tufts/workshop/yzhang85/fetchngs\n  workDir                   : /cluster/tufts/workshop/yzhang85/fetchngs/work\n  projectDir                : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-fetchngs/1.12.0/1_12_0\n  userName                  : yzhang85\n  profile                   : tufts\n  configFiles               : \n\nInput/output options\n  input                     : samplesheet.csv\n  nf_core_pipeline          : rnaseq\n  download_method           : aspera\n  outdir                    : fetchngsOut\n\nInstitutional config options\n  config_profile_description: The Tufts University HPC cluster profile provided by nf-core/configs.\n  config_profile_contact    : Yucheng Zhang\n  config_profile_url        : https://it.tufts.edu/high-performance-computing\n\nMax job request options\n  max_cpus                  : 72\n  max_memory                : 120 GB\n  max_time                  : 7d\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nIf you use nf-core/fetchngs for your analysis please cite:\n\n* The pipeline\n  https://doi.org/10.5281/zenodo.5070524\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/fetchngs/blob/master/CITATIONS.md\n------------------------------------------------------\nWARN: The following invalid input values have been detected:\n\n* --partition: batch\n* --config_profile_contact_github: @zhan4429\n* --config_profile_contact_email: Yucheng.Zhang@tufts.edu\n* --igenomes_base: /cluster/tufts/biocontainers/datasets/igenomes/\n\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 1\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (1)\n[01/68b765] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (2)\n[06/90f0d7] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (3)\n[3b/33ec6c] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (5)\n[80/39b1b0] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (7)\n[80/39b1b0] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [ 83%] 5 of 6\n[9e/846ed1] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [  0%] 0 of 4\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (8)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[48/d703d4] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (10)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[b5/171552] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (12)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (12)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[9e/846ed1] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [ 16%] 1 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_CLI -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (13)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[46/4aa4c0] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [  0%] 0 of 5\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (15)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[55/81029f] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (16)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[1a/eaaf05] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [  0%] 0 of 6\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (1)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[46/4aa4c0] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [ 16%] 1 of 6\n[29/62b05a] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [  0%] 0 of 1\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (3)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[55/81029f] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [ 50%] 3 of 6\n[cd/e82a75] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [ 66%] 2 of 3\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (3)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[1a/eaaf05] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [ 66%] 4 of 6\n[cd/e82a75] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [ 66%] 2 of 3\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (4)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[1a/eaaf05] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [ 66%] 4 of 6\n[bc/d2790a] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 4 of 4\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (5)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c3/26736c] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [ 83%] 5 of 6\n[c1/a5ca55] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [ 80%] 4 of 5\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... -\n\nexecutor &gt;  slurm (18), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [  0%] 0 of 1\n\nexecutor &gt;  slurm (19), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[9c/6b23e9] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [  0%] 0 of 1\n\nexecutor &gt;  slurm (19), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[9c/6b23e9] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [  0%] 0 of 1\n\nexecutor &gt;  slurm (19), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[9c/6b23e9] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [100%] 1 of 1 \u2714\n\nexecutor &gt;  slurm (19), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[9c/6b23e9] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [100%] 1 of 1 \u2714\n-[nf-core/fetchngs] Pipeline completed successfully-\nWARN: =============================================================================\n  Please double-check the samplesheet that has been auto-created by the pipeline.\n\n  Public databases don't reliably hold information such as strandedness\n  information, controls etc\n\n  All of the sample metadata obtained from the ENA has been appended\n  as additional columns to help you manually curate the samplesheet before\n  running nf-core/other pipelines.\n===================================================================================\n\nexecutor &gt;  slurm (19), local (6)\n[71/53b661] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS... [100%] 6 of 6 \u2714\n[aa/4d377f] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUN... [100%] 6 of 6 \u2714\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_FAS... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_D... -\n[c6/019c9d] process &gt; NFCORE_FETCHNGS:SRA:ASPERA_... [100%] 6 of 6 \u2714\n[6a/2e0f70] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_... [100%] 6 of 6 \u2714\n[9c/6b23e9] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC... [100%] 1 of 1 \u2714\n-[nf-core/fetchngs] Pipeline completed successfully-\nWARN: =============================================================================\n  Please double-check the samplesheet that has been auto-created by the pipeline.\n\n  Public databases don't reliably hold information such as strandedness\n  information, controls etc\n\n  All of the sample metadata obtained from the ENA has been appended\n  as additional columns to help you manually curate the samplesheet before\n  running nf-core/other pipelines.\n===================================================================================\nCompleted at: 02-Apr-2024 17:05:05\nDuration    : 10m 8s\nCPU hours   : 3.0\nSucceeded   : 25\n\n\nCleaning up...\n</code></pre></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#check-the-output-files","title":"Check the output files","text":"<p>Once your job is completed, you should be able to see the following output files in your output directory.         </p> <p>In the <code>fastq/</code> directory, the downloaded FASTQ files are located.                       </p> <p>Within the <code>samplesheet/</code> directory, there's a file called <code>samplesheet.csv</code> that holds all the essential information needed for the subsequent nf-core/rnaseq pipeline.           </p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#clean-up","title":"Clean up","text":""},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#check-the-size-of-the-output-files","title":"Check the size of the output files","text":"<p>If you check the size of your output files through <code>du -sh *</code>, you can see the work/ directory occupies significant storage space. </p> <p></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#nextflow-clean","title":"nextflow clean","text":"<p>Let's check the log file first.      <pre><code>cd /cluster/tufts/workshop/yzhang85/fetchngs\nmodule load nextflow\nnextflow log\n</code></pre></p> <p>You should see some useful runtime information of completed jobs in the current directory. We can also use the <code>RUN NAME</code> and <code>nextflow clean</code> to clean the <code>work</code> directory. In this case, the <code>RUN NAME</code> is <code>irreverent_rutherford</code>.</p> <pre><code>TIMESTAMP           DURATION    RUN NAME        STATUS  REVISION ID SESSION ID                              COMMAND                                                                                                                                                \n2024-04-02 16:54:56 10m 9s      chaotic_wescoff OK      0f0b67958c  0857d9f5-2ebe-48cb-a85d-0f0808a77496    nextflow run /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-fetchngs/1.12.0/1_12_0 -params-file nf-params.json -profile tufts --partition batch\n</code></pre> <pre><code>$ nextflow clean chaotic_wescoff -f\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/01/68b7658bc646ef617e006094556faa\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/06/90f0d770a353e3827f61493361b756\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/3b/33ec6cb90b84d1209c3f34b738b70a\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/ee/0922d9edcc4c1549b6d1340feb3f1a\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/80/39b1b03fd711a8ea9bef282d7fc125\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/71/53b661f0cd56a9e6bb45480bcf5676\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/9e/846ed1911d06053e84719b31a442dd\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/48/d703d40187f2d1518d5c84c35063be\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/c4/3b014c613532f5e4a939116592a3fb\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/b5/171552017ca1e7e0458da38e8f4338\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/8b/8877252cd5a7cb4905265f456cde91\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/aa/4d377ffa0133c83fa5310bba069847\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/46/4aa4c0c5edb9a0499ee152a1853f41\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/0c/f19f2fd5025c8e1954bc183f33e8c6\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/55/81029fdb988aa8a37267eb821d73d3\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/1a/eaaf05e3dec16dd0e16fffcb76143c\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/c3/26736c6bc1faa638cab3cc6d7e7b88\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/c6/019c9d3e5c1ff32ca0b2c0af5758ba\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/29/62b05ab9fd87a8ecde69f4d14ecd4c\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/92/1af049324bb3fc85367a4e691720c7\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/cd/e82a75cbe6bbb5494987accac728af\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/bc/d2790ae814446df06655e9449e6f5d\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/c1/a5ca55d38b7b7def3b8660041cda8f\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/6a/2e0f707f95c7a32a63c07f877b61b1\nRemoved /cluster/tufts/workshop/yzhang85/fetchngs/work/9c/6b23e94d4c044755be76bf76c71489\n</code></pre> <p>You also clean the <code>work/</code> directory by <code>rm -r work/</code> </p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/01_fetchngs/#summary","title":"Summary","text":"<p>After successfully running this pipeline, your specified output directory will contain the FASTQ files that were downloaded from the NCBI database. These files will be the raw sequencing reads that you will use for the next pipeline, nf-core/rnaseq. Next, we will proceed with the nf-core/rnaseq pipeline to further process this dataset and conduct RNA-Seq data analysis.</p> <p>Next: rnaseq</p> <p>Previous: Introduction</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/","title":"nf-core/rnaseq","text":"<p>nf-core/rnaseq is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.</p> <p></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#create-the-working-directory","title":"Create the working directory","text":"<pre><code>mkdir -p /cluster/tufts/workshop/UTLN/rnaseq  ## Replace UTLN with your own UTLN\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#prepare-the-input-samplesheet","title":"Prepare the input samplesheet","text":""},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#samplesheet-generated-by-fetchngs","title":"Samplesheet generated by fetchngs","text":"<pre><code>cd /cluster/tufts/workshop/UTLN/fetchngs/fetchngsOut/samplesheet\nls\n</code></pre> <p>You should see the below output:</p> <pre><code>id_mappings.csv  multiqc_config.yml  samplesheet.csv\n</code></pre> <p>We can use <code>samplesheet.csv</code> directly for RNAseq analysis, but I prefer creating a simpler samplesheet.</p> <pre><code>head -n 3 samplesheet.csv\n</code></pre> <pre><code>\"sample\",\"fastq_1\",\"fastq_2\",\"strandedness\",\"run_accession\",\"experiment_accession\",\"sample_accession\",\"secondary_sample_accession\",\"study_accession\",\"secondary_study_accession\",\"submission_accession\",\"run_alias\",\"experiment_alias\",\"sample_alias\",\"study_alias\",\"library_layout\",\"library_selection\",\"library_source\",\"library_strategy\",\"library_name\",\"instrument_model\",\"instrument_platform\",\"base_count\",\"read_count\",\"tax_id\",\"scientific_name\",\"sample_title\",\"experiment_title\",\"study_title\",\"sample_description\",\"fastq_md5\",\"fastq_bytes\",\"fastq_ftp\",\"fastq_galaxy\",\"fastq_aspera\"\n\"SRX1693951\",\"fetchngsOut/fastq/SRX1693951_SRR3362661_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693951_SRR3362661_2.fastq.gz\",\"auto\",\"SRR3362661\",\"SRX1693951\",\"SAMN04639576\",\"SRS1386868\",\"PRJNA318251\",\"SRP073189\",\"SRA409858\",\"GSM2114336_r1\",\"GSM2114336\",\"GSM2114336\",\"GSE80182\",\"PAIRED\",\"cDNA\",\"TRANSCRIPTOMIC\",\"RNA-Seq\",\"\",\"Illumina HiSeq 2500\",\"ILLUMINA\",\"9527214600\",\"31757382\",\"9606\",\"Homo sapiens\",\"A549_GFPkd_1\",\"Illumina HiSeq 2500 sequencing: GSM2114336: A549_GFPkd_1 Homo sapiens RNA-Seq\",\"A TGFbeta-PRMT5-MEP50 Axis Regulates Cancer Cell Invasion through Histone H3 and H4 Arginine Methylation Coupled Transcriptional Activation and Repression\",\"A549_GFPkd_1\",\"d2da6755e6219a091e0a84e9fc63a6fa;ac0e3cd5d59ef55aa3c6d3bb0c31a10c\",\"3720225514;3596832711\",\"ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_1.fastq.gz;ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_2.fastq.gz\",\"ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_1.fastq.gz;ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_2.fastq.gz\",\"fasp.sra.ebi.ac.uk:/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_1.fastq.gz;fasp.sra.ebi.ac.uk:/vol1/fastq/SRR336/001/SRR3362661/SRR3362661_2.fastq.gz\"\n\"SRX1693952\",\"fetchngsOut/fastq/SRX1693952_SRR3362662_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693952_SRR3362662_2.fastq.gz\",\"auto\",\"SRR3362662\",\"SRX1693952\",\"SAMN04639580\",\"SRS1386867\",\"PRJNA318251\",\"SRP073189\",\"SRA409858\",\"GSM2114337_r1\",\"GSM2114337\",\"GSM2114337\",\"GSE80182\",\"PAIRED\",\"cDNA\",\"TRANSCRIPTOMIC\",\"RNA-Seq\",\"\",\"Illumina HiSeq 2500\",\"ILLUMINA\",\"11198386200\",\"37327954\",\"9606\",\"Homo sapiens\",\"A549_GFPkd_2\",\"Illumina HiSeq 2500 sequencing: GSM2114337: A549_GFPkd_2 Homo sapiens RNA-Seq\",\"A TGFbeta-PRMT5-MEP50 Axis Regulates Cancer Cell Invasion through Histone H3 and H4 Arginine Methylation Coupled Transcriptional Activation and Repression\",\"A549_GFPkd_2\",\"152754e5004274d7e0d8f56ea481e2d7;f2290b266571d98c782e7614528ab3c9\",\"4367522067;4230444730\",\"ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_1.fastq.gz;ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_2.fastq.gz\",\"ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_1.fastq.gz;ftp.sra.ebi.ac.uk/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_2.fastq.gz\",\"fasp.sra.ebi.ac.uk:/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_1.fastq.gz;fasp.sra.ebi.ac.uk:/vol1/fastq/SRR336/002/SRR3362662/SRR3362662_2.fastq.gz\"\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#reformating-samplesheet","title":"Reformating samplesheet","text":"<pre><code>awk -F\",\" 'OFS=\",\"{print $27,$2,$3,$4}' samplesheet.csv | sed 's/A549_//g' | sed 's/sample_title/sample/g'&gt; /cluster/tufts/workshop/UTLN/rnaseq/samplesheet.csv\n</code></pre> <p>We can clearly see that the samplesheet we generated is simple to read. And it only has 4 columns: <code>sample</code>, <code>fastq_1</code>, <code>fastq_2</code>, and <code>strandness</code>.</p> <pre><code>cat /cluster/tufts/workshop/UTLN/rnaseq/samplesheet.csv\n</code></pre> <pre><code>\"sample\",\"fastq_1\",\"fastq_2\",\"strandedness\"\n\"GFPkd_1\",\"fetchngsOut/fastq/SRX1693951_SRR3362661_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693951_SRR3362661_2.fastq.gz\",\"auto\"\n\"GFPkd_2\",\"fetchngsOut/fastq/SRX1693952_SRR3362662_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693952_SRR3362662_2.fastq.gz\",\"auto\"\n\"GFPkd_3\",\"fetchngsOut/fastq/SRX1693953_SRR3362663_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693953_SRR3362663_2.fastq.gz\",\"auto\"\n\"PRMT5kd_1\",\"fetchngsOut/fastq/SRX1693954_SRR3362664_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693954_SRR3362664_2.fastq.gz\",\"auto\"\n\"PRMT5kd_2\",\"fetchngsOut/fastq/SRX1693955_SRR3362665_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693955_SRR3362665_2.fastq.gz\",\"auto\"\n\"PRMT5kd_3\",\"fetchngsOut/fastq/SRX1693956_SRR3362666_1.fastq.gz\",\"fetchngsOut/fastq/SRX1693956_SRR3362666_2.fastq.gz\",\"auto\"\n</code></pre> <p>Note</p> <p>Please note that the <code>fastq_1</code> and <code>fastq_2</code> columns in the data contain the location of fastq files. However, these paths are relative rather than absolute. To ensure the <code>rnaseq</code> pipeline can locate these fastq files, we can create a soft link for <code>fetchngsOut</code> in the working directory of <code>rnaseq</code> pipeline.</p> <pre><code>cd /cluster/tufts/workshop/UTLN/rnaseq/\nln -s /cluster/tufts/workshop/UTLN/fetchngs/fetchngsOut .\nls -l\n</code></pre> <p>Please verify that we have successfully linked <code>fetchngsOut</code> to the current directory.</p> <pre><code>lrwxrwxrwx 1 yzhang85 workshop  53 Apr  2 18:19 fetchngsOut -&gt; /cluster/tufts/workshop/yzhang85/fetchngs/fetchngsOut/\n-rw-rw---- 1 yzhang85 workshop 788 Apr  2 18:18 samplesheet.csv\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#rnaseq-on-open-ondemand","title":"rnaseq on Open OnDemand","text":"<p>We have already downloaded the raw fastq files for RNAseq using <code>fetchngs</code>. However, for conducting RNAseq analysis, we also need the reference genome <code>fasta</code> file and <code>gtf</code> annotation file. Since these are human samples, we require the human reference genome.</p> <p>There are two ways to obtain the human reference genome:</p> <ol> <li>Choose <code>GRCh38</code> in <code>iGenomes</code>. This method is simple, and the <code>iGenomes</code> have been set up for users. You only need to select the reference to use. However, this method is <code>not recommended</code> because the annotation files in <code>iGenomes</code> have not been updated in some years, making them out of date. We advise against using them for your research and recommend them for classroom/workshop purposes only.</li> <li>Download the latest version of genomes from public databases such as <code>Ensembl</code> or <code>NCBI</code>.</li> </ol> <p>In this workshop, we will guide you on how to download your own reference genomes from Ensemble database.</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#open-ondemand-arguments","title":"Open OnDemand Arguments","text":"<ul> <li>Number of hours: 24</li> <li>Select cpu partition: batch</li> <li>Reservation for class, training, workshop: Default</li> <li>Version: 3.14.0</li> <li>Working Directory: <code>/cluster/tufts/workshop/UTLN/rnaseq/</code> ## Replace UTLN with your own UTLN</li> <li>outdir: rnaseqOut</li> <li>input: samplesheet.csv</li> <li>multiqc_title: PRMT5kd vs. GFPkd</li> <li>iGenomes: None</li> <li>fasta: https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz</li> <li>gtf: https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz</li> <li>trimmer: trimgalore</li> <li>extra_fastp_args: -q 35 --paired</li> <li>aligner: star_salmon</li> <li>save_reference: true</li> <li>skip_umi_extract: true</li> <li>skip_pseudo_alignment: true</li> <li>skip_stringtie: true</li> </ul> <pre><code>------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.14.0\n------------------------------------------------------\nCore Nextflow options\n  runName                   : irreverent_watson\n  containerEngine           : singularity\n  launchDir                 : /cluster/tufts/workshop/UTLN/rnaseq\n  workDir                   : /cluster/tufts/workshop/UTLN/rnaseq/work\n  projectDir                : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.14.0/3_14_0\n  userName                  : yzhang85\n  profile                   : tufts\n  configFiles               : \n\nInput/output options\n  input                     : samplesheet.csv\n  outdir                    : rnaseqOut\n  multiqc_title             : PRMT5kd vs. GFPkd\n\nReference genome options\n  fasta                     : https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\n  gtf                       : https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\n  igenomes_base             : /cluster/tufts/biocontainers/datasets/igenomes/\n\nRead trimming options\n  extra_trimgalore_args     : -q 35 --paired\n\nOptional outputs\n  save_reference            : true\n\nProcess skipping options\n  skip_umi_extract          : true\n  skip_pseudo_alignment     : true\n  skip_stringtie            : true\n\nInstitutional config options\n  config_profile_description: The Tufts University HPC cluster profile provided by nf-core/configs.\n  config_profile_contact    : Yucheng Zhang\n  config_profile_url        : https://it.tufts.edu/high-performance-computing\n\nMax job request options\n  max_cpus                  : 72\n  max_memory                : 120 GB\n  max_time                  : 7d\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\n------------------------------------------------------\nIf you use nf-core/rnaseq for your analysis please cite:\n\n* The pipeline\n  https://doi.org/10.5281/zenodo.1400710\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/rnaseq/blob/master/CITATIONS.md\n\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:CAT_FASTQ -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... -\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... -\n\n.\n.\n.\n\nMonitor the execution with Nextflow Tower using this URL: https://tower.nf/user/yucheng-zhang/watch/5di71eyYto2FH9\nexecutor &gt;  slurm (209)\n[d1/517f0f] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[29/eb99ef] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[eb/937d34] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[30/ba7437] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[f8/be5fd5] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[b8/092179] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[c1/7d9c49] process &gt; NFCORE_RNASEQ:RNASEQ:PREPAR... [100%] 1 of 1 \u2714\n[-        ] process &gt; NFCORE_RNASEQ:RNASEQ:CAT_FASTQ -\n[2e/e95ccb] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... [100%] 6 of 6 \u2714\n[d3/24f617] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... [100%] 6 of 6 \u2714\n[97/2bb6ec] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... [100%] 1 of 1 \u2714\n[49/f116ce] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... [100%] 6 of 6 \u2714\n[66/7832b0] process &gt; NFCORE_RNASEQ:RNASEQ:FASTQ_... [100%] 6 of 6 \u2714\n[df/7eb1ea] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[6a/857d7c] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[46/65e709] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[06/684719] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[01/2fc01d] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[0f/f9bf31] process &gt; NFCORE_RNASEQ:RNASEQ:ALIGN_... [100%] 6 of 6 \u2714\n[81/eaf8db] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 6 of 6 \u2714\n[d5/12a872] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[17/ceb58b] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[f7/3dcf2d] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[4b/2991d1] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[9d/1204f5] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[14/249750] process &gt; NFCORE_RNASEQ:RNASEQ:QUANTI... [100%] 1 of 1 \u2714\n[76/2d571a] process &gt; NFCORE_RNASEQ:RNASEQ:DESEQ2... [100%] 1 of 1 \u2714\n[b7/1658e4] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[64/31c31b] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[c6/9ff3a9] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[36/df1dd9] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[f4/fb9e0d] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_MA... [100%] 6 of 6 \u2714\n[42/28fcdb] process &gt; NFCORE_RNASEQ:RNASEQ:SUBREA... [100%] 6 of 6 \u2714\n[68/b06f5c] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQ... [100%] 6 of 6 \u2714\n[ce/7d55f5] process &gt; NFCORE_RNASEQ:RNASEQ:BEDTOO... [100%] 6 of 6 \u2714\n[24/45057e] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[36/194966] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[de/70fc07] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[be/a747c3] process &gt; NFCORE_RNASEQ:RNASEQ:BEDGRA... [100%] 6 of 6 \u2714\n[2b/25dca2] process &gt; NFCORE_RNASEQ:RNASEQ:QUALIM... [100%] 6 of 6 \u2714\n[21/ebcfd0] process &gt; NFCORE_RNASEQ:RNASEQ:DUPRAD... [100%] 6 of 6 \u2714\n[91/91276e] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[18/b555d7] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[8c/a1307b] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[5f/d7cac7] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[c7/ce1ec3] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[78/10ee50] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[44/7c4656] process &gt; NFCORE_RNASEQ:RNASEQ:BAM_RS... [100%] 6 of 6 \u2714\n[56/57e6ac] process &gt; NFCORE_RNASEQ:RNASEQ:CUSTOM... [100%] 1 of 1 \u2714\n[6b/f6514d] process &gt; NFCORE_RNASEQ:RNASEQ:MULTIQ... [100%] 1 of 1 \u2714\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 26-Mar-2024 14:57:46\nDuration    : 6h 45m 41s\nCPU hours   : 109.2\nSucceeded   : 209\n\n\nCleaning up...\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#running-pipeline-on-the-command-line","title":"Running pipeline on the command line","text":"<p>If you prefer to run the pipelines using the command line interface, you can submit a slurm jobscript with the following code.</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#run-the-pipeline-directly","title":"Run the pipeline directly","text":"<pre><code>module load nf-core\nexport NXF_SINGULARITY_CACHEDIR=/cluster/tufts/biocontainers/nf-core/singularity-images\n\nnextflow run /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-rnaseq/3.14.0/3_14_0\n  -profile tufts \\\n  --input  samplesheet.csv \\\n  --outdir rnaseqOut \\\n  --gtf \"https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\" \\\n  --fasta \"https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\" \\\n  --extra_trimgalore_args \"-q 35 --paired\" \\\n  --skip_pseudo_alignment \\\n  --save_reference\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#another-easier-way","title":"Another easier way","text":"<pre><code>module load nf-core-rnaseq/3.14.0\nrnaseq -profile tufts \\\n  --input  samplesheet.csv \\\n  --outdir rnaseqOut \\\n  --gtf \"https://ftp.ensembl.org/pub/release-111/gtf/homo_sapiens/Homo_sapiens.GRCh38.111.gtf.gz\" \\\n  --fasta \"https://ftp.ensembl.org/pub/release-111/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\" \\\n  --extra_trimgalore_args \"-q 35 --paired\" \\\n  --skip_pseudo_alignment \\\n  --save_reference\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#nextflow-clean","title":"Nextflow clean","text":""},{"location":"2024_workshops/nfcore_rnaseq_sp24/02_rnaseq/#clean-the-work","title":"Clean the work","text":"<p>You can clean the <code>work</code> directory, by mannualy run</p> <p><pre><code>rm -rf work\n</code></pre> Next: Differentialabuandance</p> <p>Previous: fetchngs</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/","title":"nf-core/differentialabundance","text":"<p>nf-core/differentialabundance is a bioinformatics pipeline that can be used to analyse data represented as matrices, comparing groups of observations to generate differential statistics and downstream analyses. The pipeline supports RNA-seq data such as that generated by the nf-core rnaseq workflow, and Affymetrix arrays via .CEL files.</p> <p></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#create-the-working-directory","title":"Create the working directory","text":"<pre><code>mkdir -p /cluster/tufts/workshop/UTLN/differentialabundance\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#reference-genome-gtf","title":"reference genome gtf","text":"<p>In the last RNAseq workshop, we selected <code>save_reference</code>. So that all refereneced data will be saved for our future use. Today we can reuse the gtf file for human genome.</p> <pre><code>ls -1 /cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/genome/\n</code></pre> <p>You can see the GRCh38 reference genome's gtf and fasta files. In addition, you can see the newly created STAR <code>index</code> and <code>rsem</code> folders that can be used for your future RNA-Seq analysis.</p> <pre><code>Homo_sapiens.GRCh38.111.gtf\nHomo_sapiens.GRCh38.dna.primary_assembly.fa\nHomo_sapiens.GRCh38.dna.primary_assembly.fa.fai\nHomo_sapiens.GRCh38.dna.primary_assembly.fa.sizes\nHomo_sapiens.GRCh38.dna.primary_assembly.filtered.bed\nHomo_sapiens.GRCh38.dna.primary_assembly.filtered.gtf\ngenome.transcripts.fa\nindex/\nrsem/\n</code></pre> <p>Let's create a softlink of the gtf to our <code>differentialabundance</code> folder</p> <pre><code>cd /cluster/tufts/workshop/UTLN/differentialabundance\nln -s /cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/genome/Homo_sapiens.GRCh38.111.gtf .\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#gene-expression-count-matrix","title":"gene expression count matrix","text":"<p>In the output folder of RNAseq workshop, you can find the count file we need <code>salmon.merged.gene_counts.tsv</code> via <code>ls</code>.</p> <pre><code>$ ls -1 /cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/*.tsv\n</code></pre> <pre><code>/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_counts.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_counts_length_scaled.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_counts_scaled.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_lengths.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_tpm.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.transcript_counts.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.transcript_lengths.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.transcript_tpm.tsv\n/cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/tx2gene.tsv\n</code></pre> <p>We can create a soft link of <code>salmon.merged.gene_counts.tsv</code> into our differentialabundance folder</p> <pre><code>cd /cluster/tufts/workshop/UTLN/differentialabundance\nln -s /cluster/tufts/workshop/UTLN/rnaseq/rnaseqOut/star_salmon/salmon.merged.gene_counts.tsv .\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#samplesheetcsv","title":"samplesheet.csv","text":"sample treatment replicate batch GFPkd_1 GFPkd 1 A GFPkd_2 GFPkd 2 A GFPkd_3 GFPkd 3 A PRMT5kd_1 PRMT5kd 1 A PRMT5kd_2 PRMT5kd 2 A PRMT5kd_3 PRMT5kd 3 A <p>You can copy my samplesheet.csv to your workding directory. <pre><code>cd /cluster/tufts/workshop/UTLN/differentialabundance\ncp /cluster/tufts/workshop/shared/samplesheet.csv .\n</code></pre></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#contrastcsv","title":"contrast.csv","text":"id variable reference target blocking PRMT5kd_vs_GFPkd treatment GFPkd PRMT5kd <p>You can copy my contrast.csv to your working directory. <pre><code>cd /cluster/tufts/workshop/UTLN/differentialabundance\ncp /cluster/tufts/workshop/shared/contrast.csv .\n</code></pre></p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#open-ondemand","title":"Open OnDemand","text":"<p>Click <code>differentialabundance</code> in <code>Bioinformatics Apps</code>.</p>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#arguments","title":"Arguments","text":"<ul> <li>Number of hours: 2</li> <li>Select cpu parition: batch</li> <li>Reservation for class, training, workshop: Bioinformatics Workshop</li> <li>Version: 1.4.0</li> <li>Working Directory: <code>/cluster/tufts/workshop/UTLN/differentialabundance</code> ## Change this to your own directory</li> <li>outdir: DEGout</li> <li>study_type: rnaseq</li> <li>input: samplesheet.csv</li> <li>contrasts: contrast.csv</li> <li>matrix: salmon.merged.gene_counts.tsv</li> <li>observations_id_col: sample</li> <li>observations_name_col: sample</li> <li>differential_min_fold_change: 1.5</li> <li>deseq2_vs_method: rlog</li> <li>gsea_run: true</li> <li>gsea_gene_sets: /cluster/tufts/workshop/shared/gsea/h.all.v2023.2.Hs.symbols.gmt.txt</li> <li>shinyngs_build_app: true</li> <li>report_title: PRMT5kd vs. GFPkd</li> <li>report_author: Yucheng Zhang ## You can put your name as the author</li> <li>gtf: Homo_sapiens.GRCh38.111.gtf</li> </ul> <pre><code>------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/differentialabundance v1.4.0\n------------------------------------------------------\nCore Nextflow options\n  runName                     : maniac_mcclintock\n  containerEngine             : singularity\n  container                   : [RMARKDOWNNOTEBOOK:biocontainers/r-shinyngs:1.8.4--r43hdfd78af_0]\n  launchDir                   : /cluster/tufts/workshop/UTLN/differentialabundance\n  workDir                     : /cluster/tufts/workshop/UTLN/differentialabundance/work\n  projectDir                  : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-differentialabundance/1.4.0/1_4_0\n  userName                    : yzhang85\n  profile                     : tufts\n  configFiles                 : \n\nInput/output options\n  input                       : samplesheet.csv\n  contrasts                   : contrast.csv\n  outdir                      : DEGout\n\nAbundance values\n  matrix                      : salmon.merged.gene_counts.tsv\n  affy_cel_files_archive      : null\n  querygse                    : null\n\nAffy input options\n  affy_cdfname                : null\n\nDifferential analysis\n  differential_min_fold_change: 1.5\n\nLimma specific options (microarray only)\n  limma_spacing               : null\n  limma_block                 : null\n  limma_correlation           : null\n\nGSEA\n  gsea_run                    : true\n  gsea_gene_sets              : /cluster/tufts/workshop/shared/gsea/h.all.v2023.2.Hs.symbols.gmt.txt\n\nShiny app settings\n  shinyngs_shinyapps_account  : null\n  shinyngs_shinyapps_app_name : null\n\nReporting options\n  report_file                 : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-differentialabundance/1.4.0/1_4_0/assets/differentialabundance_report.Rmd\n  logo_file                   : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-differentialabundance/1.4.0/1_4_0/docs/images/nf-core-differentialabundance_logo_light.png\n  css_file                    : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-differentialabundance/1.4.0/1_4_0/assets/nf-core_style.css\n  citations_file              : /cluster/tufts/biocontainers/nf-core/pipelines/nf-core-differentialabundance/1.4.0/1_4_0/CITATIONS.md\n  report_title                : PRMT5kd vs. GFPkd\n  report_author               : Yucheng Zhang\n  report_description          : null\n\nInstitutional config options\n  config_profile_description  : The Tufts University HPC cluster profile provided by nf-core/configs.\n  config_profile_contact      : Yucheng Zhang\n  config_profile_url          : https://it.tufts.edu/high-performance-computing\n\nMax job request options\n  max_cpus                    : 72\n  max_memory                  : 120 GB\n  max_time                    : 7d\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nIf you use nf-core/differentialabundance for your analysis please cite:\n\n* The pipeline\n  https://doi.org/10.5281/zenodo.7568000\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/differentialabundance/blob/master/CITATIONS.md\n\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n[-        ] process &gt; NFCORE_DIFFERENTIALABUNDANC... -\n\n.\n.\n.\n\nexecutor &gt;  slurm (14)\n[3c/aa1431] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[73/374104] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[64/cc51c4] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[c8/7b9eb7] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[bf/0ac8f6] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[f3/85ca6e] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[1c/37c98b] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[3b/8585ca] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[12/f7dac7] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[c3/a75051] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[3f/7f671c] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[51/a37574] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[21/f74ad3] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n[28/fbb019] process &gt; NFCORE_DIFFERENTIALABUNDANC... [100%] 1 of 1 \u2714\n-[nf-core/differentialabundance] Pipeline completed successfully-\nCompleted at: 27-Mar-2024 14:02:23\nDuration    : 25m 13s\nCPU hours   : 0.5\nSucceeded   : 14\n\nCleaning up...\n</code></pre>"},{"location":"2024_workshops/nfcore_rnaseq_sp24/03_differentialabundance/#check-the-output-files","title":"Check the output files","text":"<p>Under the output folder, you will see subfolders listed as below: <pre><code>other\nshinyngs_app\ntables\nplots\nreport\npipeline_info\n</code></pre> * Under <code>report</code> folder, you will see a html file which will be the report file.     Under <code>shinyngs_app/</code> folder, you will see a subfolder which stores the <code>app.R</code> shiny app for interactive visualization. You can then view <code>app.R</code> with Open OnDemand <code>shinyngs</code> app. </p> <p>Previous: rnaseq Next: report</p>"},{"location":"2024_workshops/software101/00_introduction/","title":"Bioinformatics application installation at Tufts HPC","text":"<p>This repository stores the slides and hands-on sessions for bioinformatics training workshops provided by Tufts Research Technology in October 2024.</p>"},{"location":"2024_workshops/software101/00_introduction/#agenda","title":"Agenda","text":"<ul> <li>Intro to Linux/Unix</li> <li>Available bioinformatics resources/tools</li> <li>Common bioinformatics data formats</li> <li>Submit bioinformatics jobs to Tufts HPC</li> </ul>"},{"location":"2024_workshops/software101/00_introduction/#presenters","title":"Presenters","text":"<sub>Shirley Li</sub> <sub>Yucheng Zhang</sub> <p>Next: fetchngs</p>"},{"location":"about/about/","title":"About Tufts Research Technology Bioinformatics","text":"<p>Research Technology Bioinformatics provides consultations to Tufts students, faculty and researchers.  In addition we maintain bioinformatics tools on the Tufts HPC Cluster and the Tufts Galaxy Platform.  We also lead in-class sessions, partner on grants, and develop workshops.</p>"}]}